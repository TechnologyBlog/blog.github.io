<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>技术博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.datadriven.top/"/>
  <updated>2018-11-20T14:26:08.326Z</updated>
  <id>http://www.datadriven.top/</id>
  
  <author>
    <name>DanielJyc</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据驱动应用（一）：整体概述</title>
    <link href="http://www.datadriven.top/2018/11/20/DDA-%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E6%95%B4%E4%BD%93%E6%A6%82%E8%BF%B0/"/>
    <id>http://www.datadriven.top/2018/11/20/DDA-数据驱动应用（一）：整体概述/</id>
    <published>2018-11-20T14:31:44.000Z</published>
    <updated>2018-11-20T14:26:08.326Z</updated>
    
    <content type="html"><![CDATA[<h1 id="dda-数据驱动应用一整体概述">DDA-数据驱动应用（一）：整体概述</h1><h2 id="概述">概述</h2><p>随着互联网的快速发展和广泛普及，产生的数据也在呈几何倍数增长。数据成了企业至关重要的资源，企业产生、收集和分析的数据也达到了前所未有的规模。从而，进一步加速了大数据技术的快速发展。</p><p>近几年，出现了各种驱动技术，包括产品驱动、技术驱动、政策驱动等，大数据也不甘寂寞，于是乎“数据驱动”一词也渐渐热了起来。那么到底什么是数据驱动呢？在讨论数据驱动前，先看几个不同领域的场景：</p><ul><li>运营场景：当你在下午五点来到商场时，口碑或者美团自动给你推送“XX火锅优惠券”，正好这就是你非常喜欢吃的火锅店，于是你毫不犹豫地选择去消费。</li><li>出行场景：当打滴滴快车出行，到达目的地后发现比预计费用多了一倍，此时滴滴自动提醒你司机是否绕道，点击“是”后自动把你多付的钱退了回来。</li><li>运维场景：当系统或者业务在运行过程中出现问题时，自动根据历史数据和当前数据识别该故障故障，并快速止损。</li><li>客服场景：当你在支付宝咨询智能客服时，根据你的提问，并结合你的历史操作，给出最佳解答。</li></ul><p>数据驱动是指，以公司内部数据（业务数据、系统数据）和公司外部数据为基础，对数据进行组织形成信息，之后利用规则、算法、机器学习、深度学习等手段进一步处理信息，最终形成自动化的决策模型，同时还要形成闭环，自动调整决策模型。当新的情况发生时，系统利用前面建立的决策模型，以人工智能的方式，对新数据进行处理，得到决策结果。</p><h2 id="数据金字塔">数据金字塔</h2><p>为了更好地理解数据驱动，我们引入数据分析模型——数据金字塔理论。数据本身是没有意义的，如果它不能转化为信息和知识的话；但如果没有数据，或者数据匮乏，信息和知识的产生也就成了无水之源。数据金字塔理论可以帮助我们理解数据、信息、知识和人工智能的关系。</p><p><img src="/images/media/15423697054649/15166724412673.jpg"></p><p>在数据金字塔(即 DIKW pyramid)体系中，每一层比下一层赋予某些特质。数据层是最基本的，信息层加入内容，知识层加入“如何去使用”，智慧层加入“什么时候才用”。</p><ul><li>信息：是被组织起来的数据，是为了特定目的对数据进行处理和建立内在关联，从而让数据具有意义，它可以回答谁（who）、什么（what）、哪里（where）、什么时候（when）的问题。</li><li>知识：对信息的总结和提炼。是基于信息之间的联系，总结出来的规律和方法论，主要用于回答为什么（why）和怎么做（how）的问题，在企业里的应用包括问题诊断、预测和最佳做法。</li><li>人工智能：机器对信息和知识的自主应用。人工智能是系统基于数据、信息和知识，形成类似于人脑的思维能力（包括学习、推理、决策等）。在信息和知识层面，数据都是提供决策支持作用，而到了人工智能阶段，则是系统模仿人类应用信息和知识进行<strong>自主决策</strong>了。</li></ul><p>总之，DIKW理论是一个数据分析模型，由下往上依次递进，其递进关系需要借助数据驱动技术实现。</p><h2 id="数据驱动型应用">数据驱动型应用</h2><p>​ 数据驱动型应用是数据驱动的体现形式。从数据到应用，它是一个不断进化的过程。如下图，主要包含这四步：数据获取、数据应用、效果评估、算法挖掘。通过数据采集得到日志数据、关系型数据、事件数据。基于决策模型和算法，形成具体的应用和产品，包括可视化类、推荐类、客服类、风控类、保险类等产品。再通过效果评估和算法挖掘形成的闭环，自动调整模型、改进结果。</p><figure><img src="/images/media/15423697054649/image-20181101132248766.png" alt="image-20181101132248766"><figcaption>image-20181101132248766</figcaption></figure><h2 id="数据驱动型应用功能抽象">数据驱动型应用功能抽象</h2><p>​ 每个智能应用都可以分解为感知器（眼）、决策器（手）、执行器（脑）。如下图，基础能力层提供这3类基础能力，智能应用层使用这3类基础能力，快速组装成需要的各种智能应用。</p><figure><img src="/images/media/15423697054649/image-20181101132442134.png" alt="image-20181101132442134"><figcaption>image-20181101132442134</figcaption></figure><ul><li><strong>感知器是智能应用的眼睛。</strong>就像人有两只眼睛一样，智能应用可以基于有向无环图DAG获取多种数据源的数据，甚至可以将多个数据源数据进行聚合，比如同时获取消息数据和数据库数据。这些数据可以通过推和拉两种方式，提供给智能应用。</li><li><strong>决策器是智能应用的大脑。</strong>决策器是整个智能应用的核心所在，主要提供规则决策、决策树、统计类算法、AI等决策能力。规则决策是基于简单规则的决策，决策树提供有优先级的复杂决策。统计类算法提供概率统计相关的一些算法，包括同比类算法、环比类算法、时间序列算法等。AI主要针对复杂场景，并且有足够的数据量，可以提供机器学习、深度学习能算法能力。</li><li><strong>执行器是智能应用的手。</strong>执行器主要用来执行任务，将执行过程抽象为工作流。将各种执行能力不断积累下来，从而方便智能应用的快速实现。</li></ul><h2 id="小结">小结</h2><p>本文首先介绍了数据驱动的基本概念和相关理论，然后引出了数据驱动型应用的基本思想和功能抽象。下一篇文章将主要介绍数据驱动的整体架构。</p><h2 id="参考">参考</h2><ul><li><p><a href="https://blog.csdn.net/dawningblue/article/details/75119639?utm_source=blogxgwz1" target="_blank" rel="noopener">什么叫做「数据驱动方法」</a></p></li><li><a href="http://blog.51cto.com/itrustdata/1961018" target="_blank" rel="noopener">数据驱动到底是什么？如何驱动，又能驱动什么？</a></li><li><a href="https://cloud.tencent.com/developer/article/1171103" target="_blank" rel="noopener">数据驱动产品智能——数据应用与用户智能</a></li><li><a href="https://book.douban.com/subject/30168661/" target="_blank" rel="noopener">数据驱动：从方法到实践</a></li><li><p><a href="https://mp.weixin.qq.com/s/gMN43aZGtU5nGAG-kvVjbQ" target="_blank" rel="noopener">基于 AIOps 的无人运维</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;dda-数据驱动应用一整体概述&quot;&gt;DDA-数据驱动应用（一）：整体概述&lt;/h1&gt;
&lt;h2 id=&quot;概述&quot;&gt;概述&lt;/h2&gt;
&lt;p&gt;随着互联网的快速发展和广泛普及，产生的数据也在呈几何倍数增长。数据成了企业至关重要的资源，企业产生、收集和分析的数据也达到了前所未有的规
      
    
    </summary>
    
      <category term="数据驱动" scheme="http://www.datadriven.top/categories/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8/"/>
    
    
      <category term="数据驱动" scheme="http://www.datadriven.top/tags/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8/"/>
    
      <category term="架构" scheme="http://www.datadriven.top/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>数据驱动应用（三）：数据服务</title>
    <link href="http://www.datadriven.top/2018/11/20/DDA-%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E5%BA%94%E7%94%A8%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1/"/>
    <id>http://www.datadriven.top/2018/11/20/DDA-数据驱动应用（三）：数据服务/</id>
    <published>2018-11-20T14:31:44.000Z</published>
    <updated>2018-11-20T14:27:24.329Z</updated>
    
    <content type="html"><![CDATA[<h1 id="dda-数据驱动应用三数据服务">DDA-数据驱动应用（三）：数据服务</h1><h2 id="概述">概述</h2><h3 id="主要概念">主要概念</h3><ul><li><strong>数据服务(Data Service)</strong>：对异构数据源，基于有向无环图，提供异构数据的查询和推送能力。</li><li><strong>指标</strong>：用于衡量事物发展程度的单位或方法，它还有个IT上常用的名字，也就是度量。例如：人口数、GDP、收入、用户数、利润率、留存率、覆盖率等。</li><li><strong>维度</strong>：是事物或现象的某种特征，如性别、地区、时间等都是维度。一般指查询约束条件。</li><li><strong>粒度</strong>：维度的一个组合。描述分析需要细分的程度。</li><li><strong>数据集</strong>：用来描述数据从哪里来，有哪些字段输出，提供哪些能力(过滤、分组)，数据表的Join关系，粒度等等</li><li><strong>HTAP数据库</strong>：Gartner提出了HTAP数据库概念，一个数据库既能支持OLTP(在线事务处理)，又能支持OLAP(在线分析处理)，涵盖大部分企业级应用的需求，一站解决这些问题。</li></ul><h3 id="异构数据查询产品对比和分析">异构数据查询产品对比和分析</h3><p>本文的主要目的是设计一个轻量级的异构数据查询和推送系统。首先，先看下主流的异构数据查询系统，如下表格：</p><table><thead><tr class="header"><th></th><th>Presto</th><th>F1:Query</th><th>Calcite</th><th>TiDB</th></tr></thead><tbody><tr class="odd"><td>描述</td><td>分布式大数据查询引擎(SQL)</td><td>大数据异构查询引擎</td><td>动态数据管理框架</td><td>分布式 HTAP数据库</td></tr><tr class="even"><td>interactive交互式查询</td><td>提供</td><td>提供：集中式查询、分散式、批量查询ETL</td><td>支持</td><td>支持</td></tr><tr class="odd"><td>数据源类型</td><td>多种</td><td>多种</td><td>多种</td><td>多种</td></tr><tr class="even"><td>支持自定义数据源</td><td></td><td></td><td>支持</td><td></td></tr><tr class="odd"><td>使用公司</td><td>Facebook</td><td>Google</td><td></td><td></td></tr><tr class="even"><td>代码开源</td><td>是</td><td>否</td><td>是</td><td>是</td></tr><tr class="odd"><td>异构聚合</td><td>是</td><td>是</td><td>是</td><td></td></tr><tr class="even"><td>模式</td><td>Coordinator-Worker</td><td>Master-Server-Worker</td><td></td><td></td></tr><tr class="odd"><td>存储数据</td><td>否</td><td>否</td><td>否</td><td>是</td></tr><tr class="even"><td>分布式</td><td>是</td><td>是</td><td></td><td>是</td></tr><tr class="odd"><td>ACID事务特性</td><td></td><td></td><td></td><td>分布式事务</td></tr><tr class="even"><td>UDF</td><td></td><td>UDF server</td><td></td><td></td></tr><tr class="odd"><td>水平弹性扩展</td><td></td><td>支持</td><td></td><td>支持</td></tr><tr class="even"><td>HTAP(OLAP/OLTP)</td><td></td><td>支持</td><td></td><td>支持</td></tr><tr class="odd"><td>特性</td><td></td><td>批量化 + 异步化 + 流水线化DAG</td><td></td><td></td></tr></tbody></table><blockquote><p>空白部分，表示没有找到相关说明。</p></blockquote><h3 id="本文的目标">本文的目标</h3><p>本文设计的数据服务，基于有向无环图DAG，对异构数据源进行处理和聚合，主要提供简单数据查询复合、复杂数据查询服务和数据实时推送服务。 <strong>优势</strong></p><ul><li>轻量级的异构数据查询引擎</li><li>结构化语义，无SQL</li><li>融合数据管理</li><li>支持异步消息的推送</li><li>面向智能应用</li></ul><p><strong>存在的缺点</strong></p><ul><li>不支持离线批处理</li><li>不支持分布式计算</li><li>只支持大量数据的异构join和聚合</li></ul><h2 id="三层数据集思想">三层数据集思想</h2><p>根据功能和层次的不同，将数据集分为3类，由下向上依次为原子数据集、公共层数据集、应用层数据集，如下图所示。</p><ul><li>原子数据集：原子数据集是一层逻辑上面的概念，用来接入数据源，包括各种数据源的数据，如HBASE/MySQL/API等。</li><li>公共层数据集：多个原子数据集沉淀之后，形成公共层数据集；当然，一个原子数据集也可以被多个公共数据集使用。前期，该层也是一个逻辑概念，需要随着时间慢慢积累；用来管理不同业务的数据，原则上同一个业务可以抽象为几个公共数据集。公共层主要起到 了管理作用，并确定统一的数据口径。</li><li>应用层数据集：一个公共数据集可以产生多个应用数据集，面向智能应用，直接赋能给应用。</li></ul><figure><img src="/images/media/15423697054405/image-20181106201548669.png" alt="image-20181106201548669"><figcaption>image-20181106201548669</figcaption></figure><h2 id="领域模型">领域模型</h2><p>如下图，领域模型分为逻辑层和物理层。</p><ul><li>物理层：数据源主要是各种数据的来源，包括MySQL、HBASE、kafka等。数据源里面的表抽象为物理表，表中包含物理字段，物理字段主要有维度、指标和标签3种类型。</li><li>逻辑层：数据集是对物理表的逻辑抽象，一个数据集可能来自不同数据源的多个物理表，其字段称为数据集字段。对数据集字段包装的时候，可以使用自定义的Transform和UDF。通过主题来管理数据集，一个主题包含多个数据集，一个数据集也可以属于多个主题。</li></ul><figure><img src="/images/media/15423697054405/image-20181106202720647.png" alt="image-20181106202720647"><figcaption>image-20181106202720647</figcaption></figure><h2 id="技术架构">技术架构</h2><figure><img src="/images/media/15423697054405/image-20181106204030223.png" alt="image-20181106204030223"><figcaption>image-20181106204030223</figcaption></figure><ul><li><strong>元数据管理</strong>：主要提供数据集、物理表、标签、指标等元数据能力。</li><li><strong>Meta Data Local Cache</strong>: 提供元数据的本地缓存。元数据使用定时刷新机制。</li><li><strong>数据安全</strong>：主要是SQL注入等安全问题。</li><li><strong>数据权限</strong>：主要是权限的控制，分为数据集维度和字段维度的权限控制。</li><li><strong>Query Parse</strong>r：查询条件的解析，以及元数据的解析。一般减少每次解析的工作量，需要缓存解析后的元数据。</li><li><strong>Query Plan Builder (构建执行计划)</strong>：主要是根据查询条件和元数据信息，构建有向无环图DAG。这里是本系统的难点。</li><li><strong>Optimizer (执行计划的优化)</strong>：根据查询条件和元数据信息，对执行计划进行优化。</li><li><strong>Scheduler (调度)</strong>：根据构件好的DAG，调度相应的执行器Executor执行。</li><li><strong>Executor (执行器)</strong>：这里的执行，多线程并发执行。</li><li><strong>Runner</strong>：主要是各种数据源的适配和性能优化，是一个原子的取数逻辑。主要是实时返回数据的数据库，不支持ETL离线型数据源。</li><li><strong>Merger (合并)</strong>：主要是数据的合并，可以抽象为<code>left join</code>和<code>full join</code>两种类型。</li><li><strong>Assemble (包装)</strong>：对字段进行包装，对行列数据进行转换。同时，可以自定义用户函数。这里会用到标签和指标元数据等信息。包装后的结果，将直接输出。</li></ul><blockquote><p>注：OLTP和OLAP两类查询是隔离的，OLTP为快查询，OLAP为慢查询，因为OLAP一般耗时会到1s左右，可能影响OLTP的性能。</p></blockquote><h2 id="系统整体设计">系统整体设计</h2><h3 id="数据集设计">数据集设计</h3><p>根据查询粒度的不同，将查询分为三类：多为统计查询、主键查询和非主键多条件查询。其中，主键查询和非主键多条件查询为单维查询，主键查询的维度就是主键。</p><figure><img src="/images/media/15423697054405/15253283517773.jpg" alt="-w661"><figcaption>-w661</figcaption></figure><h4 id="多维统计">1.多维统计</h4><p>多维统计是由&quot;维度+指标&quot;组成的聚合类数据，即聚合字段(group by)+过滤字段(where)+筛选字段(select)。数据集中的粒度(grain)决定了需要输出的维度组合，聚合维度(group by)字段属于粒度的子集，查询条件中的维度字段属于聚合维度的子集。多维统计由多个异构主表(事实表)内存Join，分别查完后按同粒度Grain进行join后输出。如下图所示，两个事实表包含相同的维度，不同的指标，经过内存join后，输出所需指标和维度。</p><figure><img src="/images/media/15423697054405/image-20181107102744337.png" alt="image-20181107102744337"><figcaption>image-20181107102744337</figcaption></figure><h4 id="单维详情查询按主键pk查询">2.单维详情查询——按主键PK查询</h4><p>通过主键(PK)查询主表一条记录，通过主表的外键查连接其他表(一般是维表)的数据，并且可以支持多级连接，支持多个主表查询。通过主键，在内存进行merge join，输出结果一般为单条记录。</p><p><img src="/images/media/15423697054405/15245723067223.jpg"></p><h4 id="单维详情查询非主键查询">3. 单维详情查询——非主键查询</h4><p>通过非主键的各种条件，查询一批主表记录。该查询中需要有一个确定的主表，通过主表来对结果进行分页查询；其他的表均为关联表，通过主表的外键连接多个关联表。</p><figure><img src="/images/media/15423697054405/image-20181107111452242.png" alt="image-20181107111452242"><figcaption>image-20181107111452242</figcaption></figure><p>参考：</p><ul><li><a href="http://www.sohu.com/a/121034331_472083" target="_blank" rel="noopener">大数据分析：多维数据分析基础与方法(钻取、切片和切块)</a></li></ul><h2 id="系统详细设计">系统详细设计</h2><h3 id="系统流程">系统流程</h3><p>如下图，描述了整个系统的设计流程上的关键部分，同时，对各部分的关键点和设计模式进行了说明。</p><figure><img src="/images/media/15423697054405/image-20181107113511326.png" alt="image-20181107113511326"><figcaption>image-20181107113511326</figcaption></figure><h3 id="元数据加载">元数据加载</h3><p>该部分主要做了：入参解析、元数据解析、元数据缓存。</p><ul><li><p>入参解析：将用户的查询解析为系统参数，主要包含3种解析类型：<code>GroupByQuery/PkQuery/NormalQuery</code></p></li><li><p>元数据解析：将数据库非结构元数据解析为结构对象，减少每次结构化的成本。同时，将查询入参和结构化元数据进行合并，得到一个内部对象。</p></li><li><p>元数据缓存：缓存采用定时刷新机制。根据数据集name加载数据集配置信息，包含：数据集、数据集字段、用到的物理表和用到物理表的字段。</p></li></ul><h3 id="执行计划构建">执行计划构建</h3><p>执行计划构建就是根据请求参数和元数据，生成数据查询的执行计划。构建的查询计划是一个有向无环图<code>DAG(Directed Acyclic Graph)</code>，每个节点是一个原子数据集。执行计划生成逻辑如下图，左边为加载的元数据信息，中间为执行计划构建流程，右边为执行计划的内容。</p><p><img src="/images/media/15423697054405/15248968893819.jpg"></p><h3 id="执行计划调度">执行计划调度</h3><p>执行计划调度是指，根据前面构建的执行计划DAG，分配线程、调用Runner执行查询，再合并结果。</p><h4 id="执行模式">执行模式</h4><p>DAG的执行过程，就是执行各个子节点（原子数据集）的过程。根据每次执行节点数的不同，分为三种执行模式：Single模式，串行模式、并行模式。如图，每个阶段均为一次原子执行，同时节点中还可能包含合并merge操作。</p><ul><li><strong>single模式</strong>：一次只支持一种原子查询。</li><li><strong>串行模式</strong>：支持多次原子查询及合并。在同一个线程中串行执行，有多少个节点就串行执行多少次。</li><li><strong>并行模式</strong>：支持多次原子查询及合并。在多个线程中并行执行。根据DAG层数进行调度，即DAG有多少层，就并发执行多少次。执行过程中，每次执行入度为零的所有节点。如图所示，第一次执行<code>node1/node2/node4</code>，第二次执行<code>node3</code>，第三次执行<code>node5</code>。</li></ul><figure><img src="/images/media/15423697054405/15248146685958.jpg" alt="-w676"><figcaption>-w676</figcaption></figure><h4 id="调度执行过程">调度执行过程</h4><p>如下图，左侧为执行环境（元数据），右侧为调度过程。首先将DAG拍平，每次找出入度为零的节点执行、合并结果，执行完该节点后，对被引用的节点入度减一。递归执行，直到所有节点入度为-1。</p><figure><img src="/images/media/15423697054405/15248195495426.jpg" alt="-w526"><figcaption>-w526</figcaption></figure><h3 id="runner执行">Runner执行</h3><p>类似于jdbc模块的方案，本文使用<code>SPI(Service Provider Interface)</code>机制发现服务，从而实现模块的解耦。同时，如果需要在外部开发定制Runner，使用SPI机制可以更好地实现。Runner种类包括<code>MySQL</code>、<code>HBASE</code>、<code>kafka</code>、<code>dubbo</code>等。下面主要介绍一种实现，即SQL类Runner：<code>SqlRunner</code></p><h4 id="sqlrunner">SqlRunner</h4><p>如下图所示，SqlRunner主要作用是将结构化的对象解析为数据库可执行的SQL。解析工具包括：<code>FilterBuilder</code>、<code>JoinBulider</code>、<code>GroupByBulider</code>、<code>SortBuilder</code>、<code>PageBuilder</code>。</p><p><img src="/images/media/15423697054405/15248311695610.jpg"></p><h5 id="filterbuilder">FilterBuilder</h5><ul><li>根据Filter构造Where子句。需要对&quot;=”、&quot;in”、&quot;range”、&quot;like&quot;四种类型的Filte进行解析。其中，range条件已经被上层区分为“&gt;”、“&gt;=”、“&lt;”、“&lt;=”，可以与&quot;=&quot;、&quot;like”共同视为单一条件值条件，组装为“字段操作符条件值”，其中条件值要根据类型决定是否加单引号。</li><li>特殊类型字段(如时间) 需要根据底层DB的标准进行翻译。</li></ul><h5 id="joinbulider">JoinBulider</h5><p>构造Join子句，包括<code>left join</code>和<code>full join</code>。</p><h5 id="groupbybulider">GroupByBulider</h5><p>根据grain字段构造<code>group by</code>子句，聚合列以grain为准。</p><h5 id="sortbuilder">SortBuilder</h5><p>取OrderBy对象中的字段名和升降序类型拼接<code>ORDER BY</code>子句。</p><h5 id="pagebuilder">PageBuilder</h5><p>取原子数据集上面的<code>pageSize</code>和<code>pageNum</code>字段。如果~，则无偏移；否则用<code>(pageNum-1)*pageSize</code>计算偏移，拼接LIMIT子句。</p><h4 id="kafka-runner">Kafka-Runner</h4><p>系统监听指定的Topic消息，收到消息后进行处理，并以异步消息的方式再把消息发送出去。其中，发送出去的消息中，携带其他异构Runner的数据信息。</p><h5 id="执行流程">执行流程</h5><p>如下图所示，接收到消息后，通过KafkaRunner处理后，再将消息发送出去。</p><ul><li>接收端：<ul><li>接收不同的topic，接收的topic对应于领域模型的<code>Table</code>。使用同一个groupId，每个消息只接收一次。</li><li>将入参解析为KV的map，本期只解析一层，后续有需求再支持解析多层。</li><li>异步数据集，只能通过消息来触发。</li><li>一个数据集只支持引用一个Topic。（引用多个Topic，会导致消息等待的问题，暂时不考虑支持这种复杂场景。）</li></ul></li><li>发送端：<ul><li>将经过处理后的数据，以消息形式发送出去。对应领域模型的数据集<code>Dataset</code>。</li></ul></li></ul><figure><img src="/images/media/15423697054405/image-20181107155543474.png" alt="image-20181107155543474"><figcaption>image-20181107155543474</figcaption></figure><h5 id="运行时更新topic">运行时更新Topic</h5><p>对于Kafka，一张物理表对应一个Topic。动态新增一张表的时候，需要在运行时，定时更新其对应的Topic。其中的关键点在于，更新Topic要保证在元数据加载完成之后，否则，可能导致接收到的消息数据，无法消费发送出去。kafka对自动新增和删除topic支持不是很好，需要自己实现一些内容。具体可以参考下面链接。</p><p>参考：</p><ul><li><a href="https://github.com/spring-projects/spring-kafka/issues/132" target="_blank" rel="noopener">how can i set consumer's topic by code</a></li><li>https://github.com/spring-projects/spring-kafka/issues/213</li></ul><h3 id="数据组装">数据组装</h3><p>组装器是对字段或者行列的处理，包含包装、转换、排序、内存分页。</p><ul><li>包装：针对单字段的处理。<ul><li>类型转换包装</li><li>展示数据包装</li><li>可视化参数包装</li><li>字段跳转URL包装</li></ul></li><li>转换：行列转换、多行转单行等。</li><li>排序：主要是对异构合并的数据进行排序。优先使用Runner自身的分类器。</li><li>分页：对异构合并的数据进行分页返回。总条目数不宜过多。</li></ul><p>总体原则：优先使用底层的能力，比如要分页的时候，如果Runner底层不能满足的时候，才使用上层的<code>Pager</code>。或者要进行数据合并时，优先使用数据库自身的join，不能满足时再考虑内存<code>Merger</code>。</p><h2 id="小结">小结</h2><p>本文主要介绍了一个轻量级的异构数据查询和推送系统。由于内容较多，本设计的很多细节没有深入介绍。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;dda-数据驱动应用三数据服务&quot;&gt;DDA-数据驱动应用（三）：数据服务&lt;/h1&gt;
&lt;h2 id=&quot;概述&quot;&gt;概述&lt;/h2&gt;
&lt;h3 id=&quot;主要概念&quot;&gt;主要概念&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;数据服务(Data Service)&lt;/strong&gt;：对异
      
    
    </summary>
    
      <category term="数据驱动" scheme="http://www.datadriven.top/categories/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8/"/>
    
    
      <category term="数据驱动" scheme="http://www.datadriven.top/tags/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8/"/>
    
      <category term="数据服务" scheme="http://www.datadriven.top/tags/%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="DAG" scheme="http://www.datadriven.top/tags/DAG/"/>
    
  </entry>
  
  <entry>
    <title>数据驱动应用（四）：数据决策</title>
    <link href="http://www.datadriven.top/2018/11/20/DDA-%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E5%BA%94%E7%94%A8%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%86%B3%E7%AD%96/"/>
    <id>http://www.datadriven.top/2018/11/20/DDA-数据驱动应用（四）：数据决策/</id>
    <published>2018-11-20T14:31:44.000Z</published>
    <updated>2018-11-20T14:27:02.589Z</updated>
    
    <content type="html"><![CDATA[<h1 id="dda-数据驱动应用四数据决策">DDA-数据驱动应用（四）：数据决策</h1><h2 id="概述">概述</h2><p>决策引擎主要目标是将业务决策逻辑从系统逻辑中抽离出来，使两种逻辑可以独立于彼此而变化，这样可以明显降低两种逻辑的维护成本。下面列举三种方案，然后分析各自优缺点，从而确定本文的方案。</p><h3 id="方案一硬编码实现方式">方案一：硬编码实现方式</h3><p><strong>优点</strong>：</p><ul><li>当规则较少、变动不频繁时，开发效率最高。</li><li>稳定性较佳，语法级别错误不会出现，由编译系统保证。</li></ul><p><strong>缺点</strong>：</p><ul><li>规则迭代成本高，对规则的少量改动就需要走全流程（开发、测试、部署）。</li><li>当存量规则较多时，可维护性差。</li><li>规则开发和维护门槛高，规则对业务分析人员不可见。业务分析人员有规则变更需求后无法自助完成开发，需要由开发人员介入开发。</li></ul><h3 id="方案二开源方案drools">方案二：开源方案Drools</h3><p><strong>配置流程</strong></p><p>使用 Drools 的规则配置流程如下图。一般只适合开发人员使用。</p><figure><img src="/images/media/15423697053653/image-20181107174702180.png" alt="image-20181107174702180"><figcaption>image-20181107174702180</figcaption></figure><p><strong>优点</strong>：</p><ul><li>策略规则和执行逻辑解耦方便维护。</li></ul><p><strong>缺点</strong>：</p><ul><li>业务分析师无法独立完成规则配置，由于规则主体 DSL 是编程语言（支持 Java，Groovy，Python），因此仍然需要开发工程师维护。</li><li>规则规模变大以后也会变得不好维护，相对硬编码的优势便不复存在。</li><li>规则的语法仅适合扁平的规则，对于嵌套条件语义（then 里嵌套 when...then 子句）的规则只能将条件进行笛卡尔积组合以后进行配置，不利于维护。</li></ul><h3 id="方案三设计轻量级决策引擎">方案三：设计轻量级决策引擎</h3><p>针对硬编码和drools方案的不足，我们开发了一套适用于运营、产品、业务、开发等人员的决策引擎。</p><p>下图是配置系统，业务人员配置决策、决策树和决策列表，开发人员负责配置底层更原子的一些组件：规则、动作和因子。随着平台积累和完善，逐渐也将规则、动作和因子交给业务人员去配置。配置后的决策元数据落地到决策库。决策引擎从决策库获取元数据，依次经过获取因子数据--&gt;规则解析--&gt;决策执行，将结果输出到数据应用层。</p><figure><img src="/images/media/15423697053653/image-20181107181546309.png" alt="image-20181107181546309"><figcaption>image-20181107181546309</figcaption></figure><p><strong>优点</strong>：</p><ul><li>规则配置门槛低，因此业务分析师很容易上手。</li><li>系统支持规则热部署。</li><li>业务和规则解耦，可以推广到别的业务。</li></ul><h3 id="小结">小结</h3><p>通过三种方案的分析，总结如下：</p><ul><li>硬编码迭代成本高。</li><li>Drools 维护门槛高。视图对非技术人员不友好，即使对于技术人员来说维护成本也不比硬编码低。</li><li>自开发决策引擎，配置门楷低，可以方便推广到其他业务。同时，后期还可以扩展更多能力：灰度、试跑、效果评估等。</li></ul><p>因此，我们自开发一套数据决策引擎。主要功能包括：</p><ul><li><p>基础决策能力：包含单个决策、决策树和决策表。</p></li><li><p>AI决策：部署训练后的模型，提供基于AI算法的决策。</p></li></ul><h2 id="领域模型">领域模型</h2><figure><img src="/images/media/15423697053653/image-20181107192620073.png" alt="image-20181107192620073"><figcaption>image-20181107192620073</figcaption></figure><p>领域模型，由下往上依次为：</p><ul><li>决策的最小元素是因子，因子由数据字段的取值和可选项组成，可选项主要用作配置过程中，方便非开发人员选择可选的值。</li><li>条件由因子和操作符组成，操作符包括：加、减、乘、除、包含、不包含等。</li><li>规则由多个条件，通过连接符<code>and/or</code>组成。本质上，规则就是一些条件，通过<strong>语法树</strong>组合而成。</li><li>决策输出的3种模式：<ul><li>决策：决策时对外输出的最小方式，由规则和Action组成。</li><li>决策树：本质上是一颗多叉树，每个节点由规则和Action组成。根节点没有规则和Action，叶子节点必有规则和Action，中间节点必有规则（Action可选）。</li><li>决策表：多个决策组成决策表。</li></ul></li><li>业务单元：用来做业务隔离，类似于类目管理的作用。不同业务单元之间的因子、规则模板和动作模板，都是相互隔离的。一般来讲，业务单元都是比较大的概念，整个公司不会有太多业务单元。</li></ul><h2 id="技术架构">技术架构</h2><figure><img src="/images/media/15423697053653/image-20181107194934545.png" alt="image-20181107194934545"><figcaption>image-20181107194934545</figcaption></figure><p><strong>架构主要流程：</strong></p><ol type="1"><li>最底层为数据层，HBASE主要存放一些实时计算的结果；MySQL为业务数据；API为http/https/dubbo接口数据，异步消息主要是Kafka数据。</li><li>算法平台使用数据层的数据，进行模型训练，再通过实时打分模块输出结果。结果输出给数据服务，或者直接输出到数据决策。</li><li>数据服务将数据层和算法的结果输出给数据决策。</li><li>数据决策模块，基于规则和算法模型，进行评分或者匹配，同时可以对结果进行排序。将决策的结果和动作，通过dubbo服务提供出去。</li><li>数据应用层使用数据决策和数据服务的能力，完成自己的功能。</li><li>通过埋点数据，进行决策效果评估和监控，评估的结果可以反馈给算法平台和管控平台。</li><li>管控平台，主要配置规则、因子、Action，进行灰度发布，规则试跑，多版本管理等。</li></ol><p><strong>数据决策的主要流程：</strong></p><ol type="1"><li>元数据加载：<ul><li>缓存：引擎运行过程中，需要使用缓存技术降低远程通信开销。</li><li>预解析：缓存解析后的规则，而不是原始规则。从而，减小解析开销。</li></ul></li><li>因子数据获取：获取决策所需要的实例数据，直接使用数据服务DS的输出。</li><li>执行：基于表达式解析引擎MVEL(表达式引擎性能对比，详见参考文档)，提供决策、决策树、决策表3种决策能力。<ul><li>决策：多个规则和一个Action。</li><li>决策表：N * (多个规则+1个Action)。注：决策表种的每个决策都有自己的优先级，默认值为100。</li><li>决策树：通过深度优先遍历或者广度优先遍历，根据节点优先级，执行每个决策点。注：每个决策点，都有自己的优先级，默认值为100。</li></ul></li><li>执行策略：针对决策树和决策表，两种模式：<ul><li><code>ONCE</code>：只将匹配的第一个叶子节点的结果返回。</li><li><code>ALL</code>：将匹配到的所有叶子节点都返回。</li></ul></li><li>结果拼装：<ul><li>执行路径拼装：主要针对决策树和决策列表，将执行路径返回。</li><li>匹配Action：将所有匹配到的Action返回，同时返回其优先级。</li><li>Rank：主要是针对批量决策，将匹配到的结果进行排序，并返回。</li></ul></li><li>Facade：将拼装的结果通过统一接口返回。</li></ol><h2 id="系统实现">系统实现</h2><h3 id="facade接口">Facade接口</h3><p>提供两种类型的决策：单个决策和批量决策。批量决策主要是为了减少多次接口调用的消耗。</p><h3 id="语法树抽象">语法树抽象</h3><h3 id="缓存">缓存</h3><p>缓存的主键key为<code>业务单元-决策类型-决策code</code>。如下图：</p><p><img src="/images/media/15423697053653/15174046686142.jpg"></p><p>缓存数据前需要对元数据进行预解析，从而决策引擎可以执行预解析后的数据。需要预解析的部分：</p><ul><li>将模型转换为表达式引擎可执行的形式。</li><li>决策树：先遍历，将遍历的结果缓存。而不是每次都遍历执行树。</li></ul><h3 id="决策执行">决策执行</h3><p>将执行过程抽象为两部分：决策执行层和公共执行层。</p><ul><li>决策执行层：实现了决策、决策树和决策表的执行逻辑。</li><li>公共执行层：为决策执行层提供公共的执行组件，包括动作执行器和规则执行器。</li></ul><h4 id="决策执行层">决策执行层</h4><h5 id="决策树执行器">决策树执行器</h5><p>加载元数据后，利用深度优先遍历的方式执行决策树，本质是一个递归的过程。执行方式有两种：<code>ONCE</code>，即执行的过程匹配到一个叶子节点后，停止执行；<code>ALL</code>，即要执行玩所有分支，才停止。执行结束后，对结果进行封装，并返回。决策树的节点，分为3种类型：根节点、中间节点和叶子节点。各节点和<code>Rule</code>、<code>Action</code>的关系如下表：</p><table><thead><tr class="header"><th></th><th>Rule</th><th>Action</th></tr></thead><tbody><tr class="odd"><td>根节点</td><td>无</td><td>无</td></tr><tr class="even"><td>中间节点</td><td>有</td><td>无</td></tr><tr class="odd"><td>叶子节点</td><td>可选</td><td>有</td></tr></tbody></table><h5 id="决策执行器">决策执行器</h5><p>决策执行器是决策树执行器的一种特殊形式，即：一个只包含Action和Rule的节点。</p><h5 id="决策表执行器">决策表执行器</h5><p>决策表执行器是决策执行器的List形式，也包含<code>ONCE</code>和<code>ALL</code>两种执行模式。</p><h4 id="公共执行层">公共执行层</h4><h5 id="规则执行器">规则执行器</h5><p>规则执行器的作用是，根据表达式和参数得到执行结果。底层采用成熟的表达式引擎来实现。为了方便扩展，使用工厂设计模式，可以方便地切换到不同的表达式引擎。各表达式引擎对比详见详见 <a href="https://technologyblog.github.io/2018/02/03/表达式引擎性能比较/" target="_blank" rel="noopener">表达式引擎性能比较</a>。经过比对发现使用<code>Mvel预编译，且指定输入值类型的方式</code>效率最高，故采用该方式。该方式没有缓存，因此自己实现一层缓存，来缓存预编译的结果。同时，采用2小时无访问，则失效缓存的策略；从而防止堆积过多无用规则配置。</p><h5 id="动作执行器">动作执行器</h5><p>根据入参、规则结果和上下文，执行响应操作，得到结果code。Action的执行类型有多种，用户可以自己定制。下面说两种执行器：</p><ul><li>返回固定内容：匹配到规则后，直接返回配置的内容。</li><li>返回动态内容：根据入参和规则结果，对其进行处理，然后返回动态内容。</li></ul><h2 id="小结-1">小结</h2><p>本文实现一个轻量级的决策引擎，本方案具有以下特点：</p><ul><li>规则表达能力强：通过决策、决策列表和决策树3种模式，可以覆盖多数的规则需求。</li><li>接入成本低：统一的页面配置，统一的接口接入。</li><li>规则运行/切换效率高：引擎运行过程中，需要使用缓存技术降低远程通信开销。同时，需要缓存解析后的规则，而不是原始规则。</li><li>两种运行模式：执行模式和调试模式。</li><li>易用性：方便配置，面向业务人员。</li><li>易管理：通过因子、规则、动作等领域模型的抽象，更方便元数据的管理。通过业务单元进行业务隔离，从而既具有隔离性又具有复用性。</li><li>规则迭代安全：规则支持热部署：系统通过版本控制，可以灰度一部分流量，增加上线信心。</li></ul><p>当然本方案还有一些不足，比如没有实现复杂的决策场景，如动态规划、决策树剪枝等；没有实现复杂的规则能力。 ​<br>​</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;dda-数据驱动应用四数据决策&quot;&gt;DDA-数据驱动应用（四）：数据决策&lt;/h1&gt;
&lt;h2 id=&quot;概述&quot;&gt;概述&lt;/h2&gt;
&lt;p&gt;决策引擎主要目标是将业务决策逻辑从系统逻辑中抽离出来，使两种逻辑可以独立于彼此而变化，这样可以明显降低两种逻辑的维护成本。下面列举三种方
      
    
    </summary>
    
      <category term="数据驱动" scheme="http://www.datadriven.top/categories/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8/"/>
    
    
      <category term="数据驱动" scheme="http://www.datadriven.top/tags/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8/"/>
    
      <category term="数据决策" scheme="http://www.datadriven.top/tags/%E6%95%B0%E6%8D%AE%E5%86%B3%E7%AD%96/"/>
    
      <category term="决策树" scheme="http://www.datadriven.top/tags/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    
  </entry>
  
  <entry>
    <title>数据驱动应用（二）：架构设计</title>
    <link href="http://www.datadriven.top/2018/11/20/DDA-%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E5%BA%94%E7%94%A8%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    <id>http://www.datadriven.top/2018/11/20/DDA-数据驱动应用（二）：架构设计/</id>
    <published>2018-11-20T14:31:44.000Z</published>
    <updated>2018-11-20T14:26:56.224Z</updated>
    
    <content type="html"><![CDATA[<h1 id="dda-数据驱动应用二架构设计">DDA-数据驱动应用（二）：架构设计</h1><h2 id="整体概述">整体概述</h2><p>在本文中，我们采用整体到部分的分析思路。首先介绍大数据系统在整个公司架构中的位置，然后具体介绍大数据系统的架构实现，再次对大数据系统中的数据驱动部分进行分析，最后对数据驱动中的各个部分依次概述。 ## 整体架构 首先，我们需要确定大数据系统在一个公司整体架构中的位置。为了方便分析，我们引入云计算中的四个概念来设计整体架构，包括：IaaS、PaaS、SaaS、DaaS。不同于云计算中服务的概念，本文主要使用这4个概念对整体架构进行粗略划分。如下图，各层依次为：</p><ul><li>IaaS：意思是基础设施即服务。主要包括虚拟机、网络、负载均衡等一些基础设施。</li><li>PaaS：意思是平台即服务。主要包括限流、通讯（RPC/http）、消息组件、注册中心、安全组件、文件系统等平台类软件。</li><li>SaaS：意思是软件即服务。主要是公司的业务应用，具有很强的领域特性。</li><li>DaaS：意思是数据即服务。可以简单的理解为，大数据系统是DaaS的一种实现形式。这里我们将其分为五层，由下往上依次为：采集层、计算层、存储层、驱动层、应用层。</li></ul><figure><img src="/images/media/15423697053589/image-20181103180601858.png" alt="image-20181103180601858"><figcaption>image-20181103180601858</figcaption></figure><h2 id="大数据系统架构">大数据系统架构</h2><h3 id="架构分层设计">架构分层设计</h3><p>如下图，结合上篇文章介绍的数据金字塔理论，得到如下分层。</p><ul><li>纵向分析：由下往上是一个逐步递进的关系，最上层数据量最小，但是为最有用的部分。大数据系统架构，主要从数据流的角度出发，将其分为：采集层、计算层、存储层、驱动层、应用层。</li><li>横向分析：如图所示，通过颜色将金字塔理论和架构分层映射起来，渐进色表示涉及到了多层。<ul><li>采集层：对应金字塔理论的“数据”。</li><li>计算层和存储层：对应金字塔理论的“数据”、“信息”和“知识”。</li></ul></li><li>应用层和数据驱动应用层：对应金字塔理论的“知识”和“AI”。</li></ul><figure><img src="/images/media/15423697053589/image-20181103181134952.png" alt="image-20181103181134952"><figcaption>image-20181103181134952</figcaption></figure><h3 id="架构设计">架构设计</h3><p>如图所示，为大数据系统的架构图，主要分为采集层、计算层、存储层、驱动层、应用层5层。大数据系统根据采集的数据，使用实时计算和离线计算能力进行初步加工，然后，再利用规则、算法等能力赋能数据应用。</p><figure><img src="/images/media/15423697053589/image-20181103181314458.png" alt="image-20181103181314458"><figcaption>image-20181103181314458</figcaption></figure><h4 id="采集层">采集层</h4><p>致力于全面、高性能、规范地完成海量数据的采集，并将其传输到计算层。被采集的数据种类包括：数据库数据和日志数据。</p><ul><li><strong>数据库数据</strong>：一般是采用T+1方式，离线同步到Hadoop。</li><li><strong>日志数据</strong>：包括应用日志和系统日志，通过Flume采集日志，以Kafka消息的方式实时同步给Storm集群和Hadoop。</li></ul><h4 id="计算层">计算层</h4><p>采集层得到的数据，将进入数据计算层中被进一步整合和计算。数据只有经过计算和整合，才能被用于洞察商业规律，挖掘潜在信息，从而实现大数据价值，达到赋能于商业和创造价值的目的。对于海量的数据，从数据计算频率来看，包括实时计算和离线计算两种方式。</p><ul><li><strong>实时计算</strong>：一般采用Storm、Spark等技术，提供秒级的计算能力。</li><li><strong>离线计算</strong>：一般采用Hadoop技术。数据计算频率主要以天（还包括小时、周、月）为单位，比如T-1，则每天凌晨处理上一天的数据。接收kafka消息数据，计算的结果输出到存储层，或直接给驱动层。</li></ul><p>工具平台提供数据管理、开发和整合的方法体系。用来构建统一、规范和可共享的全域数据体系，避免数据冗余和重复计算，规避数据烟囱和不一致性。</p><h4 id="存储层">存储层</h4><p>该层主要用来存储计算层加工后的数据。</p><ul><li><strong>关系型DB</strong>：用来存储业务数据和元数据。</li><li><strong>分析型DB</strong>：用来存储报表、多维查询类数据。一般RT较高，为百ms级别，甚至s级。</li><li><strong>时序型DB</strong>：用来存储状态数据、时间序列数据。</li><li><strong>消息</strong>：以推的方式将数据传输到上层。</li><li><strong>KV数据库</strong>：实时计算的结果一般流出到Hbase，Hbase为分布式架构，方便扩容，从而做到容量大、QPS高、低RT。</li><li><strong>API(Dubbo/Https)</strong>提供一些业务系统数据，或者公司外部数据。注：业务相关的DB，通过dubbo接口查询。</li><li><strong>缓存数据</strong>：通过分布式缓存对数据查询进行加速。</li></ul><h4 id="数据驱动层">数据驱动层</h4><p>该层主要是给数据应用层提供平台能力。包含3大部分：</p><ul><li><strong>数据服务</strong>：提供数据的查询和推送能力。</li><li><strong>数据决策</strong>：提供数据的决策能力。</li><li><strong>AI算法平台</strong>：特征提取、模型选择、参数校验、模型训练。</li></ul><h4 id="应用层">应用层</h4><p>基于数据服务、数据决策和AI算法这些基础能力，创建数据驱动型的应用。</p><h4 id="管控层">管控层</h4><p>提供数据的管理能力、监控能力、运维能力等，包括：配置管控、数据治理、元数据管理、血缘关系管理、监控、运维等。</p><h2 id="数据驱动的架构">数据驱动的架构</h2><h3 id="数据驱动的思想和架构对应关系">数据驱动的思想和架构对应关系</h3><p>上篇文章讲到了数据驱动的思想，现在将其对应到架构中，则对应数据驱动层和数据应用层。如下图，感知器即为数据服务模块；决策器为数据决策模块和AI算法模块；执行器为数据应用层。</p><figure><img src="/images/media/15423697053589/image-20181103181723636.png" alt="image-20181103181723636"><figcaption>image-20181103181723636</figcaption></figure><h3 id="数据驱动的架构-1">数据驱动的架构</h3><p>下面对数据驱动的架构进行详细说明。下图是数据驱动的关键架构，也是我们要探讨的重点内容。</p><figure><img src="/images/media/15423697053589/image-20181103181931158.png" alt="image-20181103181931158"><figcaption>image-20181103181931158</figcaption></figure><h4 id="数据服务">数据服务</h4><p>通过接入数据存储层的各种数据源，基于异构分布式数据库访问层DAG，实现了对异构数据源进行处理和聚合，主要提供简单数据查询复合、复杂数据查询服务和数据实时推送服务。该模块主要将能力输出给数据引用层，好比将眼睛有选择地看到一些事物，然后传输给大脑。</p><h4 id="ai算法平台">AI算法平台</h4><p>包括统计类算法、机器学习、深度学习、时间序列算法等。该模块接收存储层的数据进行离线训练得到所需算法模型，在对新数据进行预测。就好像大脑基于历史经历得到经验，经验即为模型。</p><h4 id="数据决策">数据决策</h4><p>基于规则引擎、动作、决策树，提供决策能力。同时，也可以使用算法平台的算法能力进行决策。好比大脑根据经验，做出决策结果。</p><h4 id="数据应用层">数据应用层</h4><p>对于多类数据应用，抽象出执行组件和状态管理两部分。基于这两类组件，可以构建出行领域、运营领域、风控领域、信用领域等应用。该层使用了数据驱动层的平台能力，可以方便的创建各类数据应用，让数据快速、方便、有效地发挥价值。该层负责将眼睛看到的新数据传输给大脑，得到大脑的决策结果后，再用手去执行。</p><h4 id="数据管理">数据管理</h4><p>对计算层、数据服务、数据决策、AI算法平台、数据应用层的配置进行管控。计算层包括Pipline、DSL、Transform、UDF等元数据；数据服务包括数据集配置和数据资产管理；算法平台包含算法相关配置；决策模块包括决策配置、规则配置、动作配置和算法配置；应用层包括各个应用的配置。</p><h2 id="小结">小结</h2><p>本文重点讲述了数据驱动的架构。在后面的文章中会依次介绍数据服务、数据决策、算法3部分的详细架构实现。对于数据采集层、数据计算层和数据存储层成熟方案已有很多，非本系列文章重点。</p><h2 id="参考">参考</h2><ul><li>大数据之路——阿里巴巴大数据实践</li><li><a href="https://wiki.mbalib.com/wiki/%E6%95%B0%E6%8D%AE%E5%8D%B3%E6%9C%8D%E5%8A%A1" target="_blank" rel="noopener">数据即服务(Data as a Service; DaaS)</a></li><li><a href="https://bbs.sangfor.com.cn/forum.php?mod=viewthread&amp;tid=32598&amp;page=1" target="_blank" rel="noopener">云计算四层分——IaaS、PaaS、SaaS、DaaS</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;dda-数据驱动应用二架构设计&quot;&gt;DDA-数据驱动应用（二）：架构设计&lt;/h1&gt;
&lt;h2 id=&quot;整体概述&quot;&gt;整体概述&lt;/h2&gt;
&lt;p&gt;在本文中，我们采用整体到部分的分析思路。首先介绍大数据系统在整个公司架构中的位置，然后具体介绍大数据系统的架构实现，再次对大数据
      
    
    </summary>
    
      <category term="数据驱动" scheme="http://www.datadriven.top/categories/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8/"/>
    
    
      <category term="数据驱动" scheme="http://www.datadriven.top/tags/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8/"/>
    
      <category term="架构" scheme="http://www.datadriven.top/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>数据驱动应用（五）：基于时间序列数据的异常识别模型</title>
    <link href="http://www.datadriven.top/2018/11/09/DDA-%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8%E5%BA%94%E7%94%A8%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E7%9A%84%E5%BC%82%E5%B8%B8%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B/"/>
    <id>http://www.datadriven.top/2018/11/09/DDA-数据驱动应用（五）：基于时间序列数据的异常识别模型/</id>
    <published>2018-11-09T14:31:44.000Z</published>
    <updated>2018-11-20T14:24:51.378Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述">1. 概述</h2><p>大型集群系统中，可能存在软件问题和硬件问题导致的系统故障，严重影响了系统的高可用性。这就要求7*24小时，对系统不间断监控。这就意味着需要不间断地监控大量时间序列数据，以便检测系统潜在的故障和异常现象。然而，实际当中的系统异常很多，且不容易发现；从而导致人工方式监控方式效率很低。</p><p>异常场景本质上是一个或者多个数据点；数据点一般在系统运行过程中产生，且能反应系统的功能是否正常，多以日志形式呈现。当系统功能发生异常时，就会产生异常数据。快速高效地发现这些异常值，对于快速止损具有重要意义。对此，我们提出一种基于时间序列的异常识别模型，用来及时发现异常。</p><p>对于多数系统，一般都有成功率、流量等指标，故障发生时，这些指标也会出现响应的异常。我们将系统成功率、流量统一称为特征值变量，并对其进行建模，从而方便后续其它特征变量的扩展。为了更好地感知这些特征变量的突变，需要对特征变量进行计算处理或者空间转换。那么异常识别问题就转换为以下两个问题：</p><ul><li>特征变量的计算处理和转换</li><li>突变的判断</li></ul><p>针对这两个关键问题，我们将在下文中进行建模和分析。</p><h2 id="异常识别">2. 异常识别</h2><p>如下图，通过计算器进行特征变量的计算处理和转换，通过异常检测器来判断数值的突变，从而解决上面的两个问题。其中，异常检测器由比较器和决策器组成。</p><figure><img src="/images/image-20180815093422131.png" alt="image-20180815093422131"><figcaption>image-20180815093422131</figcaption></figure><p>对于给定时间序列二维矩阵<span class="math inline">\(X=\{x^m_t∈R：∀t≥0, ∀m≥0\}\)</span> ，<span class="math inline">\(x_t^m\)</span>为<span class="math inline">\(t\)</span>时刻的第m个指标的真实数据，<span class="math inline">\(u_t^m\)</span>表示时间<span class="math inline">\(t\)</span>的<span class="math inline">\(x_t^m\)</span>的计算值，<span class="math inline">\(y_t^m\)</span>为第m个指标的输出结果，<span class="math inline">\(y_t\)</span>为整体预测结果。</p><p><span class="math inline">\(x_t^m\)</span>通过计算器得到计算值<span class="math inline">\(u_t^m\)</span>，然后<span class="math inline">\(x_t^m\)</span> 和 <span class="math inline">\(u_t^m\)</span>分别作为比较器的输入，得到第m个指标的输出<span class="math inline">\(y_t^m\)</span>。<span class="math inline">\(y_t^1\)</span>,<span class="math inline">\(y_t^2\)</span>…<span class="math inline">\(y_t^m\)</span>作为决策器的输入得到<span class="math inline">\(y_t\)</span>。<span class="math inline">\(y_t\)</span>是一个二元值，可以用<code>TRUE</code>（表示输出数据正常），<code>FALSE</code>（表示输入数据异常）表示。下面对计算器和检测器进行说明。</p><h3 id="计算器">2.1 计算器</h3><p><strong>计算器</strong>用来对输入值<span class="math inline">\(x_t^m\)</span> 进行计算或者空间转换，从而得到特征变量的计算值<span class="math inline">\(u_t^m\)</span>。一般情况下，特征变量具有趋势性、周期性等特征。基于这些特征，计算值的获取，可以使用以下三种方式：累计窗口均值计算器、基于趋势性的环比计算器、基于周期性的同比计算器。</p><h4 id="累积窗口均值计算器">2.1.1 累积窗口均值计算器</h4><p>输入值为<span class="math inline">\(x_t\)</span>（为了方便省略指标参数<code>m</code>），如果直接只用单个点<span class="math inline">\(x_t\)</span>的抖动来判断，受噪声影响较大。因此，使用累积窗口均值的方式：</p><p><span class="math display">\[  u(t)={\dfrac{x_t+x_{t-1}+...+x_{t-w+1}}{w}}   \tag{1}\]</span></p><p>其中，<span class="math inline">\(w\)</span>为累计窗口的大小。通过窗口平滑之后，会过滤掉尖刺等噪声。</p><h4 id="基于趋势性的计算器">2.1.2 基于趋势性的计算器</h4><p>为了描述数据的趋势性，引入环比类算法。对<span class="math inline">\(x_t\)</span>进行空间转换，得到环比，再使用检测器进行检测。</p><p><span class="math display">\[u(t)={\dfrac{x_t+x_{t-1}+...+x_{t-w+1}}{x_{t-w}+x_{t-w-1}+...+x_{t-2w+1}}}  \tag{2}\]</span></p><p>其中，分子为当前窗口<span class="math inline">\(w\)</span>内的数据，分母为上一窗口<span class="math inline">\(w\)</span>内数据。通过窗口<span class="math inline">\(w\)</span>对数据进行平滑。</p><h4 id="基于周期性的计算器">2.1.3 基于周期性的计算器</h4><p>为了描述数据的周期性，引入同比算法。当同比值过大或者过小时，认为发生故障。同比公式如下：</p><p><span class="math display">\[u(t)={\dfrac{x_t+x_{t-1}+...+x_{t-w+1}}{x_{t-kT}+x_{t-kT-1}+...+x_{t-kT-w+1}}} \tag{3} \]</span></p><p>其中<span class="math inline">\(T\)</span>为周期，<span class="math inline">\(k\)</span>表示第几个周期。一般选取<span class="math inline">\(k\)</span>为<code>1</code>、<code>7</code>、<code>30</code>，来表示昨天、上周、上个月。</p><h4 id="其他类型计算器">2.1.4 其他类型计算器</h4><p>计算器还可以使用其他算法，包括：</p><ul><li>统计类算法：包括同比、环比算法的改进，或者其他统计算法。此时，计算器的输出结果为预测值，预测值和输入值进行比较即可。</li><li>时序型算法：包含ARIMA、Holter-Winter等时序型算法。计算器的输出结果为预测值。</li><li>机器学习：根据有监督、无监督、深度学习(LSTM)等算法，训练出的模型即为计算器。此时，计算器的输出结果一般为归一化的值，根据归一化的值进行比较。</li></ul><p>这些算法，在这里不再做深入研究和阐述。</p><h3 id="异常检测器">2.2 异常检测器</h3><p>当数据出现异常时，计算值会出现较大偏差，该偏差由<strong>异常检测器</strong>来判断。<strong>异常检测器</strong>由比较器和决策器组成，计算值和真实值通过该模块后，得到最终预测结果。</p><h4 id="比较器">2.2.1 比较器</h4><p>比较器的本质是求解如下公式的过程：</p><p><span class="math display">\[f(x^m_t,u^m_t;h^m)\ \    =  \ \ boolean \tag{4}\]</span></p><p>其中，<span class="math inline">\(x^m_t\)</span>为真实值，<span class="math inline">\(u\)</span>为计算值，<span class="math inline">\(h^m\)</span>为阈值参数，<span class="math inline">\(boolean\)</span>为结果<code>TRUE/FALSE</code>。真实值已知，计算值通过计算器得到；剩下的阈值参数<span class="math inline">\(h^m\)</span>，则需要根据故障发生时的实际值进行参数估计。</p><p>很多场景下，该公式还可以简化为：<span class="math inline">\(f(u^m_t;h^m)\ \  = \ \ boolean\)</span> ，即计算值直接和阈值比较即可。</p><h5 id="比较器种类">2.2.1.1 比较器种类</h5><p>比较器有两种：相对值比较器和绝对值比较器。给定计算值<span class="math inline">\(u^m_t\)</span>和输入值<span class="math inline">\(x^m_t\)</span>，得到绝对值比较器：</p><p><span class="math display">\[f= x_t^m−u_t^m\ \    opretor  \ \ h^m  \tag{5}\]</span></p><p>其中，<span class="math inline">\(opretor\)</span>为比较操作符，比如<code>&gt; &lt; &gt;= &lt;=</code>。由于<span class="math inline">\(u_t\)</span>由<span class="math inline">\(x_t\)</span>得到，所以很多情况下公式可以简化为 $ u_t^m    opretor    h_t^m$，即确定计算值的阈值即可。</p><p>对于一些场景来说，需要捕获特征变量的相对性。因此，引入相对值比较器：</p><p><span class="math display">\[ f={\dfrac{x_t^m−u_t^m}{u_t^m}}\ \    opretor  \ \ h^m  \tag{6} \]</span></p><p>通过对相对值比较器进行阈值处理，既可以检测异常值，同时还能对期望值进行归一化。</p><h5 id="比较器阈值h的选取">2.2.1.2 比较器阈值<span class="math inline">\(h\)</span>的选取</h5><p>一般情况下，阈值参数决定了异常检测模块的敏感度。最优阈值的选择，取决于数据分布的性质以及先验数据。一般情况下，<strong>阈值的选取方法为</strong>：</p><ul><li>方法一：跟踪一组故障数据和正常数据，根据经验估计阈值。</li><li>方法二：跟踪一组故障数据和正常数据，根据经验，并结合<code>3σ准则</code>确定，来确定阈值。（特征变量或者特征变量的组合，服从正态分布）</li></ul><h4 id="决策器">2.2.2 决策器</h4><p>如下公式，基于逻辑操作符，对比较器结果进行合并.</p><ul><li>方式一：逻辑组合</li></ul><p><span class="math display">\[   y_t=y_t^1 \ \   \&amp;| \ \  y_t^2  \ \ \&amp;| \ \  y_t^3 \ \  \&amp;| \ \  ...  \ \  y_t^m \tag{7}\]</span></p><p>其中，<span class="math inline">\(|\)</span>表示逻辑或操作，<span class="math inline">\(\&amp;\)</span>表示逻辑与操作。</p><ul><li><p>方式二：权重设置法</p><p><span class="math display">\[y_t=k_1*y_t^1 \ \   + \ \  k_2*y_t^2  \ \ + \ \  k_3*y_t^3 \ \  + \ \  ...  \ \ k_m* y_t^m \tag{8}\]</span></p></li></ul><p>其中，<span class="math inline">\(k_m\)</span>为系数，这种方式一般适合基本无负样本的场景，参数的确定需要使用<code>层次分析法</code>，将在后面的文章进行说明。</p><h2 id="故障止损">3. 故障止损</h2><p>上面主要阐述了异常识别的方式。如果条件过于严格，刚开始并不容易被识别出来；如果条件过松，可能导致误识别。对此，我们将止损策略分为两级：</p><ul><li>级别一：预警。对于不能完全确定故障发生的场景，使用级别一。</li><li>级别二：预警+止损（踢IDC）。对于能确定IDC故障的场景，使用级别二。</li></ul><h2 id="实际场景应用">4. 实际场景应用</h2><p>下面通过一个规则的场景，进行举例说明。假如存在如下异常场景：</p><figure><img src="/images/image-20181108211144340.png" alt="image-20181108211144340"><figcaption>image-20181108211144340</figcaption></figure><p><strong>体现在模型中，则级别一（预警）的模型图</strong></p><figure><img src="/images/image-20180815093336600.png" alt="image-20180815093336600"><figcaption>image-20180815093336600</figcaption></figure><p><strong>级别二（预警+踢IDC）的模型图：</strong></p><figure><img src="/images/image-20180815093352931.png" alt="image-20180815093352931"><figcaption>image-20180815093352931</figcaption></figure><p>最终，得到故障识别规则：</p><ul><li>级别一触发条件: <span class="math inline">\(u_1&lt;h_1 \ \  | \ \  (u5&lt;h5 \ \  \&amp; \ \  u6&lt;h6 \ \  \&amp; \ \  u7&lt;h7 )\)</span></li><li>级别二触发条件：<span class="math inline">\(u_1&lt;h_2 \ \  \&amp; \ \  u_3&gt;h_3​\)</span></li></ul><p>其中，<span class="math inline">\(h_1, h_2,h_3,h_5,h_6,h_7\)</span>为阈值参数。需要结合经验和实际数据估计得到。</p><h2 id="小结">5. 小结</h2><p>本文主要基于时间序列的数据，提出了异常场景识别模型，并重点对基于规则的识别进行了说明。</p><h2 id="参考">参考</h2><ul><li><a href="http://dl.acm.org/citation.cfm?id=2788611" target="_blank" rel="noopener">Generic and Scalable Framework for Automated Time-series Anomaly Detection</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;1. 概述&lt;/h2&gt;
&lt;p&gt;大型集群系统中，可能存在软件问题和硬件问题导致的系统故障，严重影响了系统的高可用性。这就要求7*24小时，对系统不间断监控。这就意味着需要不间断地监控大量时间序列数据，以便检测系统潜在的故障和异常现象。然而，实际当中的系统异常很
      
    
    </summary>
    
      <category term="数据驱动" scheme="http://www.datadriven.top/categories/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8/"/>
    
    
      <category term="数据驱动" scheme="http://www.datadriven.top/tags/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8/"/>
    
      <category term="时间序列" scheme="http://www.datadriven.top/tags/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97/"/>
    
      <category term="异常识别" scheme="http://www.datadriven.top/tags/%E5%BC%82%E5%B8%B8%E8%AF%86%E5%88%AB/"/>
    
      <category term="模型" scheme="http://www.datadriven.top/tags/%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>各公司智能运维 AIOPs架构分析和对比</title>
    <link href="http://www.datadriven.top/2018/07/08/%E5%90%84%E5%85%AC%E5%8F%B8%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4-AIOPs%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90%E5%92%8C%E5%AF%B9%E6%AF%94/"/>
    <id>http://www.datadriven.top/2018/07/08/各公司智能运维-AIOPs架构分析和对比/</id>
    <published>2018-07-08T12:35:06.000Z</published>
    <updated>2018-11-13T13:49:52.784Z</updated>
    
    <content type="html"><![CDATA[<h1 id="各公司智能运维-aiops架构分析和对比">各公司智能运维 AIOPs架构分析和对比</h1><p>[TOC]</p><h2 id="数据驱动的智能运维平台"><a href="https://mp.weixin.qq.com/s/mIB3fPME1MyO67wNQW74Lg" target="_blank" rel="noopener">数据驱动的智能运维平台</a> ★★★★★</h2><h3 id="什么是aipos">什么是AIPOs</h3><p>在定义 AIOps 时画了一张图，除了中间有机器学习、BigData、Platform 外，外层的内容就是监管控，这也就是做 AIOps 的目的。只不过是在做监管控时，要使用一些新的方式，以减轻运维的工作量。</p><figure><img src="/images/QQ20180703-164936.jpg" alt="QQ20180703-164936"><figcaption>QQ20180703-164936</figcaption></figure><h3 id="机器学习的运用算法分类">机器学习的运用算法分类</h3><p>这个可以用来做系统设计的参考</p><figure><img src="/images/QQ20180703-163744.jpg" alt="QQ20180703-163744"><figcaption>QQ20180703-163744</figcaption></figure><h3 id="典型应用场景">典型应用场景</h3><p>主要包含：时序预测、异常检测、模式聚类。</p><h4 id="时序预测">时序预测</h4><p>（eg：Holt-Winters），开源选择：</p><figure><img src="/images/QQ20180703-164402.jpg" alt="QQ20180703-164402"><figcaption>QQ20180703-164402</figcaption></figure><h4 id="异常检测">异常检测</h4><p>分析思路：Basic 采用的是四分位方法，Agile 用的是 SARIMA 算法，Robust 用的是趋势分解，Adaptive 在我看起来，采用的是 sigma 标准差。如下图：</p><figure><img src="/images/QQ20180703-171236.jpg" alt="QQ20180703-171236"><figcaption>QQ20180703-171236</figcaption></figure><p>对于异常检测的开源库选择，有些是原子的，有些是组合的。Etsy 的 skyline 是比较高级的场景，里面带有数据存储、异常检测分析、告警等；Twitter、Netflix、Numenta 是纯粹的机器学习算法库，没有任何附加内容；Yahoo 的 egads 库可以算是异常检测的原子场景，比 Twitter 和 Netflix 层级稍高。开源选择：</p><figure><img src="/images/QQ20180703-172107.jpg" alt="QQ20180703-172107"><figcaption>QQ20180703-172107</figcaption></figure><p>另外这部分，还提到了很多场景案例的分析和对比。</p><p>监控的目标需要直达业务结果，业务量下跌即为出现故障，虽然故障可能不是由于系统本身引起，但仍需要发现并定位该故障。如此将一个对业务的监控通过以下流程进行转换，首先对故障进行等级定义，<strong>对时间序列的业务指标监控，定位异常点，做出故障通告</strong>。</p><h4 id="模式聚类">模式聚类</h4><p>第一个，像 Num、Date、IP、ID 等都是运维 IT 日志里一定会出现的，但在关注模式时不会关注这些。因此，可以在开始就把这些信息替换，节省工作量。</p><p>第二个是对齐，对齐也是耗资源的，如何减少对齐的时候强行匹配资源呢？可以开始先走一个距离极其小的聚类，这样每一类中的原始文本差异非常小。此时意味着第二步得到的最底层聚类去做对齐时，在这个类里的对齐耗损就会非常小，可以直接做模式发现。</p><p>到第四步的时候，虽然还是聚类，但是消耗的资源已经非常少，因为给出的数据量已经很小，可以快速完成整个速度的迭代。</p><figure><img src="/images/QQ20180703-173508.jpg" alt="QQ20180703-173508"><figcaption>QQ20180703-173508</figcaption></figure><p>##《企业级 AIOps 实施建议》白皮书 ★★★★★</p><p>AIOPs的白皮书，关键内容：</p><h3 id="能力框架">能力框架</h3><h4 id="学件抽象">学件抽象</h4><p>学件，亦称 AI 运维组件，类似程序中的 API 或公共库，但 API 及公共库不含具体业务数据，只是某种算法，而 AI 运维组件（或称学件），则是在类似 API 的基础上，兼具对某个运维场景智能化解决的“记忆”能力，将处理这个场景的智能规则保存在了这个组件中。学件（Learnware）= 模型（ model ） + 规 约 （ specification ） ， 具 有 可 重 用 、 可 演 进 、 可 了 解 的 特 性 。</p><p>学件类似于百度AIOPs的机器人，但是，角度不同，含义也不同。</p><h4 id="能力分级">能力分级</h4><figure><img src="/images/QQ20180703-174341.jpg" alt="QQ20180703-174341"><figcaption>QQ20180703-174341</figcaption></figure><h4 id="整体架构">整体架构</h4><figure><img src="/images/QQ20180703-174440.jpg" alt="QQ20180703-174440"><figcaption>QQ20180703-174440</figcaption></figure><ul><li>“可重用”的特性使得能够获取大量不同的样本；</li><li>“可演进”的特性使得可以适应环境的变化；</li><li>“可了解”的特性使得能有效地了解模型的能力。</li></ul><h4 id="关键运维场景和经历阶段">关键运维场景和经历阶段</h4><p>见百度部分的场景图</p><figure><img src="/images/QQ20180705-142858.jpg" alt="QQ20180705-142858"><figcaption>QQ20180705-142858</figcaption></figure><h3 id="平台架构">平台架构</h3><h4 id="整体功能架构模块">整体功能架构(模块)</h4><figure><img src="/images/QQ20180703-174801.jpg" alt="QQ20180703-174801"><figcaption>QQ20180703-174801</figcaption></figure><h4 id="ai部分架构">AI部分架构</h4><figure><img src="/images/QQ20180703-174949.jpg" alt="QQ20180703-174949"><figcaption>QQ20180703-174949</figcaption></figure><p>如上面的两张架构图，具体的工具或者产品应具备以下功能或模块：</p><ul><li><ol type="1"><li>交互式建模功能：该功能支持用户在平台上交互式的进行模型的开发调试，通过简单的方法配置完成模型的构建。<br></li></ol></li><li><ol start="2" type="1"><li>算法库：用户可以在算法库中找到常见常用的算法直接使用，算法按照用途分类， 以供用户方便的使用。<br></li></ol></li><li><ol start="3" type="1"><li>样本库：样本库用于管理用户的样本数据，供用户建模时使用，支持样本的增删改查等基本操作。<br></li></ol></li><li><ol start="4" type="1"><li>数据准备：该功能支持用户对数据进行相关的预处理操作，包括关联、合并、分支路由、过滤等。<br></li></ol></li><li><ol start="5" type="1"><li>灵活的计算逻辑表达：在基本常用的节点功能之外，用户还需要自由的表达一些计算逻辑，该需求主要是通过让用户写代码或表达式来支持。<br></li></ol></li><li><ol start="6" type="1"><li>可扩展的底层框架支持：平台本身要能够灵活的支持和兼容多种算法框架引擎，如Spark、TensorFlow 等，以满足不同的场景以及用户的需求。<br></li></ol></li><li><ol start="7" type="1"><li>数据分析探索：该功能是让用户能够方便快捷地了解认识自己的数据，用户只有基于对数据充分的认识与理解，才能很好的完成模型的构建。</li></ol></li><li><ol start="8" type="1"><li>模型评估：对模型的效果进行评估的功能，用户需要依据评估的结论对模型进行调整。</li></ol></li><li><ol start="9" type="1"><li>参数以及算法搜索：该功能能够自动快速的帮助用户搜索算法的参数，对比不同的算法，帮助用户选择合适的算法以及参数，辅助用户建模。</li></ol></li><li><ol start="10" type="1"><li>场景模型：平台针对特定场景沉淀的解决方案，这些场景都是通用常见的，用户可以借鉴参考相关的解决方案以快速的解决实际问题</li></ol></li><li><ol start="11" type="1"><li>实验报告：模型除了部署运行，相关挖掘出来的结论也要能够形成报告，以供用户导出或动态发布使用。</li></ol></li><li><ol start="12" type="1"><li>模型的版本管理：模型可能有对个不同的版本，线上运行的模型实例可能分属各个不同的版本，版本管理支持模型不同版本构建发布以及模型实例版本切换升级等。</li></ol></li><li><ol start="13" type="1"><li>模型部署应用：模型构建完成后需要发布应用，模型部署应用功能支持模型的实例化，以及相关计算任务的运行调度管理。</li></ol></li><li><ol start="14" type="1"><li>数据质量保障：全链路的数据监控，能够完整的掌控数据的整个生命周期，具备对丢失的数据执行回传补录的能力，保障数据的可用性。</li></ol></li></ul><h3 id="aiops的团队角色">AIOPs的团队角色</h3><figure><img src="/images/QQ20180703-175610.jpg" alt="QQ20180703-175610"><figcaption>QQ20180703-175610</figcaption></figure><h3 id="常见场景和分类">常见场景和分类</h3><p>效率提升、质量保障、成本管理。我们的重点在于质量管理，还有一部分的效率提升。</p><figure><img src="/images/QQ20180703-175704.jpg" alt="QQ20180703-175704"><figcaption>QQ20180703-175704</figcaption></figure><h3 id="个场景的五个阶段">3个场景的五个阶段</h3><p>详见附件中的表格。</p><h3 id="实施和关键技术">实施和关键技术</h3><p>数据平台、需要用到的关键算法。—重点在于怎么对算法进行抽象，得到算法的公共层，从而各个垂直场景中都可以使用。</p><figure><img src="/images/QQ20180703-195515.jpg" alt="QQ20180703-195515"><figcaption>QQ20180703-195515</figcaption></figure><h3 id="应用案例">应用案例：</h3><ul><li>时间序列异常检测（腾讯）：有监督算法+无监督算法。</li><li>根源告警（京东）</li><li>单机房故障自愈（百度）：详见<code>调度</code>部分</li></ul><h3 id="面向-aiops-的算法技术">面向 AIOps 的算法技术</h3><p>运维场景通常无法直接基于通用的机器学习算法以黑盒的方式解决，因此需要一些面向AIOps 的算法技术，作为解决具体运维场景的基础。有时一个算法技术还可用于支撑另外一个算法技术。 常见的面向 AIOps 的算法技术包括：</p><ul><li><ol type="1"><li>指标趋势预测：通过分析指标历史数据，判断未来一段时间指标趋势及预测值，常见有 <code>Holt-Winters</code>、时序数据分解、<code>ARIMA</code> 等算法。该算法技术可用于异常检测、容量预测、容量规划等场景。</li></ol></li><li><ol start="2" type="1"><li>指标聚类: 根据曲线的相似度把多个 KPI 聚成多个类别。该算法技术可以应用于大规模的指标异常检测：在同一指标类别里采用同样的异常检测算法及参数，大幅降低训练和检测开销。常见的算法有 <code>DBSCAN</code>, <code>K-medoids</code>, <code>CLARANS</code> 等，应用的挑战是数据量大，曲线模式复杂。</li></ol></li><li><ol start="3" type="1"><li>多指标联动关联挖掘: 多指标联动分析判断多个指标是否经常一起波动或增长。该算法技术可用于构建故障传播关系，从而应用于故障诊断。常见的算法有 <code>Pearson correlation</code>, <code>Spearman correlation</code>, <code>Kendall correlation</code> 等，应用的挑战为 KPI 种类繁多，关联关系复杂。</li></ol></li><li><ol start="4" type="1"><li>指标与事件关联挖掘: 自动挖掘文本数据中的事件与指标之间的关联关系（ 比如在程序 A 每次启动的时候 CPU 利用率就上一个台阶）。该算法技术可用于构建故障传播关系，从而应用于故障诊断。常见的算法有 <code>Pearson correlation</code>, <code>J-measure</code>, <code>Two-sample test</code> 等，应用的挑战为事件和 KPI 种类繁多，KPI 测量时间粒度过粗会导致判断相关、先后、单调关系困难。</li></ol></li><li><ol start="5" type="1"><li>事件与事件关联挖掘: 分析异常事件之间的关联关系，把历史上经常一起发生的事件关联在一起。该算法技术可用于构建故障传播关系，从而应用于故障诊断。常见的算法有 <code>FP-Growth</code>, <code>Apriori</code>，<code>随机森林</code>等，但前提是异常检测需要准确可靠。</li></ol></li><li><ol start="6" type="1"><li>故障传播关系挖掘：融合文本数据与指标数据，基于上述多指标联动关联挖掘、指标与事件关联挖掘、事件与事件关联挖掘等技术、由 tracing 推导出的模块调用关系图、辅以服务器与网络拓扑，构建组件之间的故障传播关系。该算法技术可以应用于故障诊断，其有效性主要取决于其基于的其它技术。</li></ol></li></ul><h2 id="百度智能运维">百度智能运维 ★★★★★</h2><p>参考：</p><ul><li>百度智能运维AIOPs技术沙龙关键PPT.pdf</li><li><a href="https://mp.weixin.qq.com/s/mLDKcQpmRJI0gCpW1yEcNQ" target="_blank" rel="noopener">百度智能运维的技术演进之路</a></li><li><a href="https://mp.weixin.qq.com/s/iq2-Y0QQCm6Z5cZ25_QKew" target="_blank" rel="noopener">百度智能运维 | 框架在手，AI我有</a></li></ul><h3 id="智能运维分级标准阶段">智能运维分级标准(阶段)</h3><figure><img src="/images/QQ20180703-203026.jpg" alt="QQ20180703-203026"><figcaption>QQ20180703-203026</figcaption></figure><h3 id="智能运维架构场景象限划分">智能运维架构——场景象限划分</h3><p><code>高频 X 复杂</code></p><figure><img src="/images/image-20180702142018403.jpg" alt="image-20180702142018403"><figcaption>image-20180702142018403</figcaption></figure><h3 id="智能运维架构工程思想">智能运维架构——工程思想</h3><figure><img src="/images/image-20180702143030520.jpg" alt="image-20180702143030520"><figcaption>image-20180702143030520</figcaption></figure><h3 id="运维工程架构整体框架">运维工程架构——整体框架</h3><figure><img src="/images/image-20180702141741650.jpg" alt="image-20180702141741650"><figcaption>image-20180702141741650</figcaption></figure><h3 id="智能运维架构技术栈技术架构">智能运维架构——技术栈(技术架构)</h3><figure><img src="/images/image-20180702143355014.jpg" alt="image-20180702143355014"><figcaption>image-20180702143355014</figcaption></figure><h4 id="运维知识库运维数据">4.1 运维知识库（运维数据）</h4><p>所有要处理的数据都来自知识库，以及所有处理后的数据也都会再进入到知识库中。知识库由三部分组成，分别是分为元数据(MetaDB)、状态数据(TSDB)和事件数据(EventDB)。持续的数据建设，是智能运维建设的关键。</p><figure><img src="/images/QQ20180703-202209.jpg" alt="QQ20180703-202209"><figcaption>QQ20180703-202209</figcaption></figure><h4 id="运维工程研发框架">4.2 运维工程研发框架</h4><p>每个运维智能操作都可以分解成感知、决策、执行这样一个标准流程，这样一个流程叫做智能运维机器人。运维工程研发框架提供感知、决策、执行过程常用的组件，便于用户快速构建智能运维机器人。</p><figure><img src="/images/image-20180702144434325.jpg" alt="image-20180702144434325"><figcaption>image-20180702144434325</figcaption></figure><ul><li><strong>1、感知方面，</strong>智能异常检测算法替代过去大量误报漏报的阈值检测方法；</li><li><strong>2、决策方面，</strong>具备全局信息、自动决策的算法组件替代了过去“老中医会诊”的人工决策模式；</li><li><strong>3、执行方面，</strong>状态机等执行长流程组件的加入，让执行过程可定位、可复用。</li></ul><figure><img src="/images/image-20180627160312805.png" alt="image-20180627160312805"><figcaption>image-20180627160312805</figcaption></figure><ul><li><p><strong>感知器是运维机器人的眼睛和耳朵。</strong>就像人有两个眼睛和两个耳朵一样。运维机器人也可以挂载多个感知器来获取不同事件源的消息，比如监控的指标数据或者是报警事件，变更事件这些，甚至可以是一个定时器。这些消息可以以推拉两种方式被感知器获取到。这些消息也可以做一定的聚合，达到阈值再触发后续处理。</p></li><li><p><strong>决策器是运维机器人的大脑，</strong>所以为了保证决策的唯一，机器人有且只能有一个决策器。决策器也是使用者主要要扩展实现的部分。除了常见的逻辑判断规则之外，未来我们还会加入决策树等模型，让运维机器人自主控制决策路径。</p></li><li><p><strong>执行器是运维机器人的手脚，</strong>所以同样的，执行器可以并行的执行多个不同的任务。执行器将运维长流程抽象成状态机和工作流两种模式。这样框架就可以记住当前的执行状态，如果运维机器人发生了故障迁移，还可以按照已经执行的状态让长流程断点续起。</p></li></ul><h4 id="运维开发框架">4.3 运维开发框架</h4><figure><img src="/images/QQ20180703-201909.jpg" alt="QQ20180703-201909"><figcaption>QQ20180703-201909</figcaption></figure><p>把线上环境看做一个黑盒服务，那么我们对它的操作无非读写两类:</p><ul><li>所谓的写也就是操作控制流，是那种要对线上状态做一些改变的操作，我们常说的部署、执行命令，都属于这一类；</li><li>另一类是读，指的是数据流，也就是要从线上获取状态数据，并进行一些聚合统计之类的处理，我们常说的指标汇聚、异常检测、报警都在这个里面。</li></ul><p>通过运维知识库，可以在这两种操作的基础上，封装出多种不同的运维机器人，对业务提供高效率、高质量以及高可用方面的能力。</p><h4 id="运维大脑">4.3 运维大脑</h4><p>运维大脑包括异常检测和故障诊断，这两个部分的共同基础是基本的恒定阈值异常检测算法。恒定阈值异常检测算法利用多种概率模型估计数据的概率分布，并由此产生报警阈值。</p><figure><img src="/images/QQ20180704-100911.jpg" alt="QQ20180704-100911"><figcaption>QQ20180704-100911</figcaption></figure><h3 id="智能运维实践故障管理解决方案">智能运维实践——故障管理解决方案</h3><p>故障预防 —&gt;故障发现 —&gt;故障自愈 —&gt;...<img src="/images/image-20180702150434675.jpg" alt="image-20180702150434675"></p><h4 id="故障预防">5.1故障预防</h4><p>自动拦截异常变更。 —现阶段我们主要是通过人工流程来预防。</p><h4 id="故障发现">5.2 故障发现</h4><p>算法自动选择：</p><figure><img src="/images/QQ20180704-101456.jpg" alt="QQ20180704-101456"><figcaption>QQ20180704-101456</figcaption></figure><h4 id="故障自愈">5.3 故障自愈</h4><figure><img src="/images/image-20180702203156159.jpg" alt="image-20180702203156159"><figcaption>image-20180702203156159</figcaption></figure><h2 id="百度异常检测">百度异常检测 ★★★★★</h2><p>参考</p><ul><li><p>百度智能运维实践之异常检测.pdf</p></li><li><p><a href="https://blog.csdn.net/g2v13ah/article/details/78474370" target="_blank" rel="noopener">异常检测：百度是这样做的</a></p></li></ul><h3 id="异常检测-1">异常检测</h3><h4 id="流程">流程</h4><figure><img src="/images/image-20180627204634628.png" alt="image-20180627204634628"><figcaption>image-20180627204634628</figcaption></figure><h4 id="系统架构">系统架构</h4><figure><img src="/images/image-20180702113603340.png" alt="image-20180702113603340"><figcaption>image-20180702113603340</figcaption></figure><h4 id="恒定阈值算法">恒定阈值算法</h4><p>累计恒定阈值 用来排除单点抖动。</p><h4 id="突升突降算法">突升突降算法</h4><p>r空间：引入周期内累计值</p><h4 id="同比算法">同比算法</h4><p>z空间：引入分布和数据标准化。 参考：<a href="https://blog.csdn.net/bbbeoy/article/details/70185798" target="_blank" rel="noopener">三种常用数据标准化方法</a></p><h3 id="时序数据存储及计算职责">时序数据存储及计算职责</h3><p>使用专门的时序型数据库。</p><figure><img src="/images/image-20180702110645458.png" alt="image-20180702110645458"><figcaption>image-20180702110645458</figcaption></figure><h4 id="聚合计算实现">聚合计算实现</h4><p>支持场景聚合函数<code>sum/avg/...</code></p><figure><img src="/images/image-20180702112558001.png" alt="image-20180702112558001"><figcaption>image-20180702112558001</figcaption></figure><h4 id="二次计算实现">二次计算实现</h4><figure><img src="/images/image-20180702112434310.png" alt="image-20180702112434310"><figcaption>image-20180702112434310</figcaption></figure><h2 id="百度运维大数据存储平台设计与实践.pdf">百度运维大数据存储平台设计与实践.pdf ★★★★</h2><p>重点分析了物理层的设计，包括：</p><ul><li>大规模时序数据的存储：层次存储结构：Hadoop--冷数据；Hbase—暖数据；Redis--热数据。</li><li>海量运维事件数据存储；EventDB</li><li>知识库（元数据管理）：系统架构如下图：</li></ul><figure><img src="/images/image-20180702111933197.jpg" alt="image-20180702111933197"><figcaption>image-20180702111933197</figcaption></figure><h2 id="智能运维-单机房故障自愈收藏这一篇就够了"><a href="https://mp.weixin.qq.com/s/m5q8A-0-dHvV_dDFKov_0g" target="_blank" rel="noopener">智能运维 | 单机房故障自愈，收藏这一篇就够了</a> ★★★★★</h2><h3 id="单机房故障止损的能力标准">单机房故障止损的能力标准</h3><figure><img src="/images/QQ20180703-203917.jpg" alt="QQ20180703-203917"><figcaption>QQ20180703-203917</figcaption></figure><h3 id="故障自愈的技术架构">故障自愈的技术架构</h3><figure><img src="/images/QQ20180703-203941.jpg" alt="QQ20180703-203941"><figcaption>QQ20180703-203941</figcaption></figure><h3 id="故障自愈的算法">故障自愈的算法</h3><p><strong>基于容量水位的动态均衡</strong>（这个可能是BFE模型中的一部分）。智能IDC、专线路由都可以参考这个方案。</p><ol type="1"><li>可以参考《企业级 AIOps 实施建议》白皮书中的 <code>案例3</code></li></ol><ul><li><p><strong>基于容量水位的动态均衡</strong></p><figure><img src="/images/QQ20180703-204031.jpg" alt="QQ20180703-204031"><figcaption>QQ20180703-204031</figcaption></figure></li><li><p><strong>基于快速熔断的过载保护</strong></p></li></ul><p>在流量调度时，建立快速的熔断机制作为防止服务过载的最后屏障。一旦出现过载风险，则快速停止流量调度，降低次生故障发生的概率。</p><ul><li><strong>基于降级功能的过载保护</strong></li></ul><p>在流量调度前，如果已经出现对应机房的容量过载情况，则动态联动对应机房的降级功能，实现故障的恢复。</p><h2 id="流量调度">流量调度 ★★★★★</h2><p>参考：百度智能运维AIOPs技术沙龙关键PPT</p><ol type="1"><li>流量调度部分：重点在于模型的定义。</li></ol><figure><img src="/images/image-20180702204024368.jpg" alt="image-20180702204024368"><figcaption>image-20180702204024368</figcaption></figure><h2 id="外卖订单量预测异常报警模型实践"><a href="https://tech.meituan.com/order-holtwinter.html" target="_blank" rel="noopener">外卖订单量预测异常报警模型实践</a> ★★★★★</h2><p>这篇介绍了Holt-Winters算法在订单异常检测中的应用。文章关键点在于： 1. 之前问过百度的智能运维，说曾经用过该算法，效果不是很好。但在该文章中效果看似还不错。其原因可能在，该文章对Holt-Winters算法进行了2种方式的精简和改进；这点值得借鉴。 2. 给出了异常检测模型，及模型内的各部件关系。 3. 给出了报警模型结构图。从该图可以看出，要实现整体过程，需要离线计算模块，这一块依赖于Hadoop，是我们还不具备的能力。</p><h3 id="异常检测模型">异常检测模型</h3><p>基于预测的异常检测模型如下图所示，<em>xt</em>是真实数据，通过预测器得到预测数据，然后<em>xt</em>和<em>pt</em>分别作为比较器的输入，最终得到输出<em>yt</em>。<em>yt</em>是一个二元值，可以用+1（+1表示输入数据正常），-1（-1表示输入数据异常）表示。</p><figure><img src="/images/QQ20180703-204703.jpg" alt="QQ20180703-204703"><figcaption>QQ20180703-204703</figcaption></figure><h3 id="预测器">预测器</h3><figure><img src="/images/QQ20180703-204909.jpg" alt="QQ20180703-204909"><figcaption>QQ20180703-204909</figcaption></figure><h3 id="比较器模型">比较器模型</h3><figure><img src="/images/QQ20180703-205017.jpg" alt="QQ20180703-205017"><figcaption>QQ20180703-205017</figcaption></figure><ul><li><strong>离散度Filter</strong>：根据预测误差曲线离散程度过滤出可能的异常点。一个序列的方差表示该序列离散的程度，方差越大，表明该序列波动越大。如果一个预测误差序列方差比较大，那么我们认为预测误差的报警阈值相对大一些才比较合理。离散度Filter利用了这一特性，取连续15分钟的预测误差序列，分为首尾两个序列（e1,e2），如果两个序列的均值差大于e1序列方差的某个倍数，我们就认为该点可能是异常点。</li><li><strong>阈值Filter</strong>：根据误差绝对值是否超过某个阈值过滤出可能的异常点。利用离散度Filter进行过滤时，报警阈值随着误差序列波动程度变大而变大，但是在输入数据比较小时，误差序列方差比较小，报警阈值也很小，容易出现误报。所以设计了根据误差绝对值进行过滤的阈值Filter。阈值Filter设计了一个分段阈值函数<em>y</em>=<em>f</em>(<em>x</em>)，对于实际值<em>x</em>和预测值<em>p</em>，只有当|<em>x</em>-<em>p</em>|&gt;<em>f</em>(<em>x</em>)时报警。实际使用中，可以寻找一个对数函数替换分段阈值函数，更易于参数调优。</li></ul><h3 id="异常预警模型整体结构图">异常预警模型整体结构图</h3><p>最终的外卖订单异常报警模型结构图如图所示，每天会有定时Job从ETL中统计出最近10天的历史订单量，经过预处理模块，去除异常数据，经过周期数据计算模块得到周期性数据。对当前时刻预测时，取60分钟的真实数据和周期性数据，经过实时预测模块，预测出当前订单量。将连续15分钟的预测值和真实值通过比较器，判断当前时刻是否异常。</p><figure><img src="/images/QQ20180703-205152.jpg" alt="QQ20180703-205152"><figcaption>QQ20180703-205152</figcaption></figure><h2 id="阿里智能运维">阿里智能运维 ★★★★</h2><h3 id="运维体系职责">运维体系职责</h3><figure><img src="/images/QQ20180704-154755.jpg" alt="QQ20180704-154755"><figcaption>QQ20180704-154755</figcaption></figure><blockquote><p><strong>自动化是智能化的前提</strong>: 我认为智能化最重要的前提是自动化。如果你的系统还没有完成自动化的过程，我认为就不要去做智能化，你还在前面的阶段。智能化非常多的要求都是自动化，如果不够自动化，意味着后边看起来做了一个很好的智能化的算法等等，告诉别人我能给你很大的帮助，结果发现前面自动化过程还没有做完全。</p></blockquote><h3 id="阿里巴巴智能化运维五步走">阿里巴巴智能化运维五步走</h3><p>在运维这五个领域，我们看到的一些可能性，包括我们正在做的事情。</p><figure><img src="/images/QQ20180704-155008.jpg" alt="QQ20180704-155008"><figcaption>QQ20180704-155008</figcaption></figure><ul><li><strong>资源</strong><ul><li><strong>DC大脑，让控制更加智能化</strong>：谷歌的一篇文章，里面有一个消息透露，他们通过更好的智能化，去控制整个机房的智能等等。比如说控制空调的出口，就是风向往哪边吹，控制这个，然后为谷歌节省了非常多的钱，非常可观。</li><li><strong>资源画像让资源更好搭配</strong></li></ul></li><li><strong>监控AI化</strong><ul><li><strong>智能报警</strong>：最火的领域。阿里在尝试的一个方向是让你不要去配，阿里根据分析来决定什么情况下需要报警。</li><li><strong>异常检测直接影响到效率</strong>：很多公司都在做。</li></ul></li><li><strong>用智能化做好故障定位</strong></li></ul><h3 id="智能运维整体架构统一的大数据运营平台">智能运维整体架构——统一的大数据运营平台</h3><figure><img src="/images/QQ20180705-151848.jpg" alt="QQ20180705-151848"><figcaption>QQ20180705-151848</figcaption></figure><h3 id="智能运维异常检测算法架构">智能运维——异常检测算法架构</h3><h4 id="整体算法框架">整体算法框架</h4><p>整体算法框架如下图所示。</p><ul><li>首先对数据进行预处理，包括差值补缺和平滑去噪。</li><li>然后基于优化后的时间序列分解Seanonal Trend LOESS方法进行基线拟合，滑动平均使曲线平滑。</li><li>然后结合时间序列分析、机器学习以及特征工程中的各种方法，判断一个时间片段是否需要报警。<ul><li>开始设计时并未确定该算法应采取哪些方法，而是被阿里巴巴各行业的业务、形态各异的数据以及判断标准训练出来的。它的优势在于对各行业的数据有较高的适配性，对非技术性的曲线波动有较强的抗干扰能力。</li><li>此外，该算法会输出拟合的基线，并且内部系统中可以通过该基线提前100分钟预测趋势，当然距离越近的预测越准确，预测时会将历史波动和局部变化趋势都考虑在内，每个瞬间都会判断这个时刻是否需要报警。</li></ul></li><li>出现报警后，可以回溯到该报警的开始时间和结束时间。由此达到整体的报警功能。</li></ul><figure><img src="/images/QQ20180704-174605.jpg" alt="QQ20180704-174605"><figcaption>QQ20180704-174605</figcaption></figure><h4 id="集群智能分析架构-异常分析">集群智能分析架构-异常分析</h4><figure><img src="/images/QQ20180705-152108.jpg" alt="QQ20180705-152108"><figcaption>QQ20180705-152108</figcaption></figure><h3 id="智能根因推荐">智能根因推荐</h3><p>分析流程</p><figure><img src="/images/QQ20180704-175204.jpg" alt="QQ20180704-175204"><figcaption>QQ20180704-175204</figcaption></figure><h3 id="人工智能驱动大数据">人工智能驱动大数据</h3><p>技术主要包含4部分：智能解决方案算法平台、 数据资产 管理平台、数据计算开发平台、数据采集管理平台。</p><h4 id="算法平台架构">算法平台架构</h4><p>人工智能解决方案算法平台架构：</p><figure><img src="/images/QQ20180704-162223.jpg" alt="QQ20180704-162223"><figcaption>QQ20180704-162223</figcaption></figure><h4 id="数据清洗层架构">数据清洗层架构</h4><h5 id="整体结构">整体结构</h5><p>下图中，模型层对各算法进行了抽象，分为：<strong>规则</strong>、<strong>异常检测算法</strong>、<strong>监督&amp;无监督算法</strong>、图算法等。</p><figure><img src="/images/QQ20180704-160916.jpg" alt="QQ20180704-160916"><figcaption>QQ20180704-160916</figcaption></figure><h4 id="自动化标签工厂架构">自动化标签工厂架构</h4><p>流程：基础数据 --&gt; 特征 --&gt; 算法 --&gt; 质量评估</p><figure><img src="/images/QQ20180704-163658.jpg" alt="QQ20180704-163658"><figcaption>QQ20180704-163658</figcaption></figure><h3 id="参考">参考</h3><ul><li><a href="http://www.360doc.com/content/17/1105/10/34701069_701024221.shtml" target="_blank" rel="noopener">阿里毕玄：智能时代的新运维</a></li><li><a href="https://yq.aliyun.com/articles/598336" target="_blank" rel="noopener">还不知道AIOps嘛？阿里这么火的智能运维，你不能不知道！</a></li></ul><h2 id="清华裴丹aiops落地路线图">清华裴丹:AIOps落地路线图 ★★★★</h2><h3 id="整体流程">整体流程</h3><ol type="1"><li><p><strong>AIOps引擎 中的“异常检测”模块在检测到异常之后可以将报警第一时间报给运维人员，达到“故障发现”的效果；</strong></p></li><li><p><strong>“异常定位”模块达到“故障止损”的效果，它会给出一些止损的建议，</strong>运维专家看到这个定位之后也许他不知道根因，但是他知道怎么去根据已有的预案来进行止损，然后再执行自动化的脚本。如果是软件上线导致的问题我们回卷，如果业务不允许回卷就赶紧发布更新版本；如果是容量不够了，那我们动态扩容；如果部分软硬件出问题了，我们切换一下流量等等。</p></li><li><p><strong>AIOps引擎中的“根因分析”模块会找出故障的根因，从而对其进行修复。</strong>如果根因是硬件出了问题，像慢性病一样的问题，那我们可以让我们的运维人员去修复。</p></li><li><strong>同时，AIOps 引擎中的“异常预测模块”能够提前预测性能瓶颈、容量不足、故障等，从而实现“故障规避”。</strong>比如，如果我们预测出来了设备故障的话，那么可以更新设备；如果说我们发现性能上的瓶颈是代码导致的，那就交给研发人员去修改。</li><li><p>核心的AIOps的引擎会积累一个知识库，从里边不断的学习。也就是说监控数据会给AIOps提供训练数据的基础，然后专家会反馈一部分专家知识，上图是我展望的AIOps大概的体系结构，这里面关键的一点是，我们还是离不开运维专家的。最终的止损、规避的决策、软件的代码修复以及设备的更换还是要靠人来做的，但是机器把绝大部分工作都做了，包括异常检测、异常定位、根因分析、异常预测。</p></li></ol><figure><img src="/images/QQ20180704-191853.jpg" alt="QQ20180704-191853"><figcaption>QQ20180704-191853</figcaption></figure><h3 id="ai-擅长解决问题">AI 擅长解决问题</h3><figure><img src="/images/QQ20180704-192310.jpg" alt="QQ20180704-192310"><figcaption>QQ20180704-192310</figcaption></figure><h3 id="架构技术路线图">架构（技术路线图）</h3><figure><img src="/images/QQ20180704-192434.jpg" alt="QQ20180704-192434"><figcaption>QQ20180704-192434</figcaption></figure><h3 id="各类算法对比">各类算法对比</h3><figure><img src="/images/QQ20180705-113940.jpg" alt="QQ20180705-113940"><figcaption>QQ20180705-113940</figcaption></figure><h3 id="算法产品">算法产品</h3><figure><img src="/images/QQ20180705-112614.jpg" alt="QQ20180705-112614"><figcaption>QQ20180705-112614</figcaption></figure><h4 id="异常检测-2">异常检测</h4><figure><img src="/images/QQ20180704-192520.jpg" alt="QQ20180704-192520"><figcaption>QQ20180704-192520</figcaption></figure><h4 id="故障发现-1">故障发现</h4><figure><img src="/images/QQ20180704-192606.jpg" alt="QQ20180704-192606"><figcaption>QQ20180704-192606</figcaption></figure><h4 id="故障止损">故障止损</h4><figure><img src="/images/QQ20180704-192803.jpg" alt="QQ20180704-192803"><figcaption>QQ20180704-192803</figcaption></figure><h4 id="故障修复">故障修复</h4><figure><img src="/images/QQ20180704-192914.jpg" alt="QQ20180704-192914"><figcaption>QQ20180704-192914</figcaption></figure><h3 id="参考-1">参考</h3><ul><li><a href="https://www.jianshu.com/p/126288065148" target="_blank" rel="noopener">清华裴丹分享AIOps落地路线图，看智能运维如何落地生根</a></li><li>6、裴丹-AIOps在传统行业的落地探索.pdf</li></ul><h2 id="腾讯aiops">腾讯AIOPs ★★★★★</h2><h3 id="智能运维思路">智能运维思路</h3><figure><img src="/images/QQ20180705-144343.jpg" alt="QQ20180705-144343"><figcaption>QQ20180705-144343</figcaption></figure><h3 id="整体架构-1">整体架构</h3><h3 id="运维平台整体架构">运维平台整体架构</h3><figure><img src="/images/QQ20180705-151143.jpg" alt="QQ20180705-151143"><figcaption>QQ20180705-151143</figcaption></figure><h4 id="技术架构">技术架构</h4><figure><img src="/images/QQ20180705-145301.jpg" alt="QQ20180705-145301"><figcaption>QQ20180705-145301</figcaption></figure><h4 id="服务构建">服务构建</h4><figure><img src="/images/QQ20180705-142230.jpg" alt="QQ20180705-142230"><figcaption>QQ20180705-142230</figcaption></figure><p>智能运营白皮书用于衡量智能化产品建设的水平，白皮书中定义了:</p><ul><li>三个阶段:半智能、浅智能、智能</li><li>六项能力:感知、分析、决策、执行、呈现、干预</li></ul><h3 id="时间序列异常检测">时间序列异常检测</h3><h4 id="类算法">3类算法</h4><figure><img src="/images/QQ20180705-150219.jpg" alt="QQ20180705-150219"><figcaption>QQ20180705-150219</figcaption></figure><h4 id="经典算法的使用场景armaarmaarima">经典算法的使用场景：AR/MA/ARMA/ARIMA</h4><figure><img src="/images/QQ20180705-144654.jpg" alt="QQ20180705-144654"><figcaption>QQ20180705-144654</figcaption></figure><h4 id="时间序列异常检测的技术框架">时间序列异常检测的技术框架</h4><figure><img src="/images/QQ20180705-145655.jpg" alt="QQ20180705-145655"><figcaption>QQ20180705-145655</figcaption></figure><h5 id="sigma算法和控制图算法的优缺点">3-Sigma算法和控制图算法的优缺点</h5><figure><img src="/images/QQ20180705-145900.jpg" alt="QQ20180705-145900"><figcaption>QQ20180705-145900</figcaption></figure><h5 id="无监督学习算法的优缺点">无监督学习算法的优缺点</h5><h5 id="有监督算法">有监督算法</h5><h3 id="告警收敛根源分析">告警收敛根源分析</h3><figure><img src="/images/QQ20180705-150747.jpg" alt="QQ20180705-150747"><figcaption>QQ20180705-150747</figcaption></figure><h3 id="参考-2">参考</h3><ul><li>3、涂彦-AIOps仅仅是异常检测么</li><li>avila--腾讯运维的AI实践v_0.4.pdf ★★★★★</li><li>max--复杂业务的自动化运维精髓V2.pdf</li></ul><h2 id="aiops">360 AIOps ★★★★</h2><h3 id="时序序列算法">时序序列算法</h3><h3 id="ewma指数加权移动平均">EWMA(指数加权移动平均)</h3><figure><img src="/images/QQ20180705-143453.jpg" alt="QQ20180705-143453"><figcaption>QQ20180705-143453</figcaption></figure><p>另外还有各种同环比等算法介绍</p><h3 id="机器学习架构">机器学习架构</h3><figure><img src="/images/QQ20180705-143825.jpg" alt="QQ20180705-143825"><figcaption>QQ20180705-143825</figcaption></figure><p>机器学习算法选择</p><figure><img src="/images/QQ20180705-143924.jpg" alt="QQ20180705-143924"><figcaption>QQ20180705-143924</figcaption></figure><h3 id="参考-3">参考</h3><ul><li>4、谭学士-360 AIOps 亮剑网络运维</li></ul><h2 id="华为aiops实践">华为AIOps实践 ★★★★</h2><h3 id="aiops现实中面临的痛点难点">AIOps现实中面临的痛点、难点</h3><figure><img src="/images/QQ20180705-154148.jpg" alt="QQ20180705-154148"><figcaption>QQ20180705-154148</figcaption></figure><h3 id="serverless环境中因果序列追踪">Serverless环境中因果序列追踪</h3><p>需求:</p><ul><li><p>对Serverless API调用依赖关系、异常检测、响应时间等的监控和分析</p></li><li><p>API的高频调用、临时性(使用时创建、空闲时销毁)等特性，调用链成为实</p><p>时监控的重要手段</p></li><li><p>函数trigger:提供日志事件触发函数调用的能力:1)能看到事件来源，便于跟 踪问题 2)供用户自定义功能扩展</p></li></ul><h3 id="多源数据的rca分析探索">多源数据的RCA分析探索</h3><p>为什么说异常检测是AIOps的第一需求:</p><ul><li>有异常才需要操作(Ops)<br></li><li>理解什么是正常pattern</li></ul><h4 id="单一时序变量">单一时序变量</h4><ul><li>窗口法: sliding window, xxx window。手工配置大量参数(尤其是窗口大小)，平衡延迟和误报率，即延迟短误报率会高，延迟长误报率低 。</li><li>ARIMA、EWMA)等算法</li></ul><h4 id="多个时序变量预测单一事件">多个时序变量预测单一事件</h4><ul><li>Hidden Markov Model ： 根据预设的事件依赖关系进行预测。 参数不多，计算量较小，每个节点 意义明确，但准确性非常依赖于预 设的Markov Chain</li><li>Recurrent Neural Network, e.g. LSTM ：隐变量意义不明确，参数较多，计 算复杂度较大，比较难收敛</li></ul><h3 id="聚类算法实现网络包的blackbox分析">聚类算法实现网络包的Blackbox分析</h3><ul><li>Blackbox算法 ：计算因果路径，即不同业务的服务间调用路径。</li><li>基于Hierarchical Clustering实现因果路径推导</li></ul><h3 id="参考-4">参考</h3><ul><li>1、华为-华为三位一体探索 AIOps 关键技术的实践.pdf</li></ul><h1 id="原理部分">原理部分</h1><h2 id="holt-winters模型原理">Holt-Winters模型原理</h2><ul><li><a href="https://blog.csdn.net/u010665216/article/details/78051192?locationNum=11&amp;fps=1" target="_blank" rel="noopener">Holt-Winters模型原理分析及代码实现（python)</a> ★★★★</li><li><a href="https://en.wikipedia.org/wiki/Exponential_smoothing" target="_blank" rel="noopener">Exponential smoothing</a> ★★★★</li></ul><p>这两篇文章，由浅入深，从原理上指出了，Holt-Winters为什么可以解决残差、趋势、周期的问题。</p><h2 id="holt-winters-seasonal-method"><a href="https://www.otexts.org/fpp/7/5" target="_blank" rel="noopener">Holt-Winters seasonal method</a> ★★★</h2><p>这篇文章介绍了原理，美团那篇文章中的公司参考的这里。同时，这里给出了仿真方式。</p><h2 id="holt-winters原理和初始值的确定"><a href="https://www.cnblogs.com/xuanlvshu/p/5451974.html" target="_blank" rel="noopener">Holt-Winters原理和初始值的确定</a> ★★★</h2><p>该文章介绍了参数的计算过程。同时给出来很多参考文献。</p><h2 id="facebook-prophet的探索python语言"><a href="https://blog.csdn.net/wjskeepmaking/article/details/64905745" target="_blank" rel="noopener">facebook prophet的探索（python语言）</a></h2><p>介绍了时序型的分析框架。提供了Python和R两种语言支持。</p><ol type="1"><li>可以研究下是否可以拿来使用。</li><li>吸收性该框架的思想，用到我们的设计中来。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;各公司智能运维-aiops架构分析和对比&quot;&gt;各公司智能运维 AIOPs架构分析和对比&lt;/h1&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&quot;数据驱动的智能运维平台&quot;&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/mIB3fPME1MyO67
      
    
    </summary>
    
      <category term="数据驱动" scheme="http://www.datadriven.top/categories/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8/"/>
    
    
      <category term="数据驱动" scheme="http://www.datadriven.top/tags/%E6%95%B0%E6%8D%AE%E9%A9%B1%E5%8A%A8/"/>
    
      <category term="AIOPs" scheme="http://www.datadriven.top/tags/AIOPs/"/>
    
      <category term="智能运维" scheme="http://www.datadriven.top/tags/%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4/"/>
    
      <category term="算法" scheme="http://www.datadriven.top/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="人工智能" scheme="http://www.datadriven.top/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="异常检测" scheme="http://www.datadriven.top/tags/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    
      <category term="架构" scheme="http://www.datadriven.top/tags/%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow快速入门示例</title>
    <link href="http://www.datadriven.top/2018/07/08/TensorFlow%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E7%A4%BA%E4%BE%8B/"/>
    <id>http://www.datadriven.top/2018/07/08/TensorFlow快速入门示例/</id>
    <published>2018-07-08T12:27:59.000Z</published>
    <updated>2018-11-13T13:49:19.207Z</updated>
    
    <content type="html"><![CDATA[<h1 id="设置程序">设置程序</h1><h2 id="安装最新版本的-tensorflow">安装最新版本的 TensorFlow</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install -q --upgrade tensorflow</span><br></pre></td></tr></table></figure><h2 id="配置导入和-eager-execution">配置导入和 Eager Execution</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import, division, print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br><span class="line"></span><br><span class="line">tf.enable_eager_execution()</span><br><span class="line"></span><br><span class="line">print(<span class="string">"TensorFlow version: &#123;&#125;"</span>.format(tf.VERSION))</span><br><span class="line">print(<span class="string">"Eager execution: &#123;&#125;"</span>.format(tf.executing_eagerly()))</span><br></pre></td></tr></table></figure><pre><code>TensorFlow version: 1.9.0-rc2Eager execution: True</code></pre><h1 id="导入和解析训练数据集">导入和解析训练数据集</h1><h2 id="下载数据集">下载数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_dataset_url = <span class="string">"http://download.tensorflow.org/data/iris_training.csv"</span></span><br><span class="line"></span><br><span class="line">train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),</span><br><span class="line">                                           origin=train_dataset_url)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Local copy of the dataset file: &#123;&#125;"</span>.format(train_dataset_fp))</span><br></pre></td></tr></table></figure><pre><code>Downloading data from http://download.tensorflow.org/data/iris_training.csv8192/2194 [================================================================================================================] - 0s 0us/stepLocal copy of the dataset file: /content/.keras/datasets/iris_training.csv</code></pre><h2 id="检查数据-使用-head--n5-命令查看前-5-个条目">检查数据: 使用 head -n5 命令查看前 5 个条目：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!head -n5 &#123;train_dataset_fp&#125;</span><br></pre></td></tr></table></figure><pre><code>120,4,setosa,versicolor,virginica6.4,2.8,5.6,2.2,25.0,2.3,3.3,1.0,14.9,2.5,4.5,1.7,24.9,3.1,1.5,0.1,0</code></pre><h1 id="解析数据集">解析数据集</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_csv</span><span class="params">(line)</span>:</span></span><br><span class="line">  example_defaults = [[<span class="number">0.</span>], [<span class="number">0.</span>], [<span class="number">0.</span>], [<span class="number">0.</span>], [<span class="number">0</span>]]  <span class="comment"># sets field types</span></span><br><span class="line">  parsed_line = tf.decode_csv(line, example_defaults)</span><br><span class="line">  <span class="comment"># First 4 fields are features, combine into single tensor</span></span><br><span class="line">  features = tf.reshape(parsed_line[:<span class="number">-1</span>], shape=(<span class="number">4</span>,))</span><br><span class="line">  <span class="comment"># Last field is the label</span></span><br><span class="line">  label = tf.reshape(parsed_line[<span class="number">-1</span>], shape=())</span><br><span class="line">  <span class="keyword">return</span> features, label</span><br></pre></td></tr></table></figure><h1 id="创建训练-tf.data.dataset">创建训练 tf.data.Dataset|</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = tf.data.TextLineDataset(train_dataset_fp)</span><br><span class="line">train_dataset = train_dataset.skip(<span class="number">1</span>)             <span class="comment"># skip the first header row</span></span><br><span class="line">train_dataset = train_dataset.map(parse_csv)      <span class="comment"># parse each row</span></span><br><span class="line">train_dataset = train_dataset.shuffle(buffer_size=<span class="number">1000</span>)  <span class="comment"># randomize</span></span><br><span class="line">train_dataset = train_dataset.batch(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># View a single example entry from a batch</span></span><br><span class="line">features, label = iter(train_dataset).next()</span><br><span class="line">print(<span class="string">"example features:"</span>, features[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">"example label:"</span>, label[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>example features: tf.Tensor([5.8 2.7 4.1 1. ], shape=(4,), dtype=float32)example label: tf.Tensor(1, shape=(), dtype=int32)</code></pre><h1 id="选择模型类型-使用-keras-创建模型">选择模型类型: 使用 Keras 创建模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.Sequential([</span><br><span class="line">  tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">"relu"</span>, input_shape=(<span class="number">4</span>,)),  <span class="comment"># input shape required</span></span><br><span class="line">  tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">"relu"</span>),</span><br><span class="line">  tf.keras.layers.Dense(<span class="number">3</span>)</span><br><span class="line">])</span><br><span class="line">print( model)</span><br></pre></td></tr></table></figure><pre><code>&lt;tensorflow.python.keras.engine.sequential.Sequential object at 0x7fa0842648d0&gt;</code></pre><h1 id="训练模型">训练模型</h1><h2 id="定义损失和梯度函数">定义损失和梯度函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(model, x, y)</span>:</span></span><br><span class="line">  y_ = model(x)</span><br><span class="line">  <span class="keyword">return</span> tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad</span><span class="params">(model, inputs, targets)</span>:</span></span><br><span class="line">  <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:</span><br><span class="line">    loss_value = loss(model, inputs, targets)</span><br><span class="line">  <span class="keyword">return</span> tape.gradient(loss_value, model.variables)</span><br></pre></td></tr></table></figure><h2 id="创建优化器">创建优化器</h2><p>TensorFlow 拥有许多可用于训练的优化算法。此模型使用的是 tf.train.GradientDescentOptimizer，它可以实现随机梯度下降法 (SGD)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><h2 id="训练循环">训练循环</h2><ol type="1"><li>迭代每个周期。通过一次数据集即为一个周期。</li><li>在一个周期中，遍历训练 Dataset 中的每个样本，并获取样本的特征 (x) 和标签 (y)。</li><li>根据样本的特征进行预测，并比较预测结果和标签。衡量预测结果的不准确性，并使用所得的值计算模型的损失和梯度。</li><li>使用 optimizer 更新模型的变量。</li><li>跟踪一些统计信息以进行可视化。</li><li>对每个周期重复执行以上步骤。</li></ol><p>num_epochs 变量是遍历数据集集合的次数。与直觉恰恰相反的是，训练模型的时间越长，并不能保证模型就越好。</p><p>num_epochs 是一个可以调整的超参数。选择正确的次数通常需要一定的经验和实验基础。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Note: Rerunning this cell uses the same model variables</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># keep results for plotting</span></span><br><span class="line">train_loss_results = []</span><br><span class="line">train_accuracy_results = []</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">201</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">  epoch_loss_avg = tfe.metrics.Mean()</span><br><span class="line">  epoch_accuracy = tfe.metrics.Accuracy()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Training loop - using batches of 32</span></span><br><span class="line">  <span class="keyword">for</span> x, y <span class="keyword">in</span> train_dataset:</span><br><span class="line">    <span class="comment"># Optimize the model</span></span><br><span class="line">    grads = grad(model, x, y)</span><br><span class="line">    optimizer.apply_gradients(zip(grads, model.variables),</span><br><span class="line">                              global_step=tf.train.get_or_create_global_step())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Track progress</span></span><br><span class="line">    epoch_loss_avg(loss(model, x, y))  <span class="comment"># add current batch loss</span></span><br><span class="line">    <span class="comment"># compare predicted label to actual label</span></span><br><span class="line">    epoch_accuracy(tf.argmax(model(x), axis=<span class="number">1</span>, output_type=tf.int32), y)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># end epoch</span></span><br><span class="line">  train_loss_results.append(epoch_loss_avg.result())</span><br><span class="line">  train_accuracy_results.append(epoch_accuracy.result())</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> epoch % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">    print(<span class="string">"Epoch &#123;:03d&#125;: Loss: &#123;:.3f&#125;, Accuracy: &#123;:.3%&#125;"</span>.format(epoch,</span><br><span class="line">                                                                epoch_loss_avg.result(),</span><br><span class="line">                                                                epoch_accuracy.result()))</span><br></pre></td></tr></table></figure><pre><code>Epoch 000: Loss: 1.433, Accuracy: 34.167%Epoch 050: Loss: 0.636, Accuracy: 70.000%Epoch 100: Loss: 0.417, Accuracy: 71.667%Epoch 150: Loss: 0.331, Accuracy: 87.500%Epoch 200: Loss: 0.239, Accuracy: 96.667%</code></pre><h2 id="可视化损失函数随时间推移而变化的情况">可视化损失函数随时间推移而变化的情况</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, sharex=<span class="keyword">True</span>, figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">fig.suptitle(<span class="string">'Training Metrics'</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>].set_ylabel(<span class="string">"Loss"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">axes[<span class="number">0</span>].plot(train_loss_results)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>].set_ylabel(<span class="string">"Accuracy"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">axes[<span class="number">1</span>].set_xlabel(<span class="string">"Epoch"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">axes[<span class="number">1</span>].plot(train_accuracy_results)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/TensorFlow入门示例_22_0.png"></p><h1 id="评估模型的效果">评估模型的效果</h1><h2 id="设置测试数据集">设置测试数据集</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">test_url = <span class="string">"http://download.tensorflow.org/data/iris_test.csv"</span></span><br><span class="line"></span><br><span class="line">test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url),</span><br><span class="line">                                  origin=test_url)</span><br><span class="line"></span><br><span class="line">test_dataset = tf.data.TextLineDataset(test_fp)</span><br><span class="line">test_dataset = test_dataset.skip(<span class="number">1</span>)             <span class="comment"># skip header row</span></span><br><span class="line">test_dataset = test_dataset.map(parse_csv)      <span class="comment"># parse each row with the funcition created earlier</span></span><br><span class="line">test_dataset = test_dataset.shuffle(<span class="number">1000</span>)       <span class="comment"># randomize</span></span><br><span class="line">test_dataset = test_dataset.batch(<span class="number">32</span>)           <span class="comment"># use the same batch size as the training set</span></span><br></pre></td></tr></table></figure><pre><code>Downloading data from http://download.tensorflow.org/data/iris_test.csv8192/573 [============================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 0us/step</code></pre><h2 id="根据测试数据集评估模型">根据测试数据集评估模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">test_accuracy = tfe.metrics.Accuracy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_dataset:</span><br><span class="line">  prediction = tf.argmax(model(x), axis=<span class="number">1</span>, output_type=tf.int32)</span><br><span class="line">  test_accuracy(prediction, y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Test set accuracy: &#123;:.3%&#125;"</span>.format(test_accuracy.result()))</span><br></pre></td></tr></table></figure><pre><code>Test set accuracy: 96.667%</code></pre><h1 id="使用经过训练的模型进行预测">使用经过训练的模型进行预测</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class_ids = [<span class="string">"Iris setosa"</span>, <span class="string">"Iris versicolor"</span>, <span class="string">"Iris virginica"</span>]</span><br><span class="line"></span><br><span class="line">predict_dataset = tf.convert_to_tensor([</span><br><span class="line">    [<span class="number">5.1</span>, <span class="number">3.3</span>, <span class="number">1.7</span>, <span class="number">0.5</span>,],</span><br><span class="line">    [<span class="number">5.9</span>, <span class="number">3.0</span>, <span class="number">4.2</span>, <span class="number">1.5</span>,],</span><br><span class="line">    [<span class="number">6.9</span>, <span class="number">3.1</span>, <span class="number">5.4</span>, <span class="number">2.1</span>]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">predictions = model(predict_dataset)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, logits <span class="keyword">in</span> enumerate(predictions):</span><br><span class="line">  class_idx = tf.argmax(logits).numpy()</span><br><span class="line">  name = class_ids[class_idx]</span><br><span class="line">  print(<span class="string">"Example &#123;&#125; prediction: &#123;&#125;"</span>.format(i, name))</span><br></pre></td></tr></table></figure><pre><code>Example 0 prediction: Iris setosaExample 1 prediction: Iris versicolorExample 2 prediction: Iris virginica</code></pre><h1 id="colab地址">colab地址</h1><p>https://colab.research.google.com/drive/1iGHulgf_ioKl_GP_7_v8yTwwyfzP6KTR</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;设置程序&quot;&gt;设置程序&lt;/h1&gt;
&lt;h2 id=&quot;安装最新版本的-tensorflow&quot;&gt;安装最新版本的 TensorFlow&lt;/h2&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.datadriven.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="机器学习" scheme="http://www.datadriven.top/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="TensorFlow" scheme="http://www.datadriven.top/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>以太坊白皮书笔记</title>
    <link href="http://www.datadriven.top/2018/04/11/%E4%BB%A5%E5%A4%AA%E5%9D%8A%E7%99%BD%E7%9A%AE%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
    <id>http://www.datadriven.top/2018/04/11/以太坊白皮书笔记/</id>
    <published>2018-04-11T07:06:25.000Z</published>
    <updated>2018-11-13T13:49:35.020Z</updated>
    
    <content type="html"><![CDATA[<h1 id="以太坊白皮书笔记">以太坊白皮书笔记</h1><p>比特币的两个创新：</p><ul><li>一种去中心化的点对点的网上货币</li><li>基于工作量证明的区块链概念使得人们可以就交易顺序达成共识。</li></ul><p>“智能合约”：</p><ul><li>根据事先任意制订的规则来自动转移数字资产的系统。</li></ul><p>以太坊的目标：</p><ul><li>就是提供一个带有内置的成熟的图灵完备语言的区块链，用这种语言可以创建合约来编码任意状态转换功能，用户只要简单地用几行代码来实现逻辑，就能够创建以上提及的所有系统以及许多我们还想象不到的的其它系统。从而，将区块链应用于货币以外的领域。</li></ul><h1 id="历史">历史</h1><p>中本聪的创新是引入这样一个理念：将一个非常简单的基于节点的去中心化共识协议与工作量证明机制结合在一起。节点通过工作量证明机制获得参与到系统的权利，每十分钟将交易打包到“区块”中，从而创建出不断增长的区块链。</p><h2 id="作为状态转换系统的比特币">作为状态转换系统的比特币</h2><p>从技术角度讲，比特币账本可以被认为是一个状态转换系统，该系统包括所有现存的比特币所有权状态和“状态转换函数”。状态转换函数以当前状态和交易为输入，输出新的状态。</p><p><img src="/images/15192743757568/15192753695524.jpg"></p><ul><li>UTXO:所有已经被挖出的、没有花费的比特币（技术上称为“未花费的交易输出，unspent transaction outputs)。每个UTXO都有一个面值和所有者。</li><li>一笔交易：包括一个或多个输入和一个或多个输出。</li><li>每个输入包含一个对现有UTXO的引用和由与所有者地址相对应的私钥创建的密码学签名。</li><li>每个输出包含一个新的加入到状态中的UTXO。</li></ul><p>在比特币系统中，状态转换函数<code>APPLY(S,TX)-&gt;S’</code>大体上可以如下定义：</p><ul><li>交易的每个输入：<ul><li>如果引用的UTXO不存在于现在的状态中（S），返回错误提示</li><li>如果签名与UTXO所有者的签名不一致，返回错误提示</li></ul></li><li>如果所有的UTXO输入面值总额小于所有的UTXO输出面值总额，返回错误提示</li><li>返回新状态<code>S’</code>,新状态S中移除了所有的输入UTXO，增加了所有的输出UTXO。</li></ul><h2 id="挖矿">挖矿</h2><p><img src="/images/15192743757568/15197327486094.jpg"> 每个区块包含一个时间戳、一个随机数、一个对上一个区块的引用（即哈希）和上一区块生成以来发生的所有交易列表。</p><ul><li>区块验证算法,即“工作量证明”：</li><li>防止攻击：利用交易顺序攻击时，在发生区块链分叉时，区块链长的分支被认为是诚实的区块链。</li></ul><h2 id="默克尔树">默克尔树</h2><p><img src="/images/15192743757568/15197330359160.jpg"></p><p>左：仅提供默克尔树（Merkle tree）上的少量节点已经足够给出分支的合法证明。 右：任何对于默克尔树的任何部分进行改变的尝试都会最终导致链上某处的不一致。</p><ul><li>默克尔树是一种二叉树，由一组叶节点、一组中间节点和一个根节点构成。最下面的大量的叶节点包含基础数据，每个中间节点是它的两个子节点的哈希，根节点也是由它的两个子节点的哈希，代表了默克尔树的顶部。</li><li>默克尔树的目的是允许区块的数据可以零散地传送：节点可以从一个源下载区块头，从另外的源下载与其有关的树的其它部分，而依然能够确认所有的数据都是正确的。</li></ul><p>节点：</p><ul><li>轻节点”，它下载区块头，使用区块头确认工作量证明，然后只下载与其交易相关的默克尔树“分支”。</li><li>全量节点：存储和处理所有区块的全部数据的节点。</li></ul><h2 id="其他区块链应用">其他区块链应用</h2><p>去中心化共识技术的应用将会服从幂律分布，大多数的应用太小不足以保证自由区块链的安全，我们还注意到大量的去中心化应用，尤其是去中心化自治组织，需要进行应用之间的交互。</p><h2 id="脚本">脚本</h2><p>本质上，比特币系统允许不同的密码学货币进行去中心化的兑换。然而，比特币系统的脚本语言存在一些严重的限制：</p><ul><li>缺少图灵完备性：不支持循环语句。</li><li>价值盲（Value-blindness）。UTXO脚本不能为账户的取款额度提供精细的的控制。</li><li>缺少状态 – UTXO只能是已花费或者未花费状态，这就没有给需要任何其它内部状态的多阶段合约或者脚本留出生存空间。</li><li>区块链盲（Blockchain-blindness）- UTXO看不到区块链的数据，例如随机数和上一个区块的哈希。这一缺陷剥夺了脚本语言所拥有的基于随机性的潜在价值，严重地限制了博彩等其它领域应用。</li></ul><h1 id="以太坊">以太坊</h1><ul><li>以太坊的目的是基于脚本、竞争币和链上元协议（on-chain meta-protocol）概念进行整合和提高，使得开发者能够创建任意的基于共识的、可扩展的、标准化的、特性完备的、易于开发的和协同的应用。</li><li>以太坊通过建立终极的抽象的基础层-内置有图灵完备编程语言的区块链-使得任何人都能够创建合约和去中心化应用并在其中设立他们自由定义的所有权规则、交易方式和状态转换函数。</li></ul><h2 id="以太坊账户">以太坊账户</h2><p>以太坊的账户包含四个部分：</p><ul><li>随机数，用于确定每笔交易只能被处理一次的计数器</li><li>账户目前的以太币余额</li><li>账户的合约代码，如果有的话</li><li>账户的存储（默认为空）</li></ul><p>以太币（Ether）：</p><ul><li>是以太坊内部的主要加密燃料，用于支付交易费用。</li></ul><p>两类账户：外部所有的账户（由私钥控制的）和合约账户（由合约代码控制）。</p><ul><li>外部所有的账户没有代码，人们可以通过创建和签名一笔交易从一个外部账户发送消息。</li><li>每当合约账户收到一条消息，合约内部的代码就会被激活，允许它对内部存储进行读取和写入，和发送其它消息或者创建合约。</li></ul><h2 id="消息和交易">消息和交易</h2><p>以太坊的消息在某种程度上类似于比特币的交易，但是两者之间存在三点重要的不同。</p><ul><li>第一，以太坊的消息可以由外部实体或者合约创建，然而比特币的交易只能从外部创建。</li><li>第二，以太坊消息可以选择包含数据。</li><li>第三，如果以太坊消息的接受者是合约账户，可以选择进行回应，这意味着以太坊消息也包含函数概念。</li></ul><p>以太坊中“交易”是指存储从外部账户发出的消息的签名数据包。交易包含消息的接收者、用于确认发送者的签名、以太币账户余额、要发送的数据和两个被称为STARTGAS和GASPRICE的数值。</p><ul><li>STARTGAS就是限制，GASPRICE是每一计算步骤需要支付矿工的费用。</li><li>创建合约有单独的交易类型和相应的消息类型；合约的地址是基于账号随机数和交易数据的哈希计算出来的。</li></ul><h2 id="以太坊状态转换函数">以太坊状态转换函数</h2><p><img src="/images/15192743757568/15197350638427.jpg"> 以太坊的状态转换函数：<code>APPLY(S,TX) -&gt; S'</code>，可以定义如下：</p><ul><li>检查交易的格式是否正确（即有正确数值）、签名是否有效和随机数是否与发送者账户的随机数匹配。如否，返回错误。</li><li>计算交易费用:fee=STARTGAS * GASPRICE，并从签名中确定发送者的地址。从发送者的账户中减去交易费用和增加发送者的随机数。如果账户余额不足，返回错误。</li><li>设定初值GAS = STARTGAS，并根据交易中的字节数减去一定量的瓦斯值。</li><li>从发送者的账户转移价值到接收者账户。如果接收账户还不存在，创建此账户。如果接收账户是一个合约，运行合约的代码，直到代码运行结束或者瓦斯用完。</li><li>如果因为发送者账户没有足够的钱或者代码执行耗尽瓦斯导致价值转移失败，恢复原来的状态，但是还需要支付交易费用，交易费用加至矿工账户。</li><li>否则，将所有剩余的瓦斯归还给发送者，消耗掉的瓦斯作为交易费用发送给矿工。</li></ul><h2 id="代码执行">代码执行</h2><p>EVM代码（以太坊虚拟机代码）：使用低级的基于堆栈的字节码的语言写成。代码由一系列字节构成，每一个字节代表一种操作。操作可以访问三种存储数据的空间：</p><ul><li><strong>堆栈</strong>，一种后进先出的数据存储，32字节的数值可以入栈，出栈。</li><li><strong>内存</strong>，可无限扩展的字节队列。</li><li><strong>合约的长期存储</strong>，一个秘钥/数值的存储，其中秘钥和数值都是32字节大小，与计算结束即重置的堆栈和内存不同，存储内容将长期保持。</li></ul><h2 id="区块链和挖矿">区块链和挖矿</h2><p><img src="/images/15192743757568/15197367362645.jpg"></p><p>以太坊区块不仅包含交易记录和最近的状态，还包含区块序号和难度值。以太坊中的区块确认算法如下：</p><ul><li>检查区块引用的上一个区块是否存在和有效。</li><li>检查区块的时间戳是否比引用的上一个区块大，而且小于15分钟。</li><li>检查区块序号、难度值、 交易根，叔根和瓦斯限额（许多以太坊特有的底层概念）是否有效。</li><li>检查区块的工作量证明是否有效。</li><li>将<code>S[0]</code>赋值为上一个区块的<code>STATE_ROOT</code>。</li><li>将TX赋值为区块的交易列表，一共有n笔交易。对于属于<code>0……n-1</code>的i，进行状态转换<code>S[i+1] = APPLY(S[i],TX[i])</code>。如果任何一个转换发生错误，或者程序执行到此处所花费的瓦斯（gas）超过了GASLIMIT，返回错误。</li><li>用<code>S[n]</code>给<code>S_FINAL</code>赋值, 向矿工支付区块奖励。</li><li>检查S-FINAL是否与<code>STATE_ROOT</code>相同。如果相同，区块是有效的。否则，区块是无效的。</li></ul><h1 id="应用">应用</h1><p>三种应用:</p><ul><li>第一类是金融应用，为用户提供更强大的用他们的钱管理和参与合约的方法。包括子货币，金融衍生品，对冲合约，储蓄钱包，遗嘱，甚至一些种类的全面的雇佣合约。</li><li>第二类是半金融应用，这里有钱的存在但也有很重的非金钱的方面，一个完美的例子是为解决计算问题而设的自我强制悬赏。</li><li>还有在线投票和去中心化治理这样的完全的非金融应用。</li></ul><h2 id="令牌系统">令牌系统</h2><p>链上令牌系统有很多应用，如美元或黄金等资产的子货币、公司股票、不可伪造的优惠券、积分奖励， 所有的货币或者令牌系统，从根本上来说是一个带有如下操作的数据库：从A中减去X单位并把X单位加到B上，前提条件是(1)A在交易之前有至少X单位以及(2)交易被A批准。实施一个令牌系统就是把这样一个逻辑实施到一个合约中去。</p><h2 id="金融衍生品">金融衍生品</h2><p>金融衍生品是“智能合约”的最普遍的应用，也是最易于用代码实现的之一。</p><p>实现金融合约的主要挑战是它们中的大部分需要参照一个外部的价格发布器。最简单地方法是通过由某特定机构（例如纳斯达克）维护的“数据提供“合约进行，该合约的设计使得该机构能够根据需要更新合约，并提供一个接口使得其它合约能够通过发送一个消息给该合约以获取包含价格信息的回复。 ## 身份和信誉系统</p><h2 id="去中心化文件存储">去中心化文件存储</h2><p>以太坊合约允许去中心化存储生态的开发，这样用户通过将他们自己的硬盘或未用的网络空间租出去以获得少量收益，从而降低了文件存储的成本。当一个用户想重新下载他的文件，他可以使用微支付通道协议（例如每32k字节支付1萨博）恢复文件。</p><h2 id="去中心化自治组织">去中心化自治组织</h2><ul><li>去中心化自治组织（<code>DAO</code>, decentralized autonomous organization）”的概念指的是一个拥有一定数量成员或股东的虚拟实体，依靠比如67%多数来决定花钱以及修改代码。</li><li>中心化组织（DO）</li><li>去中心化自治公司（DAC，decentralized autonomous corporation）</li></ul><h2 id="进一步的应用">进一步的应用</h2><ul><li>储蓄钱包。类似银行。</li><li>作物保险</li><li>一个去中心化的数据发布器。</li><li>云计算。</li><li><strong>点对点赌博。</strong></li><li>预测市场。</li></ul><h1 id="杂项和关注">杂项和关注</h1><h2 id="改进版幽灵协议的实施">改进版幽灵协议的实施</h2><p>如果矿工A挖出了一个区块然后矿工B碰巧在A的区块扩散至B之前挖出了另外一个区块，矿工B的区块就会作废并且没有对网络安全作出贡献。此外，这里还有中心化问题：如果A是一个拥有全网30%算力的矿池而B拥有10%的算力，A将面临70%的时间都在产生作废区块的风险而B在90%的时间里都在产生作废区块。</p><h2 id="费用">费用</h2><p>比特币使用的默认方法是纯自愿的交易费用，依靠矿工担当守门人并设定动态的最低费用。因为这种方法是“基于市场的”，使得矿工和交易发送者能够按供需来决定价格，所以这种方法在比特币社区被很顺利地接受了。然而，这个逻辑的问题在于，交易处理并非一个市场；虽然根据直觉把交易处理解释成矿工给发送者提供的服务是很有吸引力的，但事实上一个矿工收录的交易是需要网络中每个节点处理的，所以交易处理中最大部分的成本是由第三方而不是决定是否收录交易的矿工承担的。于是，非常有可能发生公地悲剧。</p><h2 id="计算和图灵完备">计算和图灵完备</h2><p>以太坊虚拟机是图灵完备的。防止恶意循环：每一个交易设定运行执行的最大计算步数来解决问题，如果超过则计算被恢复原状但依然要支付费用。</p><h2 id="货币和发行">货币和发行</h2><p>发行模式如下：</p><ul><li>通过发售活动，以太币将以每BTC 1337-2000以太的价格发售，一个旨在为以太坊组织筹资并且为开发者支付报酬的机制已经在其它一些密码学货币平台上成功使用。早期购买者会享受较大的折扣，发售所得的BTC将完全用来支付开发者和研究者的工资和悬赏，以及投入密码学货币生态系统的项目。</li><li>0.099x （x为发售总量）将被分配给BTC融资或其它的确定性融资成功之前参与开发的早期贡献者，另外一个0.099x将分配给长期研究项目。</li><li>自上线时起每年都将有0.26x（x为发售总量）被矿工挖出。</li></ul><p>除了线性的发行方式外，和比特币一样以太币的的供应量增长率长期来看也趋于零。</p><h2 id="挖矿的中心化">挖矿的中心化</h2><ul><li>以太坊使用一个基于为每1000个随机数随机产生唯一哈希的函数的挖矿算法，用足够宽的计算域，去除专用硬件的优势。</li></ul><h2 id="扩展性">扩展性</h2><ul><li>如果比特币网络处理Visa级的2000tps的交易，它将以每三秒1MB的速度增长（1GB每小时，8TB每年）。太坊全节点只需存储状态而不是完整的区块链，从而得到改善。</li><li>因为基于区块链的挖矿算法，至少每个矿工会被迫成为一个全节点，这保证了一定数量的全节点。</li></ul><h1 id="综述去中心化应用">综述：去中心化应用</h1><p>从“传统”网页的角度看来，这些网页是完全静态的内容，因为区块链和其它去中心化协议将完全代替服务器来处理用户发起的请求。</p><h1 id="结论">结论</h1><ul><li>以太坊协议最初是作为一个通过高度通用的语言，提供链上契约、提现限制、金融合约、赌博市场等高级功能，作为升级版密码学货币。</li><li>作为图灵完备编程语言，理论上任意的合约创建出来。</li><li>以太坊协议比单纯的货币走得更远，可以创建金融和非金融应用。</li><li>提供了一个具有独特潜力的平台</li></ul><h1 id="通俗理解">通俗理解</h1><ol type="1"><li>是区块链的结构？区块链就是很多块数据的存储，形成链式结构。你可以这样理解，一个区块就是一个Word文件，每分钟产生一个Word文档；如果创世区块链式001.doc，第二分钟就是002.doc,后面依次产生下去003.doc 004.doc ... xxN.doc， 这就像链条一样，所以才叫区块链。总之一句话，每分钟产生的Word文档用链条连接起来，就是区块链。</li><li>第二点就是，所有的文档存储在哪里？先说下，传统的金融行业，农业银行的系统，那就农行自己的机器上面。如果农行想修改的数据，可以随便修改。 这种情况，农行就是中心。区块链的存储是网上所有的挖矿机器上面， 任何人都没有权限修改你的数据，也就是Word文档的内容，任何人都不能修改。这也是为什么，区块链是去中心化的，没有中心，没有人能篡改。</li><li>区块里面放了什么？ 每个Word文档里面放的就是这一分钟的交易信息。比如，我给你转账1个比特币，就放在当前分钟的Word文档里面。</li><li>谁来把这些信息放到Word文档里面 矿工。 每个矿工（也就是计算机）都在抢打包交易信息去权限，抢到了就是挖到了矿，就会得到比特币的奖励。</li></ol><h1 id="参考文档">参考文档</h1><ul><li><a href="http://ethfans.org/wikis/以太坊白皮书" target="_blank" rel="noopener">以太坊白皮书</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;以太坊白皮书笔记&quot;&gt;以太坊白皮书笔记&lt;/h1&gt;
&lt;p&gt;比特币的两个创新：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一种去中心化的点对点的网上货币&lt;/li&gt;
&lt;li&gt;基于工作量证明的区块链概念使得人们可以就交易顺序达成共识。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;“智能合约”：&lt;/p&gt;
&lt;
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.datadriven.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="区块链" scheme="http://www.datadriven.top/tags/%E5%8C%BA%E5%9D%97%E9%93%BE/"/>
    
      <category term="以太坊" scheme="http://www.datadriven.top/tags/%E4%BB%A5%E5%A4%AA%E5%9D%8A/"/>
    
  </entry>
  
  <entry>
    <title>设计模式</title>
    <link href="http://www.datadriven.top/2018/04/09/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    <id>http://www.datadriven.top/2018/04/09/设计模式/</id>
    <published>2018-04-09T12:30:41.000Z</published>
    <updated>2018-11-13T13:53:22.383Z</updated>
    
    <content type="html"><![CDATA[<h1 id="代码重构">代码重构</h1><h2 id="基础">基础</h2><ul><li>重构的定义:对软件内部结构的一种调整，目的是在不改变”软件之可察行为“前提下，提高其可理解性，降低其修改成本。</li><li>重构的意义<ul><li>优秀设计的根本是：消除重复部分！（DRY = Don’t repeat yourself）</li><li>重构让代码更清晰，更容易理解</li><li>清晰的代码可以更方便的找到bug，重构可以写出更强健的代码</li><li>良好的设计可以在长远时间上提高开发速度</li></ul></li><li>什么时候重构<ul><li>随时进行重构（在我看来，重构更是一种开发的习惯）</li><li>事不过三，代码重复不要超过三次（否则就要”抽“出来）</li><li>添加功能时候并一一重构（个人理解是，添加新功能之前，分析并重构，从而更方便添加新功能）</li><li>修补错误时</li><li>code review时</li></ul></li><li>重构与性能<ul><li>重构确实会在短期内降低代码执行效率，但优化阶段是可以调整的，而且调整会更容易。</li><li>提前优化是万恶之源</li></ul></li><li>从测试开始<ul><li>无测试，无重构，只依赖手工测试，重构时候人会崩溃的。</li><li>重构的保证就是自动化测试</li></ul></li></ul><h2 id="避免事项">避免事项</h2><ul><li>重复的代码（这才是真正万恶之源，鄙视一切Ctrl+C/P）</li><li>过长函数，会导致责任不明确/难以切割/难以理解等一系列问题</li><li>过大类，职责不明确，垃圾滋生地</li><li>过长参数列（面向对象不是说说而已）</li><li>发散式变化，一个类会响应多种需求而被修改</li><li>散弹式修改（其实就是没有封装变化处，由于一个需求，多处需要被修改）</li><li>依赖情节（一个类对其他类过多的依赖）</li><li>数据泥团（如果数据有意义，就将结构数据变成对象）</li><li>type code，使用Class替代</li><li>switch，少用，考虑多态</li><li>过多平行的类，使用类继承并联起来</li><li>冗余类，去除它</li><li>夸夸其谈的未来性（Matin的文字，侯俊杰的翻译真是…出彩…）</li><li>临时值域，封装它</li><li>过度耦合的消息链，使用真正需要的函数和对象，而不要依赖于消息链</li><li>过度的deleate</li><li>过度使用其他类private值域</li><li>重复作用的类</li><li>不完美的类库，（类库老了，使用者也没办法阿）</li><li>纯数据类（类需要行为）</li><li>不纯粹的继承（拒绝父类的接口的类）</li><li>过多注释，注释多了，就说明代码不清楚了</li></ul><h1 id="重构原则">重构原则</h1><h2 id="函数重构规则">函数重构规则</h2><ul><li>Extract Method(提取函数)-------将大函数按模块拆分成几个小的函数</li><li>Inline Method ---- 内联函数：将微不足道的小函数进行整合</li><li>Introduce Explaining Variable---引入解释性变量：将复杂的表达式拆分成多个变量</li><li>Remove Assignments to Parameters----移除对参数的赋值</li><li>Replace Method with Method Object----以函数对象取代函数：一个函数如果参数过多，可抽为<code>类+函数+参数</code>。</li></ul><h2 id="类重构规则">类重构规则</h2><ul><li>Move Method----方法迁移：当类中的方法不适合放在当前类中时，就需要迁移。</li><li>Move Field----搬移字段：当在一个类中的某一个字段，被另一个类的对象频繁使用时，我们就应该考虑将这个字段的位置进行更改了。</li><li>Extract Class----提炼类：一个类如果过于复杂，做了好多的事情，违背了“单一职责”的原则，所以需要将其可以独立的模块进行拆分，当然有可能由一个类拆分出多个类。</li><li>Introduce Foreign Method----引入外加函数：在不想或者不能修改原类的情况下，为该类添加新的方法。使用包装设计模式。</li></ul><h2 id="数据重构规则">数据重构规则</h2><p>对数据重构是很有必要的，因为我们的程序主要是对数据进行处理。如果你的业务逻辑非常复杂，那么对数据进行合理的处理是很有必要的。对数据的组织形式以及操作进行重构，提高了代码的可维护性以及可扩展性。</p><ul><li>Self Encapsulate Field (自封装字段)：虽然字段对外是也隐藏的，但是还是有必要为其添加getter方法，在类的内部使用getter方法来代替self.field，该方式称为自封装字段，自己封装的字段，自己使用。</li><li>Replace data Value with Object(以对象取代数据值)</li><li>Change Value to Reference (将值对象改变成引用对象)</li><li>Replace Array or Dictionary with Object(以对象取代数组或字典)：数据组合起来代表一定的意义，这是最好将其定义成一个实体类。</li><li>Duplicate Observed Data(复制“被监测数据”)：即不要将业务代码和非业务代码糅合在一起。比如，不要再Integration层写业务代码。</li><li>Change Unidirectional Association to Bidirectional(将单向关联改为双向关联)：Customer与Order的关系是单向关联的，也就是说Order引用了Customer, 而Customer没有引用Order。</li><li>Encapsulate Field（封装字段)：public--&gt;private,再通过getter/setter操作。</li><li>Encapsulate Collection（封装集合)：当你的类中有集合时，为了对该集合进行封装，你需要为集合创建相应的操作方法，例如增删改查等等。</li><li>Replace Subclass with Fields（以字段取代子类)：当你的各个子类中唯一的差别只在“返回常量数据”的函数上。当遇到这种情况时，你就可以将这个返回的数据放到父类中，并在父类中创建相应的工厂方法，然后将子类删除即可。</li></ul><h2 id="条件表达式重构规则if-else">条件表达式重构规则(if-else)</h2><p>有时候在实现比较复杂的业务逻辑时，各种条件各种嵌套。如果处理不好的话，代码看上去会非常的糟糕，而且业务逻辑看上去会非常混乱。通过一些重构规则来对条件表达式进行重构，可以让业务逻辑更为清晰，代码更以维护和扩展。</p><ul><li>Decompose Conditional(分解条件表达式)：经if后的复杂条件表达式进行提取，将其封装成函数。</li><li>Consolidate Conditional Expression(合并条件表达式)：一些条件表达式后的语句体执行的代码块相同。</li><li>Consolidate Duplicate Conditional Fragments(合并重复的条件片段)：if与else中的相同语句。</li><li>Remove Control Flag(移除控制标记)：标记变量不易维护，不易理解。标记变量一般是可以使用其他语句进行替换的，可以使用break、return、continue等等，这个要根据具体情况而定。复杂的可以用状态机进行设计。</li><li>Replace Nested Condition with Guard Clauses(以卫语句取代嵌套的条件)：尽量不要将if-else进行嵌套，因为嵌套的if-else确实不好理解。要去除上面的嵌套模式，我们可以将if后的条件进行翻转，根据具体需求再引入return、break、continue等卫语句。</li><li>Replace Condition with Polymorphism(以多态取代条件表达式)：多态就是类的不同类型的对象有着不同的行为状态。当然，对多态再进一步包装，就是工厂设计模式。</li></ul><h2 id="继承关系重构规则">继承关系重构规则</h2><ul><li>Pull Up Field (字段上移) &amp; Pull Down Field (字段下移)<ul><li>字段上移:两个子类中的相同字段，向上移动。</li></ul></li><li>Extract Subclass (提炼子类)：每个类的职责更为单一，即“单一职责”。</li><li>Collapse Hierarchy (折叠继承关系)：与“提炼子类”规则相对应。就是当你的父类与子类差别不大时，我们就可以将子类与父类进行合并。</li><li>Form Template Method (构造模板函数)：“模板”其实就是框架，没有具体的实现细节，只有固定不变的步骤，可以说模板不关心具体的细节。</li><li>以委托取代继承（Replace Inheritance with Delegation）：子类只使用了父类的部分方法，而且没有继承或者部分继承了父类的数据。在这种情况下我们就可以将这种继承关系修改成委托的关系。具体做法就是修改这种继承关系，在原有子类中添加父类的对象字段，在子类中创建相应的方法，在方法中使用委托对象来调用原始父类中相应的方法。</li></ul><h1 id="参考">参考</h1><ul><li><a href="http://www.cnblogs.com/ludashi/category/792819.html" target="_blank" rel="noopener">代码重构</a></li><li><a href="https://book.douban.com/subject/1229923/" target="_blank" rel="noopener">《重构：改善既有代码的设计》</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;代码重构&quot;&gt;代码重构&lt;/h1&gt;
&lt;h2 id=&quot;基础&quot;&gt;基础&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;重构的定义:对软件内部结构的一种调整，目的是在不改变”软件之可察行为“前提下，提高其可理解性，降低其修改成本。&lt;/li&gt;
&lt;li&gt;重构的意义
&lt;ul&gt;
&lt;li&gt;优秀设计的根本
      
    
    </summary>
    
      <category term="研发技能" scheme="http://www.datadriven.top/categories/%E7%A0%94%E5%8F%91%E6%8A%80%E8%83%BD/"/>
    
    
      <category term="设计模式" scheme="http://www.datadriven.top/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>mybatis-generator使用方式及实例</title>
    <link href="http://www.datadriven.top/2018/02/11/mybatis-generator%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F%E5%8F%8A%E5%AE%9E%E4%BE%8B/"/>
    <id>http://www.datadriven.top/2018/02/11/mybatis-generator使用方式及实例/</id>
    <published>2018-02-11T12:09:26.000Z</published>
    <updated>2018-11-13T13:48:06.657Z</updated>
    
    <content type="html"><![CDATA[<h1 id="mybatis-generator使用方式及实例">mybatis-generator使用方式及实例</h1><p>MyBatis Generator (MBG) 是一个Mybatis的代码生成器，它可以帮助我们根据数据库中表的设计生成对应的实体类，xml Mapper文件，接口以及帮助类，也就是我们可以借助该类来进行简单的CRUD操作。这样就避免了我们每使用到一张表的数据就需要手动去创建对应的类和xml文件，这就帮我们节约了大量的时间去开发和业务逻辑有关的功能。下面我主要介绍基于Maven和普通的Java工程两种方式来生成相应的文件。</p><blockquote><p>注：如果对联合查询和存储过程您仍然需要手写SQL和对象。</p></blockquote><h2 id="mysql环境准备">MySQL环境准备</h2><ul><li>安装：brew install mysql</li><li>启动：mysql.server start</li><li>运行：mysql_secure_installation</li><li>新建库manager--表tasks</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="keyword">IF</span> <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> tasks (</span><br><span class="line">  task_id <span class="built_in">INT</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</span><br><span class="line">  subject <span class="built_in">VARCHAR</span>(<span class="number">45</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  start_date <span class="built_in">DATE</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  end_date <span class="built_in">DATE</span> <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  description <span class="built_in">VARCHAR</span>(<span class="number">200</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</span><br><span class="line">  PRIMARY <span class="keyword">KEY</span> (task_id)</span><br><span class="line">) <span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span>;</span><br></pre></td></tr></table></figure><h2 id="配置">配置</h2><p>mybatis-generator有三种用法：命令行、eclipse插件、maven插件。本文使用maven插件方式，原因：该方式可以直接集成到项目中，从而方便后续其他人迭代新增表后，自动生成。</p><p>参考文档中有详细说明，如果不想细看，可以直接clone代码 https://github.com/DanielJyc/mybatis-generator 。然后：</p><ul><li>更改<code>generatorConfig.xml</code>中，jar路径：<code>/Users/daniel/gitworkspace/JavaEETest/Test27-mybatis2/lib/mysql-connector-java-5.1.40.jar</code>为自己的真实路径。</li><li>更改表配置信息：</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">table</span> <span class="attr">tableName</span>=<span class="string">"tasks"</span> <span class="attr">enableCountByExample</span>=<span class="string">"false"</span> <span class="attr">enableUpdateByExample</span>=<span class="string">"false"</span> <span class="attr">enableDeleteByExample</span>=<span class="string">"false"</span></span></span><br><span class="line"><span class="tag">       <span class="attr">enableSelectByExample</span>=<span class="string">"false"</span> <span class="attr">selectByExampleQueryId</span>=<span class="string">"false"</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>更改数据库连接信息</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">db.driver=com.mysql.jdbc.Driver</span><br><span class="line">db.url= jdbc:mysql://localhost:3306/manager</span><br><span class="line">db.username=root</span><br><span class="line">db.password=****</span><br></pre></td></tr></table></figure><ul><li>根目录<code>generator</code>下，直接执行<code>mvn mybatis-generator:generate</code>即可。</li></ul><h2 id="集成到项目中">集成到项目中</h2><p>可以集成到项目中，两种方式：</p><ul><li>作为单独的mvn模块。</li><li>直接集成到dal层。</li></ul><p>新建子mvn模块后，把代码拷贝过去即可。</p><h1 id="参考文档">参考文档</h1><ul><li><a href="https://www.yiibai.com/mysql/create-table.html" target="_blank" rel="noopener">sql教程</a></li><li><a href="http://blog.csdn.net/lkxlaz/article/details/54580735" target="_blank" rel="noopener">Mac下记录使用Homebrew安装Mysql全过程</a></li><li><a href="http://blog.csdn.net/testcs_dn/article/details/77881776" target="_blank" rel="noopener">Mybatis Generator最完整配置详解</a></li><li><a href="https://www.cnblogs.com/yjmyzz/p/4210554.html" target="_blank" rel="noopener">利用mybatis-generator自动生成代码</a></li><li><a href="https://www.cnblogs.com/Erma-king/p/6694516.html" target="_blank" rel="noopener">idea + mybatis generator + maven 插件使用</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;mybatis-generator使用方式及实例&quot;&gt;mybatis-generator使用方式及实例&lt;/h1&gt;
&lt;p&gt;MyBatis Generator (MBG) 是一个Mybatis的代码生成器，它可以帮助我们根据数据库中表的设计生成对应的实体类，xml Ma
      
    
    </summary>
    
      <category term="研发技能" scheme="http://www.datadriven.top/categories/%E7%A0%94%E5%8F%91%E6%8A%80%E8%83%BD/"/>
    
    
      <category term="mybatis" scheme="http://www.datadriven.top/tags/mybatis/"/>
    
  </entry>
  
  <entry>
    <title>shell脚本使用和总结</title>
    <link href="http://www.datadriven.top/2018/02/04/shell%E8%84%9A%E6%9C%AC%E4%BD%BF%E7%94%A8%E5%92%8C%E6%80%BB%E7%BB%93/"/>
    <id>http://www.datadriven.top/2018/02/04/shell脚本使用和总结/</id>
    <published>2018-02-04T14:23:38.000Z</published>
    <updated>2018-11-13T13:48:36.351Z</updated>
    
    <content type="html"><![CDATA[<h1 id="shell脚本使用和总结">shell脚本使用和总结</h1><h2 id="hello-world">hello world</h2><ul><li>新建<code>test1.sh</code></li><li>内容<code>echo Hello world!</code></li></ul><blockquote><p>如果需要的话，赋予可执行权限<code>chmod +x test1.sh</code></p></blockquote><h2 id="执行指令">执行指令</h2><p>可以在脚本中写入更多的命令，比如：启动app，编译、运行java文件，链接服务器等等；只要你能想到的，基本都可以。在此，我仅以一个简单的例子来说明：通过脚本编译运行一个java文件。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /Users/daniel/Documents/tmp</span><br><span class="line">javac Test2.java</span><br><span class="line">java Test2</span><br></pre></td></tr></table></figure><h2 id="匹配规则---的使用路径处理">匹配规则${}，##, %% , ：- ，：+， ？ 的使用（路径处理）</h2><p>假设我们定义了一个变量为：<code>file=/dir1/dir2/dir3/my.file.txt</code></p><ul><li>1.可以用${ }分别替换得到不同的值：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$&#123;file#*/&#125;：删掉第一个/ 及其左边的字符串：dir1/dir2/dir3/my.file.txt</span><br><span class="line">$&#123;file##*/&#125;：删掉最后一个/  及其左边的字符串：my.file.txt</span><br><span class="line">$&#123;file#*.&#125;：删掉第一个.  及其左边的字符串：file.txt</span><br><span class="line">$&#123;file##*.&#125;：删掉最后一个.  及其左边的字符串：txt</span><br><span class="line">$&#123;file%/*&#125;：删掉最后一个 /  及其右边的字符串：/dir1/dir2/dir3</span><br><span class="line">$&#123;file%%/*&#125;：删掉第一个/  及其右边的字符串：(空值)</span><br><span class="line">$&#123;file%.*&#125;：删掉最后一个 .  及其右边的字符串：/dir1/dir2/dir3/my.file</span><br><span class="line">$&#123;file%%.*&#125;：删掉第一个 .   及其右边的字符串：/dir1/dir2/dir3/my</span><br></pre></td></tr></table></figure><ul><li>2.记忆的方法为：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 是 去掉左边（键盘上#在 $ 的左边）</span><br><span class="line">%是去掉右边（键盘上% 在$ 的右边）</span><br><span class="line">单一符号是最小匹配；两个符号是最大匹配</span><br><span class="line">$&#123;file:0:5&#125;：提取最左边的5 个字节：/dir1</span><br><span class="line">$&#123;file:5:5&#125;：提取第5 个字节右边的连续5个字节：/dir2</span><br><span class="line">也可以对变量值里的字符串作替换：</span><br><span class="line">$&#123;file/dir/path&#125;：将第一个dir 替换为path：/path1/dir2/dir3/my.file.txt</span><br><span class="line">$&#123;file//dir/path&#125;：将全部dir 替换为path：/path1/path2/path3/my.file.txt</span><br></pre></td></tr></table></figure><ul><li>3.利用${ } 还可针对不同的变数状态赋值(沒设定、空值、非空值)：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$&#123;file-my.file.txt&#125; ：假如$file 沒有设定，則使用my.file.txt 作传回值。(空值及非空值時不作处理) </span><br><span class="line">$&#123;file:-my.file.txt&#125; ：假如$file 沒有設定或為空值，則使用my.file.txt 作傳回值。(非空值時不作处理)</span><br><span class="line">$&#123;file+my.file.txt&#125; ：假如$file 設為空值或非空值，均使用my.file.txt 作傳回值。(沒設定時不作处理)</span><br><span class="line">$&#123;file:+my.file.txt&#125; ：若$file 為非空值，則使用my.file.txt 作傳回值。(沒設定及空值時不作处理)</span><br><span class="line">$&#123;file=my.file.txt&#125; ：若$file 沒設定，則使用my.file.txt 作傳回值，同時將$file 賦值為my.file.txt 。(空值及非空值時不作处理)</span><br><span class="line">$&#123;file:=my.file.txt&#125; ：若$file 沒設定或為空值，則使用my.file.txt 作傳回值，同時將$file 賦值為my.file.txt 。(非空值時不作处理)</span><br><span class="line">$&#123;file?my.file.txt&#125; ：若$file 沒設定，則將my.file.txt 輸出至STDERR。(空值及非空值時不作处理)</span><br><span class="line"></span><br><span class="line">$&#123;file:?my.file.txt&#125; ：若$file 没设定或为空值，则将my.file.txt 输出至STDERR。(非空值時不作处理)</span><br><span class="line"></span><br><span class="line">$&#123;#var&#125; 可计算出变量值的长度：</span><br><span class="line"></span><br><span class="line">$&#123;#file&#125; 可得到27 ，因为/dir1/dir2/dir3/my.file.txt 是27个字节</span><br></pre></td></tr></table></figure><h2 id="分割字符串">分割字符串</h2><p>比如，要分割<code>test=&quot;aaa,bbb,cc cc,dd dd&quot;</code>，可以这样:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.分割</span></span><br><span class="line">OLD_IFS=<span class="string">"<span class="variable">$IFS</span>"</span> </span><br><span class="line">IFS=<span class="string">"-"</span> </span><br><span class="line">arr=(<span class="variable">$filename</span>) </span><br><span class="line">IFS=<span class="string">"<span class="variable">$OLD_IFS</span>"</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.获取</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="variable">$arr</span>; <span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="variable">$x</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;arr[0]&#125;</span></span><br></pre></td></tr></table></figure><h2 id="实例">实例</h2><p>取出照片，按照日期创建文件夹，并将文件放到指定文件夹中。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> filePath <span class="keyword">in</span> /Users/***/DCIM/*.jpg; <span class="keyword">do</span></span><br><span class="line"><span class="comment"># 1.获取文件名，eg: P70827-165946.jpg</span></span><br><span class="line">filename=<span class="variable">$&#123;filePath##*/&#125;</span> </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$&#123;filename&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.获取日期标识部分P70827：从头开始取6位</span></span><br><span class="line">dirName=<span class="string">"/Users/***/DCIM/"</span><span class="variable">$&#123;filename:0:6&#125;</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$dirName</span><span class="string">"\n"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.根据日期创建文件夹</span></span><br><span class="line">mkdir -p <span class="variable">$&#123;dirName&#125;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.将文件移入该文件夹</span></span><br><span class="line">mv  <span class="variable">$&#123;filePath&#125;</span>  <span class="variable">$&#123;dirName&#125;</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h2 id="注意事项">注意事项</h2><ul><li>等号<code>=</code>两边不能有空格:空格对于linux的shell是一种很典型的分隔符，所以给变量赋值的时候中间不能够有空格。</li></ul><h1 id="参考文档">参考文档</h1><ul><li><a href="http://bbs.chinaunix.net/thread-4082007-1-1.html" target="_blank" rel="noopener">求助linux下批量建立文件夹和移动文件</a></li><li><a href="http://www.runoob.com/linux/linux-shell.html" target="_blank" rel="noopener">Shell 教程</a></li><li><a href="https://www.cnblogs.com/kaituorensheng/archive/2012/12/19/2825376.html" target="_blank" rel="noopener">shell编程--遍历目录下的文件</a></li><li><a href="https://www.douban.com/note/498011007/?type=rec" target="_blank" rel="noopener">shell中的${}，##和%%的使用</a></li><li><a href="https://www.cnblogs.com/gaochsh/p/6901809.html" target="_blank" rel="noopener">linux shell 字符串操作详解 （长度，读取，替换，截取，连接，对比，删除，位置 ）</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;shell脚本使用和总结&quot;&gt;shell脚本使用和总结&lt;/h1&gt;
&lt;h2 id=&quot;hello-world&quot;&gt;hello world&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;新建&lt;code&gt;test1.sh&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;内容&lt;code&gt;echo Hello w
      
    
    </summary>
    
      <category term="工具" scheme="http://www.datadriven.top/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="shell" scheme="http://www.datadriven.top/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>表达式引擎性能比较</title>
    <link href="http://www.datadriven.top/2018/02/03/%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%BC%95%E6%93%8E%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83/"/>
    <id>http://www.datadriven.top/2018/02/03/表达式引擎性能比较/</id>
    <published>2018-02-03T09:19:00.000Z</published>
    <updated>2018-11-13T13:52:24.090Z</updated>
    
    <content type="html"><![CDATA[<h1 id="表达式引擎性能比较">表达式引擎性能比较</h1><h2 id="表达式引擎选型">表达式引擎选型</h2><p>选择3种主要的表达式引擎进行性能对比，从而选择最优表达式引擎和最优方案。常用的3中规则引擎：QLExpress、MEVL、JUEL。</p><h2 id="性能测试">性能测试</h2><p>性能测试维度：</p><ul><li>表达式引擎维度：主要采用了3种引擎的5种实现方式，即<code>QLExpress 使用缓存</code>; <code>mvel 不编译</code>; <code>mvel 先编译</code>; <code>mvel 先编译，且指定输入值类型</code>; <code>juel 指定输入值类型</code>。注：<code>QLExpress 不使用缓存</code>的话，性能非常低，就不再进行对比测试。</li><li>表达式维度：主要采用了3种类型的表达式，后续的大多数需求，都属于这3种方式。即：<ul><li>最常见的场景（多个条件进行and操作）： <code>a&lt;100 &amp;&amp; b&gt;=100 &amp;&amp; c&lt;=123</code>,</li><li>包含特殊的操作（contains）：<code>a&lt;100 &amp;&amp; b&gt;=100 &amp;&amp; c&lt;=123 &amp;&amp; stringList.contains(str)</code></li><li>一个稍微复杂一点的语法树：<code>a&gt;1 &amp;&amp; ((b&gt;1 || c&lt;1) || (a&gt;1 &amp;&amp; b&lt;1 &amp;&amp; c&gt;1))</code></li></ul></li></ul><h2 id="测试方式">测试方式</h2><p>10次循环，每次循环执行这5种方式10万次，即每种方式执行100万次。为了保证一定的差异性，变量赋值的时候，采用变化的值。代码详见附录。</p><p>执行结果：</p><p>对结果进行可视化，很显然<code>Mvel不编译</code>方式耗时最高。</p><p><img src="/images/15172803326811/15176458278896.jpg"></p><p>为了方便查看，将<code>Mvel不编译</code>方式过滤掉，得到下图。得到以下结论：</p><ul><li>性能高低顺序为：<code>mvel 先编译，且指定输入值类型</code> &gt; <code>mvel 先编译</code> &gt; <code>juel 指定输入值类型</code> &gt; <code>QLExpress 使用缓存</code> &gt; <code>mvel 不编译</code>。</li><li><code>juel 指定输入值类型</code>性能也可以接受，但是在使用了<code>contains</code>操作后，性能明显降低很多。</li></ul><p><img src="/images/15172803326811/15176457169158.jpg"></p><h2 id="结论">结论</h2><p><code>mvel 先编译，且指定输入值类型</code>方式性能为最优，采用该方式作为表达式引擎的实现方式。但是，采用自己实现一层缓存：用来缓存预编译的结果，同时，采用2小时无访问，则更新缓存的策略。缓存使用Guava Cache实现。</p><blockquote><p>注： 1. QLExpress自身实现了缓存机制，如果性能要求没那么高的话，也可以采用该方式。 2. 如果规则非常多，比如到了亿级别的规模，并且基本不复用规则的话，就没有必要先编译并缓存了。或者调整缓存策略。</p></blockquote><h1 id="附录">附录</h1><h2 id="机器配置">机器配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports)</span><br><span class="line">处理器 2.3 GHz Intel Core i5</span><br><span class="line">内存 8 GB 2133 MHz LPDDR3</span><br><span class="line">图形卡 Intel Iris Plus Graphics 640 1536 MB</span><br></pre></td></tr></table></figure><h2 id="表达式引擎版本">表达式引擎版本：</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.mvel/mvel2 --&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.mvel<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mvel2<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.0.Final<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">       <span class="comment">&lt;!-- https://mvnrepository.com/artifact/ognl/ognl --&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>ognl<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>ognl<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">       <span class="comment">&lt;!-- https://mvnrepository.com/artifact/de.odysseus.juel/juel-api --&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>de.odysseus.juel<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>juel-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">       <span class="comment">&lt;!-- https://mvnrepository.com/artifact/de.odysseus.juel/juel-impl --&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>de.odysseus.juel<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>juel-impl<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="java代码">Java代码</h2><p>详见： https://github.com/DanielJyc/expression-language-compare/blob/master/java/src/Test.java</p><h1 id="参考文档">参考文档</h1><ul><li><a href="https://github.com/mvel/mvel" target="_blank" rel="noopener">mvel github</a></li><li><a href="https://github.com/alibaba/QLExpress" target="_blank" rel="noopener">QLExpress</a></li><li><a href="http://mvel.documentnode.com/" target="_blank" rel="noopener">MVEL 2.0 官方文档</a></li><li><a href="https://en.wikibooks.org/wiki/Transwiki:MVEL_Language_Guide" target="_blank" rel="noopener">Transwiki:MVEL Language Guide</a></li><li><a href="http://blog.csdn.net/sunnyyoona/article/details/75244442" target="_blank" rel="noopener">Mvel2.0使用指南一 基础</a></li><li><a href="http://blog.csdn.net/fhm727/article/details/6543152" target="_blank" rel="noopener">mvel2.0语法指南</a></li><li><a href="http://www.iteye.com/topic/732354" target="_blank" rel="noopener">Ognl/MVEL/Aviator/JSEL 四种表达式引擎执行效率对比</a></li><li><a href="http://www.officezhushou.com/excel2013jiqiao/5149.html" target="_blank" rel="noopener">在Excel图表中制作三维立体图表的方法</a></li><li><a href="http://juel.sourceforge.net/guide/start.html" target="_blank" rel="noopener">juel Quickstart</a></li><li><a href="http://artexpressive.blogspot.com/" target="_blank" rel="noopener">THE ART OF BEING EXPRESSIVE</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;表达式引擎性能比较&quot;&gt;表达式引擎性能比较&lt;/h1&gt;
&lt;h2 id=&quot;表达式引擎选型&quot;&gt;表达式引擎选型&lt;/h2&gt;
&lt;p&gt;选择3种主要的表达式引擎进行性能对比，从而选择最优表达式引擎和最优方案。常用的3中规则引擎：QLExpress、MEVL、JUEL。&lt;/p&gt;
&lt;h
      
    
    </summary>
    
      <category term="研发技能" scheme="http://www.datadriven.top/categories/%E7%A0%94%E5%8F%91%E6%8A%80%E8%83%BD/"/>
    
    
      <category term="表达式引擎" scheme="http://www.datadriven.top/tags/%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%BC%95%E6%93%8E/"/>
    
      <category term="QLExpress" scheme="http://www.datadriven.top/tags/QLExpress/"/>
    
      <category term="MEVL" scheme="http://www.datadriven.top/tags/MEVL/"/>
    
      <category term="JUEL" scheme="http://www.datadriven.top/tags/JUEL/"/>
    
  </entry>
  
  <entry>
    <title>Storm+kafka</title>
    <link href="http://www.datadriven.top/2018/01/19/Storm+kafka/"/>
    <id>http://www.datadriven.top/2018/01/19/Storm+kafka/</id>
    <published>2018-01-19T13:25:26.000Z</published>
    <updated>2018-11-13T13:48:50.452Z</updated>
    
    <content type="html"><![CDATA[<h1 id="stormkafka">Storm+kafka</h1><h2 id="stormkafka-1">Storm+kafka</h2><p>flume实时收集日志，kafka消息队列源源不断生产数据，然后由storm进行实时消费。</p><p><img src="/images/15152262382414/15154048911883.png"></p><h2 id="参考">参考</h2><ul><li><a href="https://www.cnblogs.com/quchunhui/p/5380260.html" target="_blank" rel="noopener">Storm+kafka的HelloWorld初体验</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;stormkafka&quot;&gt;Storm+kafka&lt;/h1&gt;
&lt;h2 id=&quot;stormkafka-1&quot;&gt;Storm+kafka&lt;/h2&gt;
&lt;p&gt;flume实时收集日志，kafka消息队列源源不断生产数据，然后由storm进行实时消费。&lt;/p&gt;
&lt;p&gt;&lt;img src
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.datadriven.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://www.datadriven.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Apache Superset</title>
    <link href="http://www.datadriven.top/2018/01/17/Apache%20Superset/"/>
    <id>http://www.datadriven.top/2018/01/17/Apache Superset/</id>
    <published>2018-01-17T12:41:00.000Z</published>
    <updated>2018-11-13T13:47:00.204Z</updated>
    
    <content type="html"><![CDATA[<h1 id="apache-superset">Apache Superset</h1><p>Superset其实是一个自助式数据分析工具，它的主要目标是简化我们的数据探索分析操作。</p><h2 id="相似产品对比">相似产品对比</h2><table><colgroup><col style="width: 33%"><col style="width: 33%"><col style="width: 33%"></colgroup><thead><tr class="header"><th>name</th><th>描述</th><th>实现</th></tr></thead><tbody><tr class="odd"><td>grafana</td><td>grafana的druid插件，比较简陋，github一年不更新了<span class="Apple-tab-span" style="white-space:pre"></span></td><td>js</td></tr><tr class="even"><td>Metabase<span class="Apple-tab-span" style="white-space:pre"></span></td><td>支持数据库种类多，启动方便，支持json查询。图形化查询，只能有一个聚合字段，两个维度<span class="Apple-tab-span" style="white-space:pre"></span></td><td>Clojure<span class="Apple-tab-span" style="white-space:pre"></span></td></tr><tr class="odd"><td>imply-pivot<span class="Apple-tab-span" style="white-space:pre"></span></td><td>基于Plywood，部署方便，能构造复杂的查询。目前已经闭源了，没法二次开发<span class="Apple-tab-span" style="white-space:pre"></span></td><td>Js</td></tr><tr class="even"><td>airbnb/superset<span class="Apple-tab-span" style="white-space:pre"></span></td><td>权限管理完善，图形可定制性也比较高，github持续更新,集合了metabase的Dashboard和pivot的查询可定制性优点，部署相对麻烦<span class="Apple-tab-span" style="white-space:pre"></span></td><td>python+js<span class="Apple-tab-span" style="white-space:pre"></span></td></tr></tbody></table><h2 id="数据库支持">数据库支持</h2><p>Superset 是基于 Druid.io 设计的，但是又支持横向到像 SQLAlchemy 这样的常见Python ORM框架上面。</p><p>Druid 是一个基于分布式的快速列式存储，也是一个为BI设计的开源数据存储查询工具。Druid提供了一种实时数据低延迟的插入、灵活的数据探索和快速数据聚合。现有的Druid已经可以支持扩展到TB级别的事件和PB级的数据了，Druid是BI应用的最佳搭档。</p><p>跟类似产品Hive相比，速度快了很多。</p><h2 id="架构">架构</h2><p>整个项目的后端是基于Python的，用到了Flask、Pandas、SqlAlchemy。</p><p><strong>后端：</strong></p><ul><li>Flask AppBuilder(鉴权、CRUD、规则）</li><li>Pandas（分析）</li><li>SqlAlchemy（数据库ORM）</li></ul><p><strong>前端：</strong></p><p>用到了npm、react、webpack,这意味着你可以在手机也可以流畅使用。</p><ul><li>d3 (数据可视化)</li><li>nvd3.org(可重用图表)</li></ul><h2 id="局限性">局限性</h2><ul><li>Superset的可视化，目前只支持每次可视化一张表，对于多表join的情况还无能为力</li><li>依赖于数据库的快速响应，如果数据库本身太慢Superset也没什么办法</li><li>语义层的封装还需要完善，因为druid原生只支持部分sql。</li></ul><h2 id="参考">参考</h2><ul><li><a href="https://segmentfault.com/a/1190000005083953" target="_blank" rel="noopener">解密Airbnb 自助BI神器：Superset 颠覆 Tableau</a></li><li><a href="http://airbnb.io/projects/superset/" target="_blank" rel="noopener">superset官网</a></li><li><a href="https://fangyeqing.github.io/2016/11/04/druid.io%E5%8F%AF%E8%A7%86%E5%8C%96%E8%B0%83%E7%A0%94/" target="_blank" rel="noopener">druid.io可视化调研</a></li><li><a href="https://siftery.com/product-comparison/grafana-vs-superset" target="_blank" rel="noopener">Grafana vs Superset</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;apache-superset&quot;&gt;Apache Superset&lt;/h1&gt;
&lt;p&gt;Superset其实是一个自助式数据分析工具，它的主要目标是简化我们的数据探索分析操作。&lt;/p&gt;
&lt;h2 id=&quot;相似产品对比&quot;&gt;相似产品对比&lt;/h2&gt;
&lt;table&gt;
&lt;colgr
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.datadriven.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://www.datadriven.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Apache Ambari</title>
    <link href="http://www.datadriven.top/2018/01/17/Apache%20Ambari/"/>
    <id>http://www.datadriven.top/2018/01/17/Apache Ambari/</id>
    <published>2018-01-17T12:41:00.000Z</published>
    <updated>2018-11-13T13:46:09.548Z</updated>
    
    <content type="html"><![CDATA[<h1 id="apache-ambari">Apache Ambari</h1><p>用来创建、管理、监视 Hadoop 的集群，但是这里的 Hadoop 是广义，指的是 Hadoop 整个生态圈（例如 Hive，Hbase，Sqoop，Zookeeper 等），而并不仅是特指 Hadoop。用一句话来说，Ambari 就是为了让 Hadoop 以及相关的大数据软件更容易使用的一个工具。</p><p>Ambari 自身也是一个分布式架构的软件，主要由两部分组成：Ambari Server 和 Ambari Agent。简单来说，用户通过 Ambari Server 通知 Ambari Agent 安装对应的软件；Agent 会定时地发送各个机器每个软件模块的状态给 Ambari Server，最终这些状态信息会呈现在 Ambari 的 GUI，方便用户了解到集群的各种状态，并进行相应的维护。</p><h2 id="参考">参考</h2><ul><li><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-bigdata-ambari/" target="_blank" rel="noopener">Ambari——大数据平台的搭建利器</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;apache-ambari&quot;&gt;Apache Ambari&lt;/h1&gt;
&lt;p&gt;用来创建、管理、监视 Hadoop 的集群，但是这里的 Hadoop 是广义，指的是 Hadoop 整个生态圈（例如 Hive，Hbase，Sqoop，Zookeeper 等），而并不仅是特
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.datadriven.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据 - Ambari" scheme="http://www.datadriven.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE-Ambari/"/>
    
  </entry>
  
  <entry>
    <title>Apache flume</title>
    <link href="http://www.datadriven.top/2018/01/17/Apache%20flume/"/>
    <id>http://www.datadriven.top/2018/01/17/Apache flume/</id>
    <published>2018-01-17T12:40:00.000Z</published>
    <updated>2018-11-13T13:46:43.960Z</updated>
    
    <content type="html"><![CDATA[<h1 id="apache-flume">Apache flume</h1><p>flume是分布式的日志收集系统，它将各个服务器中的数据收集起来并送到指定的地方去，比如说送到图中的HDFS，简单来说flume就是收集日志的。 <img src="/images/15152252519684/15153988664487.jpg"> ## 同类产品对比 Flume使用基于事务的数据传递方式来保证事件传递的可靠性。而logstash内部是没有persist queue，所以在异常情况下，是可能出现数据丢失的问题的。</p><p>具体可参考下面的文档。 ## event event将传输的数据进行封装，是flume传输数据的基本单位，如果是文本文件，通常是一行记录，event也是事务的基本单位。flume的核心是把数据从数据源(source)收集过来，在将收集到的数据送到指定的目的地(sink)。为了保证输送的过程一定成功，在送到目的地(sink)之前，会先缓存数据(channel),待数据真正到达目的地(sink)后，flume在删除自己缓存的数据。</p><p><img src="/images/15152252519684/15153992479580.jpg"></p><h2 id="flume架构介绍">flume架构介绍</h2><ul><li>agent：本身是一个java进程，运行在日志收集节点—所谓日志收集节点就是服务器节点。 agent里面包含3个核心的组件：source—-&gt;channel—–&gt;sink,类似生产者、仓库、消费者的架构。</li><li>source：source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据。</li><li>channel：source组件把数据收集来以后，临时存放在channel中。用来存放临时数据的——对采集到的数据进行简单的缓存，可以存放在memory、jdbc、file等等。</li><li>sink：sink组件是用于把数据发送到目的地的组件，目的地包括hdfs、logger、avro、thrift、ipc、file、null、hbase、solr、自定义。</li></ul><p>于flume可以支持多级flume的agent，即flume可以前后相继，例如sink可以将数据写到下一个agent的source中，这样的话就可以连成串了，可以整体处理了。flume还支持扇入(fan-in)、扇出(fan-out)。所谓扇入就是source可以接受多个输入，所谓扇出就是sink可以将数据输出多个目的地destination中。</p><p><img src="/images/15152252519684/15153997503906.jpg"></p><h2 id="参考">参考</h2><ul><li><a href="http://blog.csdn.net/a2011480169/article/details/51544664" target="_blank" rel="noopener">Flume架构以及应用介绍</a></li><li><a href="https://www.cnblogs.com/xing901022/p/5631445.html" target="_blank" rel="noopener">Flume日志采集系统——初体验（Logstash对比版）</a></li><li><a href="http://blog.csdn.net/yesicatt/article/details/52104622" target="_blank" rel="noopener">Logstash，flume，sqoop比较</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;apache-flume&quot;&gt;Apache flume&lt;/h1&gt;
&lt;p&gt;flume是分布式的日志收集系统，它将各个服务器中的数据收集起来并送到指定的地方去，比如说送到图中的HDFS，简单来说flume就是收集日志的。 &lt;img src=&quot;/images/151522
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.datadriven.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://www.datadriven.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="flume" scheme="http://www.datadriven.top/tags/flume/"/>
    
  </entry>
  
  <entry>
    <title>Siddhi</title>
    <link href="http://www.datadriven.top/2018/01/17/Siddhi/"/>
    <id>http://www.datadriven.top/2018/01/17/Siddhi/</id>
    <published>2018-01-17T12:40:00.000Z</published>
    <updated>2018-11-13T13:48:44.028Z</updated>
    
    <content type="html"><![CDATA[<h1 id="siddhi">Siddhi</h1><p>Siddhi是一个复杂事件流程引擎CEP(Complex Event Processing)。使用类SQL的语言描述事件流任务，可以很好的支撑开发一个可扩展的，可配置的流式任务执行引擎。性能管理系统之中，告警模块采用storm作为告警生成组件。传统设计之中，为了支持不同的告警规则类型，我们需要编写不同的业务逻辑代码，但是使用了Siddhi之后，我们只需要配置不同的流任务Siddhiql，即可以支持不同的告警业务。</p><h2 id="siddhi-能做什么">Siddhi 能做什么？</h2><ul><li>简单 ETL:使用类SQL</li><li>基于 window 聚合：基于时间窗口</li><li>多个流 Join</li><li>Pattern Query：<code>Pattern allows event streams to be correlated over time and detect event patterns based on the order of event arrival.</code>比如：在一天内，出现一次取现金额 &lt; 100之后，同一张卡，再次出现取现金额 &gt; 10000，则认为是诈骗。</li><li>Sequence Query:和 pattern 的区别是，pattern 的多个 event 之间可以是不连续的，但 sequence 的 events 之间必须是连续的。我们可以看个例子，用sequence 来发现股票价格的 peak：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from every e1=FilteredStockStream[price&gt;20], </span><br><span class="line">           e2=FilteredStockStream[((e2[last].price is null) and price&gt;=e1.price) or ((not (e2[last].price is null)) and price&gt;=e2[last].price)],</span><br><span class="line">           e3=FilteredStockStream[price&lt;e2[last].price] </span><br><span class="line">select e1.price as priceInitial, e2[last].price as pricePeak, e3.price as priceAfterPeak </span><br><span class="line">insert into PeakStream ;</span><br></pre></td></tr></table></figure><p>上面的查询的意思， e1，收到一条 event.price&gt;20。 e2，后续收到的所有 events 的 price，都大于前一条 event。 e3，最终收到一条 event 的 price，小于前一条 event。 ok，我们发现了一个peak。</p><h2 id="集成到-jstorm">集成到 JStorm</h2><p>我将 Siddhi core 封装成一个 Siddhi Bolt，这样可以在 JStorm 的 topology 中很灵活的，选择是否什么方案，可以部分统计用 brain，部分用 Siddhi，非常简单。</p><h2 id="参考">参考：</h2><ul><li><a href="https://www.cnblogs.com/coshaho/p/7051126.html" target="_blank" rel="noopener">Siddhi初探(内含一个实例)</a></li><li><a href="https://www.cnblogs.com/XzcBlog/p/4468679.html" target="_blank" rel="noopener">CEP简介</a></li><li><a href="http://doc.okbase.net/fxjwind/archive/195904.html" target="_blank" rel="noopener">让Storm插上CEP的翅膀 - Siddhi调研和集成</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;siddhi&quot;&gt;Siddhi&lt;/h1&gt;
&lt;p&gt;Siddhi是一个复杂事件流程引擎CEP(Complex Event Processing)。使用类SQL的语言描述事件流任务，可以很好的支撑开发一个可扩展的，可配置的流式任务执行引擎。性能管理系统之中，告警模块采用s
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.datadriven.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://www.datadriven.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>kafka</title>
    <link href="http://www.datadriven.top/2018/01/17/kafka/"/>
    <id>http://www.datadriven.top/2018/01/17/kafka/</id>
    <published>2018-01-17T12:40:00.000Z</published>
    <updated>2018-11-13T13:47:17.573Z</updated>
    
    <content type="html"><![CDATA[<h1 id="kafka">kafka</h1><p>是一个分布式消息系统，主要用作数据管道和消息系统。</p><h2 id="kafka的架构">kafka的架构</h2><p>如下图，由生产者向kafka集群生产消息，消费者从kafka集群订阅消息。 <img src="/images/15152262382414/15152273984714.jpg"></p><p>其中，kafka集群中的消息是按照主题（或者说Topic）来进行组成的。</p><ul><li>主题（Topic）：一个主题类似新闻中的体育、娱乐、教育等分类概念，在实际工程中通常一个业务一个主题。</li><li>分区（Partition）：一个Topic中的消息数据按照多个分区组织，分区是kafka消息队列组织的最小单位，一个分区可以看做是一个FIFO（先进先出）队列；kafka分区是提高kafka性能的关键手段。</li></ul><p><img src="/images/15152262382414/15152279237261.jpg"></p><p>这张图在整体上对kafka集群进行了概要，途中kafka集群是由三台机器（Broker）组成，当然，实际情况可能更多。相应的有3个分区，Partition-0~Partition-2，图中能看到每个分区的数据备份了2份。kafka集群从前端应用程序（producer）生产消息，后端通过各种异构的消费者来订阅消息。kafka集群和各种异构的生产者、消费者都使用zookeeper集群来进行分布式协调管理和分布式状态管理、分布式锁服务的。</p><h1 id="参考">参考</h1><ul><li><a href="http://www.cnblogs.com/quchunhui/p/5356511.html" target="_blank" rel="noopener">Kafka入门学习（一）</a></li><li><a href="https://www.cnblogs.com/huxi2b/p/4571309.html" target="_blank" rel="noopener">Kafka topic常见命令解析</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;kafka&quot;&gt;kafka&lt;/h1&gt;
&lt;p&gt;是一个分布式消息系统，主要用作数据管道和消息系统。&lt;/p&gt;
&lt;h2 id=&quot;kafka的架构&quot;&gt;kafka的架构&lt;/h2&gt;
&lt;p&gt;如下图，由生产者向kafka集群生产消息，消费者从kafka集群订阅消息。 &lt;img src
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.datadriven.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://www.datadriven.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>Apache Eagle</title>
    <link href="http://www.datadriven.top/2018/01/17/Apache-Eagle/"/>
    <id>http://www.datadriven.top/2018/01/17/Apache-Eagle/</id>
    <published>2018-01-17T12:40:00.000Z</published>
    <updated>2018-11-13T13:47:09.880Z</updated>
    
    <content type="html"><![CDATA[<h1 id="apache-eagle">Apache Eagle</h1><p>Apache Eagle是一个识别大数据平台上的安全和性能问题的开源解决方案。它主要用来即时监测敏感数据访问和恶意活动，并及时采取行动。除了数据活动管理，Eagle也可以用于节点异常检测,集群和作业性能分析。</p><h2 id="功能介绍">功能介绍</h2><p>主要的应用场景包括：监控Hadoop中的数据访问流量;检测非法入侵和违反安全规则的行为;检测并防止敏感数据丢失和访问;实现基于策略的实时检测和预警;实现基于用户行为模式的异常数据行为检测。</p><ul><li><strong>检测和报警</strong>：Apache Eagle依赖于Apache Storm来进行数据活动和操作日志的流处理，并且可以执行基于策略的检测和报警。它提供多个API：作为基于Storm API上的一层抽象的流式处理API和policy engine provider API的抽象，它将WSO2的开源Siddhi CEP engine作为第一类对象。Siddhi CEP engine支持报警规则的热部署，并且警报可以使用属性过滤和基于窗口的规则（例如，在10分钟内三次以上的访问）来定义。</li><li>集群和作业性能分析：通过处理YARN应用日志和对YARN中所有运行的作业进行快照分析来完成的。Eagle可以检测单个作业趋势、数据偏斜问题、故障原因和考虑所有运行的作业情况下评估集群的整体性能。</li><li>基于机器学习的policy provider：Apache Eagle中还包括一个基于机器学习的policy provider。它从过去的用户行为中学习，来将数据访问分类为异常或者正常。这个机器学习policy provider评估在Apache Spark框架中离线训练的模型。</li></ul><p>Eagle具有如下特点：</p><ul><li>高实时： 尽可能地确保能在亚秒级别时间内产生告警，一旦综合多种因素确订为危险操作，立即采取措施阻止非法行为。</li><li>可伸缩</li><li>简单易用</li><li>用户Profile：Eagle 内置提供基于机器学习算法对Hadoop中用户行为习惯建立用户Profile的功能。我们提供多种默认的机器学习算法供你选择用于针对不同HDFS特征集进行建模，通过历史行为模型，Eagle可以实时地检测异常用户行为并产生预警。</li></ul><h2 id="基本概念">基本概念</h2><ul><li>Site：A site can be considered as a physical data center. Big data platform e.g. Hadoop may be deployed to multiple data centers in an enterprise.</li><li>Application：An &quot;Application&quot; or &quot;App&quot; is composed of data integration, policies and insights for one data source.</li><li>Policy：A &quot;Policy&quot; defines the rule to alert. Policy can be simply a filter expression or a complex window based aggregation rules etc.</li><li>Alerts：An &quot;Alert&quot; is an real-time event detected with certain alert policy or correlation logic, with different severity levels like INFO/WARNING/DANGER.</li><li>Data Source：A &quot;Data Source&quot; is a monitoring target data. Eagle supports many data sources HDFS audit logs, Hive2 query, MapReduce job etc.</li><li>Stream：A &quot;Stream&quot; is the streaming data from a data source. Each data source has its own stream.</li></ul><h2 id="架构">架构</h2><p><img src="/images/15154044207385/15154064866918.jpg"></p><p><img src="/images/15152252519684/15154030269503.png"></p><p>从这个架构图，可以看到eagle会提供一些内置的应用，通过配置不同的policy，它们能够做很多eagle里面非常核心的事情。</p><p>比如一个大的集群里，我们可能希望能够探测一些恶意的操作以及误操作，类似于Hive、MR、Spark的Job，如果一个不相关的人，操作了其他应用独享的数据，这种情况应该及时的通知应用的负责人。security问题在多租户下面其实非常重要，像我们现在一个hadoop集群，不仅仅会提供一些离线的任务，其实这部分任务反而占比很少，大部分都是在线的Yarn任务以及HDFS服务，不同的应用会使用hadoop进行任务分发、状态存储、checkpoint等，但这些任务在文件系统上的隔离是不保证的。在生产环境里，如果某个在线任务的状态文件被其他应用误修改了，整个snapshot都会不可用，一般这样的情况会使当前批次整个回滚，甚至在极端情况下出现错误数据，影响应用的执行语义。</p><p>在alertEngine中，你可以添加一个policy，检查JobHistory或者logStash，如果一个Job触发了policy就会通知相关的责任人采取措施。<strong>对大集群的监控和报警实际是非常麻烦和耗时的，eagle很好的解决了这个问题。</strong></p><p>policy的设置非常灵活，甚至可以用机器学习的方式，离线去train一个模型，然后再把这个模型实时的更新上去。具体的做法是在storm这一层去做模型匹配，比如一个用户通常的几个特征量，read、delete、changename等，会去在线查看实际值与正常值的偏差，超过一定程度就会被eagle认为是异常的。或者一个未授权的用户，做了一件从来没有做过的事情，也可以认为是异常的。</p><p>通过内置的应用我们还可以进行作业的异常分析：一个Job有多个stage，上一级stage处理完之后，通过group的方式向下一级节点发送数据，假设hash的对象是userid，那么很可能出现的情况是，下一级一共有十个节点，但是partition之后的数据95%都被分配到了同一个节点上。这个问题通常是很难解决的，但有了eagle，我们就可以及时的发现这种情况，并优化提高Job的性能。</p><p>异常分析的另一个好处就是节点分析。稍微大一点规模的Job可能就需要几十上百个节点来执行，通常过程中如果挂掉一个节点，触发FailOver机制重新调度是可以解决的。但如果某个节点上的job经常失败，那么这个节点就应该被识别出来进行处理，避免大量job的重新调度开销。</p><h2 id="内置应用">内置应用</h2><p>上面这些功能需要底层架构给予支持。eagle底层的流计算引擎是基于storm的，我们知道在传统领域，比如OLAP、数据库这些，通常都会有一门DSL来方便大家使用。近来在流计算领域，一个比较火爆的概念就是我们能不能也创造一门DSL，类似于SQL，但数据却是流式的。比如Spark的StructureStreaming，Flink的TableApi都是因为这个而产生。在eagle上为了支持更方便的执行policy和动态更新，也是需要一个DSL。那么它的做法比较讨巧，相当于利用了Siddhi这个现成的CEP，集成到storm的框架里，利用Siddhi本身的SQL支持来实现storm的DSL。那么用户无论是自定义policy还是自己编写应用，都可以像下面这样写：</p><ul><li>定义流定义和查询，并将结果输出到另外一个流里面。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">define stream TempStream (deviceID long, roomNo int, temp double);</span><br><span class="line">from TempStream </span><br><span class="line"><span class="keyword">select</span> roomNo, temp * <span class="number">9</span>/<span class="number">5</span> + <span class="number">32</span> <span class="keyword">as</span> temp, F <span class="keyword">as</span> scale, roomNo &gt;= <span class="number">100</span> <span class="keyword">and</span> roomNo &lt; <span class="number">110</span> <span class="keyword">as</span> isServerRoom</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> RoomTempStream;</span><br></pre></td></tr></table></figure><ul><li>多流Join和TimeWindow。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from TempStream[temp &gt; 30.0]#window.time(1 min) as T </span><br><span class="line">join RegulatorStream[isOn == false]<span class="comment">#window.length(1) as R</span></span><br><span class="line">on T.roomNo == R.roomNo</span><br><span class="line"><span class="keyword">select</span> T.roomNo, T.temp, R.deviceID, <span class="keyword">start</span> <span class="keyword">as</span> <span class="keyword">action</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> RegulatorActionStream ;</span><br></pre></td></tr></table></figure><ul><li>Pattern Query：这个比较能体现CEP的优势，在下面的查询中，-&gt;标示的是事件顺序，也就是说，这个语义实际上表达了同一张卡在一天内，出现一次取现金额 &lt; 100后再次出现取现金额 &gt; 10000的情况，并将其判断为fraud。这是传统SQL所不具备的，也可以说是专为流式计算而设计。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from every a1 = atmStatsStream[amountWithdrawed &lt; 100]</span><br><span class="line">-&gt; b1 = atmStatsStream[amountWithdrawed &gt; 10000 and a1.cardNo == b1.cardNo]</span><br><span class="line">within 1 day</span><br><span class="line"><span class="keyword">select</span> a1.cardNo <span class="keyword">as</span> cardNo, a1.cardHolderName <span class="keyword">as</span> cardHolderName, b1.amountWithdrawed <span class="keyword">as</span> amountWithdrawed, b1.location <span class="keyword">as</span> location, b1.cardHolderMobile <span class="keyword">as</span> cardHolderMobile</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> possibleFraudStream;</span><br></pre></td></tr></table></figure><h2 id="元数据设计">元数据设计</h2><p>在集群中可能有成百上千个节点，每个节点上GB甚至上TB的的日志文件，如果出现一个异常的访问点，我们希望能在毫秒级别上对其进行预警或者是拦截。然后我们知道storm有一个很大的缺陷是它本身逻辑定义完就固定了。按照以往，这种分布式stream逻辑定义完，想再修改系统，必须要把topology重启，生产环境下肯定不希望这样，牺牲实时性的代价太大了，所以eagle整个的结构是元数据驱动（Metadata Driven）。</p><figure><img src="/images/15152252519684/15154038859885.png" alt="-w679"><figcaption>-w679</figcaption></figure><p>从上图我们可以看出eagle的输入和输出其实是非常明确的，那么在元数据的定义上，因为下面的存储是基于Hbase，所以eagle做的非常的灵活。一般对于同一个类型的采集日志(例如某个metric)，在RowKey上会采用一个固定的前缀，后面加上时间序列，这样在设计上就保证了分布性和同一个metric在数据上的连续性。</p><p>在生产场景下，可能一开始训练的Policy模型只有几个G的样本数据，但这个数据的增长是非常快的。那么我们不可能在一个月之后，还不去更换它。在eagle中这样的更新是很简单的。由于eagle的元数据驱动特性，engine会去监听元数据的变化。一旦metadata触发了alertEngine注册的listener，内部是可以通过ClassLoader动态部署的，比如动态的去更新storm里面的spout和bolt，这样整个更新过程可以在毫秒的级别就做完，相对来说，提高了几个数量级，并且这个过程是不会丢失数据的。</p><h2 id="编写自定义应用">编写自定义应用</h2><p>下面简单介绍一下如何编写一个自己的扩展应用：</p><ul><li>首先需要提供应用的Provider</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ExampleApplicationProvider</span> <span class="keyword">extends</span> <span class="title">AbstractApplicationProvider</span> </span>&#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(ExampleApplicationProvider.class);</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ExampleStormApplication <span class="title">getApplication</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ExampleStormApplication();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Optional <span class="title">getApplicationListener</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> Optional.of(<span class="keyword">new</span> ApplicationListener() &#123;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Inject</span> ExampleEntityService entityService;</span><br><span class="line">        <span class="keyword">private</span> ApplicationEntity application;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(ApplicationEntity applicationEntity)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.application = applicationEntity;</span><br><span class="line">            entityService.getEntities();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterInstall</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            LOG.info(<span class="string">"afterInstall &#123;&#125;"</span>, <span class="keyword">this</span>.application);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterUninstall</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            LOG.info(<span class="string">"afterUninstall &#123;&#125;"</span>, <span class="keyword">this</span>.application);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">beforeStart</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            LOG.info(<span class="string">"beforeStart &#123;&#125;"</span>, <span class="keyword">this</span>.application);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterStop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">            LOG.info(<span class="string">"afterStop &#123;&#125;"</span>, <span class="keyword">this</span>.application);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onRegister</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    bindToMemoryMetaStore(ExampleEntityService.class,ExampleEntityServiceMemoryImpl.class);</span><br><span class="line">    bind(ExampleCommonService.class,ExampleCommonServiceImpl.class);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里需要注意的是，应用本身的Meta是需要指定存储方式的，这个例子里面我们简单指定为Memory的方式。当然，在生产环境一般可以换成Hbase。</p><ul><li>然后提供应用本身的逻辑</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ExampleStormApplication</span> <span class="keyword">extends</span> <span class="title">StormApplication</span> </span>&#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> StormTopology <span class="title">execute</span><span class="params">(Config config, StormEnvironment environment)</span> </span>&#123;</span><br><span class="line">    TopologyBuilder builder = <span class="keyword">new</span> TopologyBuilder();</span><br><span class="line">    builder.setSpout(<span class="string">"metric_spout"</span>, environment.getStreamSource(<span class="string">"SAMPLE_STREAM"</span>, config)</span><br><span class="line">        , config.getInt(<span class="string">"spoutNum"</span>));</span><br><span class="line">    builder.setBolt(<span class="string">"sink_1"</span>, environment.getStreamSink(<span class="string">"SAMPLE_STREAM_1"</span>, config)).fieldsGrouping(<span class="string">"metric_spout"</span>,</span><br><span class="line">        <span class="keyword">new</span> Fields(<span class="string">"metric"</span>));</span><br><span class="line">    builder.setBolt(<span class="string">"sink_2"</span>, environment.getStreamSink(<span class="string">"SAMPLE_STREAM_2"</span>, config)).fieldsGrouping(<span class="string">"metric_spout"</span>,</span><br><span class="line">        <span class="keyword">new</span> Fields(<span class="string">"metric"</span>));</span><br><span class="line">    <span class="keyword">return</span> builder.createTopology();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>最后通过配置指定执行环境等参数</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">"application": &#123;</span><br><span class="line">    "sink": &#123;</span><br><span class="line">      "type": "org.apache.eagle.app.messaging.KafkaStreamSink",</span><br><span class="line">      "config": &#123;</span><br><span class="line">        "kafkaBrokerHost": "",</span><br><span class="line">        "kafkaZkConnection": ""</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    "storm": &#123;</span><br><span class="line">      "nimbusHost": "localhost"</span><br><span class="line">      "nimbusThriftPort": 6627</span><br><span class="line">    &#125;</span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">"appId": "unit_test_example_app"</span><br><span class="line">"spoutNum": 3</span><br><span class="line">"loaded": true</span><br><span class="line">"mode": "LOCAL"</span><br><span class="line">"dataSinkConfig": &#123;</span><br><span class="line">"topic": "test_topic",</span><br><span class="line">"brokerList": "sandbox.hortonworks.com:6667",</span><br><span class="line">"serializerClass": "kafka.serializer.StringEncoder",</span><br><span class="line">"keySerializerClass": "kafka.serializer.StringEncoder"</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里要配置好source和sink，比如kafka的topic、broker。限于篇幅，这里略去了coordinator自身的配置。完成上面的代码和配置，也就完成了一个自定义的应用编写。</p><h2 id="数据集成">数据集成</h2><p>数据集成使用Apache Kafka通过logstash forwarder 代理或通过log4j kafka appender来实现的。来自多个Hadoop守护进程（例如，namenode，datanode等）的日志条目被反馈到Kafka并由Storm处理。Eagle支持将数据资产分类为多个灵敏度类型。</p><h2 id="数据持久化">数据持久化</h2><p>Eagle支持使用Apache HBase和关系数据库持久化警报。警报可通过电子邮件、Kafka或存储在Eagle支持的存储中进行通知。你也可以开发自己的警报通知插件。</p><h2 id="结语">结语</h2><p>从前面的介绍我们可以看出，整个eagle其实是一套整体的解决方案，这个方案更多的是在应用的层面上进行了许多创新性的使用和整合。但eagle的实时性、可扩展性不仅仅值得在hadoop集群中使用，里面的很多思想其实也是值得给想要搭建流式计算平台的同学进行参考和学习的。而对于底层框架的开发同学，其实eagle在算子层、API层、状态存储层做的许多事情正是很多应用开发者需要自己去做的事情，能不能给开发应用更多的支持，让开发更顺畅更快速，也是值得去思考一下的。</p><h2 id="参考">参考</h2><ul><li><a href="http://www.infoq.com/cn/news/2017/02/apache-eagle-graduates-top-level" target="_blank" rel="noopener">Apache Eagle毕业成为顶级项目</a></li><li><a href="https://doc.huodongjia.com/detail-944.html" target="_blank" rel="noopener">Apache Eagle 陈浩——Apache+Eagle：架构演化和新特性</a></li><li><a href="https://www.cnblogs.com/junneyang/p/5882157.html" target="_blank" rel="noopener">Apache Eagle 简介--分布式实时 Hadoop 数据安全方案</a></li><li><a href="http://www.csdn.net/article/2015-10-29/2826076?ref=myread" target="_blank" rel="noopener">Apache Eagle——eBay开源分布式实时Hadoop数据安全方案</a></li><li><a href="http://eagle.apache.org/docs/latest/getting-started/" target="_blank" rel="noopener">Getting Started Eagle</a></li><li><a href="http://blog.csdn.net/whg18526080015/article/details/73642241" target="_blank" rel="noopener">kafka eagle安装与使用</a></li><li><a href="https://ke.smartloli.org/" target="_blank" rel="noopener">Kafka Eagle Reference Manual</a></li><li><a href=""></a></li><li><a href=""></a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;apache-eagle&quot;&gt;Apache Eagle&lt;/h1&gt;
&lt;p&gt;Apache Eagle是一个识别大数据平台上的安全和性能问题的开源解决方案。它主要用来即时监测敏感数据访问和恶意活动，并及时采取行动。除了数据活动管理，Eagle也可以用于节点异常检测,集群和
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.datadriven.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://www.datadriven.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>storm学习</title>
    <link href="http://www.datadriven.top/2018/01/17/storm%E5%AD%A6%E4%B9%A0/"/>
    <id>http://www.datadriven.top/2018/01/17/storm学习/</id>
    <published>2018-01-17T12:40:00.000Z</published>
    <updated>2018-11-13T13:48:56.280Z</updated>
    
    <content type="html"><![CDATA[<h1 id="storm学习">storm学习</h1><h1 id="原理和实例部分">原理和实例部分</h1><h2 id="流式框架对比">流式框架对比</h2><p>Hadoop只能处理适合进行批量计算的需求；Storm用来解决分布式流式计算系统。除此之外，流计算还有spark streaming和flink。对比：</p><figure><img src="/images/15152262382414/15152288920269.jpg" alt="-w389"><figcaption>-w389</figcaption></figure><ul><li>如果你想要的是一个允许增量计算的高速事件处理系统，Storm会是最佳选择。</li><li>如果你必须有状态的计算，恰好一次的递送，并且不介意高延迟的话，那么可以考虑Spark Streaming，特别如果你还计划图形操作、机器学习或者访问SQL的话，Apache Spark的stack允许你将一些library与数据流相结合(Spark SQL，Mllib，GraphX)，它们会提供便捷的一体化编程模型。尤其是数据流算法(例如：K均值流媒体)允许Spark实时决策的促进。</li><li>Flink支持增量迭代，具有对迭代自动优化的功能，在迭代式数据处理上，比Spark更突出，Flink基于每个事件一行一行地流式处理，真正的流式计算，流式计算跟Storm性能差不多，支持毫秒级计算，而Spark则只能支持秒级计算。</li></ul><h2 id="storm-主要概念">storm 主要概念</h2><p>Storm采用的是Master-Slave结构，就是使用一个节点来管理整个集群的运行状态。Master节点被称为：Nimbus，Slave节点用来维护每台机器的状态，被称为Supervisor。</p><ul><li>Nimbus的角色是只负责一些管理性的工作，它并不关心Worker之间的数据是如何传输的。</li><li>Supervisor的角色是听Nimbus的话，来启动并监控真正进行计算的Worker的进程。</li><li>Worker：运行在工作节点上面，被Supervisor守护进程创建的用来干活的JVM进程。一个Worker里面不会运行属于不同的topology的执行任务。</li><li>拓扑（topology）：在Storm中，先要设计一个用于实时计算的图状结构，我们称之为拓扑（topology）。这个拓扑将会被提交给集群，由集群中的主控节点（master node）分发代码，将任务分配给工作节点（worker node）执行。一个拓扑中包括spout和bolt两种角色。运行Topology：把代码以及所依赖的jar打进一个jar包，运行<code>strom jar all-your-code.jar backtype.storm.MyTopology arg1 arg2</code>。这个命令会运行主类:backtype.strom.MyTopology，参数是arg1, arg2。这个类的main函数定义这个topology并且把它提交给Nimbus。storm jar负责连接到nimbus并且上传jar文件。</li><li>数据源节点Spout：发送消息，负责将数据流以tuple元组的形式发送出去。</li><li>普通计算节点Bolt：负责转换这些数据流，在bolt中可以完成计算、过滤等操作，bolt自身也可以随机将数据发送给其他bolt。</li><li>记录Tuples：由spout发射出的tuple是不可变数组，对应着固定的键值对。</li><li><p>tuple：storm使用tuple来作为它的数据模型。每个tuple是一堆值，每个值有一个名字，并且每个值可以是任何类型。Tuple本来应该是一个Key-Value的Map，由于各个组件间传递的tuple的字段名称已经事先定义好了，所以Tuple只需要按序填入各个Value，所以就是一个Value List。一个Tuple代表数据流中的一个基本的处理单元，例如一条cookie日志，它可以包含多个Field，每个Field表示一个属性。 <img src="/images/15152262382414/15153820369981.jpg"></p></li><li><p>Stream：一个没有边界的、源源不断的、连续的Tuple序列就组成了Stream。</p></li></ul><p><img src="/images/15152262382414/15152291458332.jpg"></p><h2 id="一个简单的topology">一个简单的Topology</h2><p>看一下storm-starter里面的ExclamationTopology:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TopologyBuilder builder =<span class="keyword">new</span> TopologyBuilder();</span><br><span class="line">builder.setSpout(<span class="number">1</span>, <span class="keyword">new</span> TestWordSpout(),<span class="number">10</span>);</span><br><span class="line">builder.setBolt(<span class="number">2</span>, <span class="keyword">new</span> ExclamationBolt(),<span class="number">3</span>).shuffleGrouping(<span class="number">1</span>);</span><br><span class="line">builder.setBolt(<span class="number">3</span>, <span class="keyword">new</span> ExclamationBolt(),<span class="number">2</span>).shuffleGrouping(<span class="number">2</span>);</span><br></pre></td></tr></table></figure><ul><li>setSpout和setBolt的三个参数：指定的id、包含处理逻辑的对象(spout或者bolt)、并行度（可选）。</li><li>spout要实现IRichSpout的接口；bolt要实现IRichBolt接口。</li><li>并行度表示集群里面需要多少个thread来一起执行这个节点；如果你忽略它，那么storm会分配一个线程来执行这个节点。</li><li>setBolt方法返回一个InputDeclarer对象，这个对象是用来定义Bolt的输入。</li><li>shuffleGrouping表示所有的tuple会被随机的分发给bolt的所有task。<strong>给task分发tuple的策略有很多种，后面会介绍：</strong></li></ul><p>这个Topology包含一个Spout和两个Bolt。这里第一个Bolt声明它要读取spout所发射的所有的tuple — 使用shuffle grouping。而第二个bolt声明它读取第一个bolt所发射的tuple。Spout发射单词，每个bolt在每个单词后面加个”!!!”。这三个节点被排成一条线: spout发射单词给第一个bolt， 第一个bolt然后把处理好的单词发射给第二个bolt。如果spout发射的单词是[&quot;bob&quot;]和[&quot;john&quot;], 那么第二个bolt会发射[&quot;bolt!!!!!!&quot;]和[&quot;john!!!!!!&quot;]出来。</p><p>如果想第二个bolt读取spout和第一个bolt所发射的所有的tuple， 那么应该这样定义第二个bolt:<code>builder.setBolt(3,new ExclamationBolt(),5).shuffleGrouping(1).shuffleGrouping(2);</code></p><p>TestWordSpout从[&quot;nathan&quot;, &quot;mike&quot;, &quot;jackson&quot;, &quot;golda&quot;, &quot;bertels&quot;]里面随机选择一个单词发射出来。TestWordSpout里面的nextTuple()方法是这样定义的：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">nextTuple</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    Utils.sleep(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">final</span> String[] words=<span class="keyword">new</span> String[]&#123;<span class="string">"nathan"</span>,<span class="string">"mike"</span>,<span class="string">"jackson"</span>,<span class="string">"golda"</span>,<span class="string">"bertels"</span>&#125;;</span><br><span class="line">    <span class="keyword">final</span> Random rand =<span class="keyword">new</span> Random();</span><br><span class="line">    <span class="keyword">final</span> String word = words[rand.nextInt(words.length)];</span><br><span class="line">    _collector.emit(newValues(word));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ExclamationBolt把”!!!”拼接到输入tuple后面。实现：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">ExclamationBolt</span> <span class="keyword">implements</span> <span class="title">IRichBolt</span> </span>&#123;</span><br><span class="line">    OutputCollector _collector;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">prepare</span><span class="params">(Map conf, TopologyContext context, OutputCollector collector)</span></span>&#123;</span><br><span class="line">        _collector = collector;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">execute</span><span class="params">(Tuple tuple)</span> </span>&#123;</span><br><span class="line">        _collector.emit(tuple,<span class="keyword">new</span> Values(tuple.getString(<span class="number">0</span>) +<span class="string">"!!!"</span>));</span><br><span class="line">        _collector.ack(tuple);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">cleanup</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">declareOutputFields</span><span class="params">(OutputFieldsDeclarer declarer)</span> </span>&#123;</span><br><span class="line">        declarer.declare(newFields(<span class="string">"word"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>prepare方法提供给bolt一个Outputcollector用来发射tuple。Bolt可以在任何时候发射tuple: 在prepare, execute或者cleanup方法里面, 或者甚至在另一个线程里面异步发射。这里prepare方法只是简单地把OutputCollector作为一个类字段保存下来给后面execute方法使用。</li><li>execute方法从bolt的一个输入接收tuple(一个bolt可能有多个输入源). ExclamationBolt获取tuple的第一个字段，加上”!!!”之后再发射出去。如果一个bolt有多个输入源，可以通过调用Tuple#getSourceComponent方法来知道它是来自哪个输入源的。execute方法里面还有其它一些事情值得一提：输入tuple被作为emit方法的第一个参数，并且输入tuple在最后一行被ack。<strong>这些呢都是Storm可靠性API的一部分，后面会解释。</strong></li><li>cleanup方法在bolt被关闭的时候调用， 它应该清理所有被打开的资源。但是集群不保证这个方法一定会被执行。比如执行task的机器down掉了，那么根本就没有办法来调用那个方法。cleanup设计的时候是被用来在local mode的时候才被调用(也就是说在一个进程里面模拟整个storm集群), 并且你想在关闭一些topology的时候避免资源泄漏。</li><li>declareOutputFields定义一个叫做”word”的字段的tuple。</li></ul><p>storm的运行有两种模式: 本地模式和分布式模式。本地模式主要用于开发测试。</p><ul><li>本地模式：运行storm-starter里面的topology的时候，它们就是以本地模式运行的，可以看到topology里面的每一个组件在发射什么消息。</li><li>分布式模式：storm由一堆机器组成。当提交topology给master的时候，同时也需要提交topology的代码。master负责分发代码，并且负责给topolgoy分配工作进程。如果一个工作进程挂掉了，master节点会重新分配到其它节点。</li></ul><p>下面是以本地模式运行ExclamationTopology的代码:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Config conf =<span class="keyword">new</span> Config();</span><br><span class="line">conf.setDebug(<span class="keyword">true</span>);</span><br><span class="line">conf.setNumWorkers(<span class="number">2</span>);</span><br><span class="line">LocalCluster cluster =<span class="keyword">new</span> LocalCluster();</span><br><span class="line">cluster.submitTopology(<span class="string">"test"</span>, conf, builder.createTopology());</span><br><span class="line">Utils.sleep(<span class="number">10000</span>);</span><br><span class="line">cluster.killTopology(<span class="string">"test"</span>);</span><br><span class="line">cluster.shutdown();</span><br></pre></td></tr></table></figure><p>首先， 这个代码定义通过定义一个LocalCluster对象来定义一个进程内的集群。提交topology给这个虚拟的集群和提交topology给分布式集群是一样的。通过调用submitTopology方法来提交topology，三个参数：要运行的topology的名字、配置对象、要运行的topology本身。</p><p>topology的名字用来唯一区别一个topology，可以用这个名字来kill这个topology。必须显式的杀掉一个topology， 否则它会一直运行。</p><p>Conf对象可以配置很多东西， 下面两个是最常见的：</p><ul><li>TOPOLOGY_WORKERS(setNumWorkers) 定义集群分配多少个工作进程执行这个topology。topology里面的每个组件都需要线程来执行；每个组件通过setBolt和setSpout来指定需要的线程数；这些线程都运行在工作进程里面。每一个工作进程包含一些节点的一些工作线程。比如，如果你指定300个线程，60个进程， 那么每个工作进程里面要执行6个线程，而这6个线程可能属于不同的组件(Spout, Bolt)。你可以通过调整每个组件的并行度以及这些线程所在的进程数量来调整topology的性能。</li><li>TOPOLOGY_DEBUG(setDebug), 当它被设置成true的话， storm会记录下每个组件所发射的每条消息。这在本地环境调试topology很有用， 但是在线上这么做的话会影响性能的。</li></ul><h2 id="topology的三个组件">Topology的三个组件</h2><p>运行中的Topology主要由以下三个组件组成的：Worker processes（进程）、Executors (threads)（线程）、Tasks。</p><figure><img src="/images/15152262382414/15153930533601.jpg" alt="-w400"><figcaption>-w400</figcaption></figure><p>举例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Config conf =<span class="keyword">new</span> Config();</span><br><span class="line">conf.setNumWorkers(<span class="number">2</span>);</span><br><span class="line">TopologyBuilder builder =<span class="keyword">new</span> TopologyBuilder();</span><br><span class="line">builder.setSpout(<span class="string">"blue-spout"</span>, <span class="keyword">new</span> BlueSpout(),<span class="number">2</span>);</span><br><span class="line">builder.setBolt(<span class="string">"green-bolt"</span>, <span class="keyword">new</span> GreenBolt(),<span class="number">2</span>)</span><br><span class="line">        .setNumTasks(<span class="number">4</span>) <span class="comment">//设置Task数量</span></span><br><span class="line">        .shuffleGrouping(<span class="string">"blue-spout"</span>);</span><br><span class="line">builder.setBolt(<span class="string">"yellow-bolt"</span>,<span class="keyword">new</span> ExclamationBolt(),<span class="number">6</span>).shuffleGrouping(<span class="string">"green-bolt"</span>);</span><br></pre></td></tr></table></figure><p>对应的Worker processes（进程）、Executors (threads)（线程）、Tasks数量。指定了2个Worker。共2+2+6=10个Executor线程，每个Worker5个（图中未画出来）。绿色指定了Task数量为4，蓝色和黄色没有指定。</p><figure><img src="/images/15152262382414/15153932587430.jpg" alt="-w600"><figcaption>-w600</figcaption></figure><h2 id="流分组策略stream-grouping">流分组策略(Stream grouping)</h2><p>流分组策略告诉topology如何在两个组件之间发送tuple。spouts和bolts以很多task的形式在topology里面同步执行。如果从task的粒度来看一个运行的topology，它应该是这样的:</p><figure><img src="/images/15152262382414/15153941313422.jpg" alt="-w502"><figcaption>-w502</figcaption></figure><p>当Bolt A的一个task要发送一个tuple给Bolt B， 它应该发送给Bolt B的哪个task呢？下面是一些常用的 “路由选择” 机制：</p><ul><li>ShuffleGrouping：随机选择一个Task来发送。</li><li>FiledGrouping：根据Tuple中Fields来做一致性hash，相同hash值的Tuple被发送到相同的Task。</li><li>AllGrouping：广播发送，将每一个Tuple发送到所有的Task。</li><li>GlobalGrouping：所有的Tuple会被发送到某个Bolt中的id最小的那个Task。</li><li>NoneGrouping：不关心Tuple发送给哪个Task来处理，等价于ShuffleGrouping。</li><li>DirectGrouping：直接将Tuple发送到指定的Task来处理。</li></ul><h2 id="使用其他的语言来定义bolt">使用其他的语言来定义Bolt</h2><p>Bolt可以使用任何语言来定义。用其它语言定义的bolt会被当作子进程(subprocess)来执行， storm使用JSON消息通过stdin/stdout来和这些subprocess通信。这个通信协议是一个只有100行的库，storm团队给这些库开发了对应的Ruby, Python和Fancy版本。</p><h2 id="可靠的消息处理">可靠的消息处理</h2><p>Storm允许用户在Spout中发射一个新的源Tuple时为其指定一个MessageId，这个MessageId可以是任意的Object对象。多个源Tuple可以共用同一个MessageId，表示这多个源Tuple对用户来说是同一个消息单元。Storm的可靠性是指Storm会告知用户，每一个消息单元是否在一个指定的时间内被完全处理。完全处理的意思是该MessageId绑定的源Tuple以及由该源Tuple衍生的所有Tuple，都经过了Topology中每一个应该到达的Bolt的处理。</p><p><img src="/images/15152262382414/15153960805842.jpg"></p><p>在Spout中由<code>message 1</code>绑定的tuple1和tuple2分别经过bolt1和bolt2的处理，然后生成了两个新的Tuple，并最终流向了bolt3。当bolt3处理完之后，称message 1被完全处理了。</p><p>Storm中的每一个Topology中都包含有一个Acker组件。Acker组件的任务就是跟踪从Spout中流出的每一个messageId所绑定的Tuple树中的所有Tuple的处理情况。如果在用户设置的最大超时时间内这些Tuple没有被完全处理，那么Acker会告诉Spout该消息处理失败，相反则会告知Spout该消息处理成功。</p><h1 id="storm接口详解">Storm接口详解</h1><h2 id="icomponent接口">IComponent接口</h2><p>Spout和Bolt都是其Component。所以，Storm定义了一个名叫IComponent的总接口。IComponent的继承关系如下图所示：</p><p><img src="/images/15153963579254/15153981288478.jpg"></p><p>绿色部分是我们最常用、比较简单的部分。红色部分是与事务相关。BaseComponent是Storm提供的“偷懒”的类。为什么这么说呢，它及其子类，都或多或少实现了其接口定义的部分方法。这样我们在用的时候，可以直接继承该类，而不是自己每次都写所有的方法。但值得一提的是，BaseXXX这种定义的类，它所实现的方法，都是空的，直接返回null。</p><h2 id="spout">Spout</h2><figure><img src="/images/15153963579254/15153981837831.jpg" alt="-w535"><figcaption>-w535</figcaption></figure><p><img src="/images/15153963579254/15153981996980.jpg"></p><p>各个接口说明：</p><ul><li>open方法：是初始化动作。允许你在该spout初始化时做一些动作，传入了上下文，方便取上下文的一些数据。</li><li>close方法：在该spout关闭前执行，但是并不能得到保证其一定被执行。spout是作为task运行在worker内，在cluster模式下，supervisor会直接kill -9 woker的进程，这样它就无法执行了。而在本地模式下，只要不是kill -9, 如果是发送停止命令，是可以保证close的执行的。</li><li>activate和deactivate方法 ：一个spout可以被暂时激活和关闭，这两个方法分别在对应的时刻被调用。</li><li>nextTuple方法：负责消息的接入，执行数据发射。是Spout中的最重要方法。</li><li>ack(Object)方法：传入的Object其实是一个id，唯一表示一个tuple。该方法是这个id所对应的tuple被成功处理后执行。</li><li>fail(Object)方法：同ack，只不过是tuple处理失败时执行。</li></ul><p>如果继承了BaseRichSpout，就不用实现close、activate、deactivate、ack、fail和getComponentConfiguration方法，只关心最基本核心的部分。</p><p>总结：通常情况下（Shell和事务型的除外），实现一个Spout，可以直接实现接口IRichSpout，如果不想写多余的代码，可以直接继承BaseRichSpout。</p><h2 id="bolt">Bolt</h2><p>类图如下图所示：</p><figure><img src="/images/15153963579254/15153983422720.jpg" alt="-w577"><figcaption>-w577</figcaption></figure><p><img src="/images/15153963579254/15153983724519.jpg"></p><ul><li>prepare方法：IBolt继承了java.io.Serializable，我们在nimbus上提交了topology以后，创建出来的bolt会序列化后发送到具体执行的worker上去。worker在执行该Bolt时，会先调用prepare方法传入当前执行的上下文。</li><li>execute方法：接受一个tuple进行处理，并用prepare方法传入的OutputCollector的ack方法（表示成功）或fail（表示失败）来反馈处理结果。</li><li>cleanup方法：同ISpout的close方法，在关闭前调用。同样不保证其一定执行。</li></ul><p>Bolt实现时一定要注意execute方法。为什么IBasicBolt并没有继承IBolt？Storm提供了IBasicBolt接口，其目的就是实现该接口的Bolt不用在代码中提供反馈结果了，Storm内部会自动反馈成功。如果你确实要反馈失败，可以抛出FailedException。</p><p>总结：通常情况下，实现一个Bolt，可以实现IRichBolt接口或继承BaseRichBolt，如果不想自己处理结果反馈，可以实现IBasicBolt接口或继承BaseBasicBolt，它实际上相当于自动做掉了prepare方法和collector.emit.ack(inputTuple)；</p><h2 id="部署">部署</h2><ul><li>本地打jar：<code>mvn clean install -DskipTests=true</code>,jar包会打到<code>$HOME/.m2/repository</code>目录</li><li>为集群打包（包含其他依赖）：<code>mvn package</code>--&gt;<code>target/storm-starter-{version}.jar</code></li><li>local模式执行：<code>storm jar target/storm-starter-*.jar org.apache.storm.starter.ExclamationTopology -local</code></li><li>集群模式执行，名称为production-topology:<code>storm jar target/storm-starter-*.jar org.apache.storm.starter.RollingTopWords production-topology</code></li></ul><h1 id="参考">参考</h1><ul><li><a href="http://www.cnblogs.com/quchunhui/p/5370191.html" target="_blank" rel="noopener">Storm入门学习随记</a></li><li><a href="http://blog.csdn.net/cm_chenmin/article/details/53072498" target="_blank" rel="noopener">流式大数据处理的三种框架：Storm，Spark和Flink</a></li><li><a href="http://www.aboutyun.com/thread-7394-1-1.html" target="_blank" rel="noopener">storm 入门原理介绍</a></li><li><a href="https://pan.baidu.com/s/1boRcCeb" target="_blank" rel="noopener">细细品味Storm_Storm简介及安装V1.1.pdf</a></li><li><a href="https://www.cnblogs.com/zlslch/p/5965927.html" target="_blank" rel="noopener">storm的topology提交执行</a></li><li><a href="https://github.com/apache/storm/tree/master/examples/storm-starter" target="_blank" rel="noopener">Example Storm Topologies</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;storm学习&quot;&gt;storm学习&lt;/h1&gt;
&lt;h1 id=&quot;原理和实例部分&quot;&gt;原理和实例部分&lt;/h1&gt;
&lt;h2 id=&quot;流式框架对比&quot;&gt;流式框架对比&lt;/h2&gt;
&lt;p&gt;Hadoop只能处理适合进行批量计算的需求；Storm用来解决分布式流式计算系统。除此之外，流计算
      
    
    </summary>
    
      <category term="大数据" scheme="http://www.datadriven.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="大数据" scheme="http://www.datadriven.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
</feed>
