<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[基于时间序列数据的异常识别模型]]></title>
    <url>%2F2018%2F11%2F09%2F%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%95%B0%E6%8D%AE%E7%9A%84%E5%BC%82%E5%B8%B8%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[1. 概述 大型集群系统中，可能存在软件问题和硬件问题导致的系统故障，严重影响了系统的高可用性。这就要求7*24小时，对系统不间断监控。这就意味着需要不间断地监控大量时间序列数据，以便检测系统潜在的故障和异常现象。然而，实际当中的系统异常很多，且不容易发现；从而导致人工方式监控方式效率很低。 异常场景本质上是一个或者多个数据点；数据点一般在系统运行过程中产生，且能反应系统的功能是否正常，多以日志形式呈现。当系统功能发生异常时，就会产生异常数据。快速高效地发现这些异常值，对于快速止损具有重要意义。对此，我们提出一种基于时间序列的异常识别模型，用来及时发现异常。 对于多数系统，一般都有成功率、流量等指标，故障发生时，这些指标也会出现响应的异常。我们将系统成功率、流量统一称为特征值变量，并对其进行建模，从而方便后续其它特征变量的扩展。为了更好地感知这些特征变量的突变，需要对特征变量进行计算处理或者空间转换。那么异常识别问题就转换为以下两个问题： 特征变量的计算处理和转换 突变的判断 针对这两个关键问题，我们将在下文中进行建模和分析。 2. 异常识别 如下图，通过计算器进行特征变量的计算处理和转换，通过异常检测器来判断数值的突变，从而解决上面的两个问题。其中，异常检测器由比较器和决策器组成。 image-20180815093422131 对于给定时间序列二维矩阵\(X=\{x^m_t∈R：∀t≥0, ∀m≥0\}\) ，\(x_t^m\)为\(t\)时刻的第m个指标的真实数据，\(u_t^m\)表示时间\(t\)的\(x_t^m\)的计算值，\(y_t^m\)为第m个指标的输出结果，\(y_t\)为整体预测结果。 \(x_t^m\)通过计算器得到计算值\(u_t^m\)，然后\(x_t^m\) 和 \(u_t^m\)分别作为比较器的输入，得到第m个指标的输出\(y_t^m\)。\(y_t^1\),\(y_t^2\)…\(y_t^m\)作为决策器的输入得到\(y_t\)。\(y_t\)是一个二元值，可以用TRUE（表示输出数据正常），FALSE（表示输入数据异常）表示。下面对计算器和检测器进行说明。 2.1 计算器 计算器用来对输入值\(x_t^m\) 进行计算或者空间转换，从而得到特征变量的计算值\(u_t^m\)。一般情况下，特征变量具有趋势性、周期性等特征。基于这些特征，计算值的获取，可以使用以下三种方式：累计窗口均值计算器、基于趋势性的环比计算器、基于周期性的同比计算器。 2.1.1 累积窗口均值计算器 输入值为\(x_t\)（为了方便省略指标参数m），如果直接只用单个点\(x_t\)的抖动来判断，受噪声影响较大。因此，使用累积窗口均值的方式： \[ u(t)={\dfrac{x_t+x_{t-1}+...+x_{t-w+1}}{w}} \tag{1}\] 其中，\(w\)为累计窗口的大小。通过窗口平滑之后，会过滤掉尖刺等噪声。 2.1.2 基于趋势性的计算器 为了描述数据的趋势性，引入环比类算法。对\(x_t\)进行空间转换，得到环比，再使用检测器进行检测。 \[u(t)={\dfrac{x_t+x_{t-1}+...+x_{t-w+1}}{x_{t-w}+x_{t-w-1}+...+x_{t-2w+1}}} \tag{2}\] 其中，分子为当前窗口\(w\)内的数据，分母为上一窗口\(w\)内数据。通过窗口\(w\)对数据进行平滑。 2.1.3 基于周期性的计算器 为了描述数据的周期性，引入同比算法。当同比值过大或者过小时，认为发生故障。同比公式如下： \[u(t)={\dfrac{x_t+x_{t-1}+...+x_{t-w+1}}{x_{t-kT}+x_{t-kT-1}+...+x_{t-kT-w+1}}} \tag{3} \] 其中\(T\)为周期，\(k\)表示第几个周期。一般选取\(k\)为1、7、30，来表示昨天、上周、上个月。 2.1.4 其他类型计算器 计算器还可以使用其他算法，包括： 统计类算法：包括同比、环比算法的改进，或者其他统计算法。此时，计算器的输出结果为预测值，预测值和输入值进行比较即可。 时序型算法：包含ARIMA、Holter-Winter等时序型算法。计算器的输出结果为预测值。 机器学习：根据有监督、无监督、深度学习(LSTM)等算法，训练出的模型即为计算器。此时，计算器的输出结果一般为归一化的值，根据归一化的值进行比较。 这些算法，在这里不再做深入研究和阐述。 2.2 异常检测器 当数据出现异常时，计算值会出现较大偏差，该偏差由异常检测器来判断。异常检测器由比较器和决策器组成，计算值和真实值通过该模块后，得到最终预测结果。 2.2.1 比较器 比较器的本质是求解如下公式的过程： \[f(x^m_t,u^m_t;h^m)\ \ = \ \ boolean \tag{4}\] 其中，\(x^m_t\)为真实值，\(u\)为计算值，\(h^m\)为阈值参数，\(boolean\)为结果TRUE/FALSE。真实值已知，计算值通过计算器得到；剩下的阈值参数\(h^m\)，则需要根据故障发生时的实际值进行参数估计。 很多场景下，该公式还可以简化为：\(f(u^m_t;h^m)\ \ = \ \ boolean\) ，即计算值直接和阈值比较即可。 2.2.1.1 比较器种类 比较器有两种：相对值比较器和绝对值比较器。给定计算值\(u^m_t\)和输入值\(x^m_t\)，得到绝对值比较器： \[f= x_t^m−u_t^m\ \ opretor \ \ h^m \tag{5}\] 其中，\(opretor\)为比较操作符，比如&gt; &lt; &gt;= &lt;=。由于\(u_t\)由\(x_t\)得到，所以很多情况下公式可以简化为 $ u_t^m opretor h_t^m$，即确定计算值的阈值即可。 对于一些场景来说，需要捕获特征变量的相对性。因此，引入相对值比较器： \[ f={\dfrac{x_t^m−u_t^m}{u_t^m}}\ \ opretor \ \ h^m \tag{6} \] 通过对相对值比较器进行阈值处理，既可以检测异常值，同时还能对期望值进行归一化。 2.2.1.2 比较器阈值\(h\)的选取 一般情况下，阈值参数决定了异常检测模块的敏感度。最优阈值的选择，取决于数据分布的性质以及先验数据。一般情况下，阈值的选取方法为： 方法一：跟踪一组故障数据和正常数据，根据经验估计阈值。 方法二：跟踪一组故障数据和正常数据，根据经验，并结合3σ准则确定，来确定阈值。（特征变量或者特征变量的组合，服从正态分布） 2.2.2 决策器 如下公式，基于逻辑操作符，对比较器结果进行合并. 方式一：逻辑组合 \[ y_t=y_t^1 \ \ \&amp;| \ \ y_t^2 \ \ \&amp;| \ \ y_t^3 \ \ \&amp;| \ \ ... \ \ y_t^m \tag{7}\] 其中，\(|\)表示逻辑或操作，\(\&amp;\)表示逻辑与操作。 方式二：权重设置法 \[y_t=k_1*y_t^1 \ \ + \ \ k_2*y_t^2 \ \ + \ \ k_3*y_t^3 \ \ + \ \ ... \ \ k_m* y_t^m \tag{8}\] 其中，\(k_m\)为系数，这种方式一般适合基本无负样本的场景，参数的确定需要使用层次分析法，将在后面的文章进行说明。 3. 故障止损 上面主要阐述了异常识别的方式。如果条件过于严格，刚开始并不容易被识别出来；如果条件过松，可能导致误识别。对此，我们将止损策略分为两级： 级别一：预警。对于不能完全确定故障发生的场景，使用级别一。 级别二：预警+止损（踢IDC）。对于能确定IDC故障的场景，使用级别二。 4. 实际场景应用 下面通过一个规则的场景，进行举例说明。假如存在如下异常场景： image-20181108211144340 体现在模型中，则级别一（预警）的模型图 image-20180815093336600 级别二（预警+踢IDC）的模型图： image-20180815093352931 最终，得到故障识别规则： 级别一触发条件: \(u_1&lt;h_1 \ \ | \ \ (u5&lt;h5 \ \ \&amp; \ \ u6&lt;h6 \ \ \&amp; \ \ u7&lt;h7 )\) 级别二触发条件：\(u_1&lt;h_2 \ \ \&amp; \ \ u_3&gt;h_3​\) 其中，\(h_1, h_2,h_3,h_5,h_6,h_7\)为阈值参数。需要结合经验和实际数据估计得到。 5. 小结 本文主要基于时间序列的数据，提出了异常场景识别模型，并重点对基于规则的识别进行了说明。 参考 Generic and Scalable Framework for Automated Time-series Anomaly Detection]]></content>
      <categories>
        <category>数据驱动</category>
      </categories>
      <tags>
        <tag>数据驱动</tag>
        <tag>时间序列</tag>
        <tag>异常识别</tag>
        <tag>模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[各公司智能运维 AIOPs架构分析和对比]]></title>
    <url>%2F2018%2F07%2F08%2F%E5%90%84%E5%85%AC%E5%8F%B8%E6%99%BA%E8%83%BD%E8%BF%90%E7%BB%B4-AIOPs%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90%E5%92%8C%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[各公司智能运维 AIOPs架构分析和对比 [TOC] 数据驱动的智能运维平台 ★★★★★ 什么是AIPOs 在定义 AIOps 时画了一张图，除了中间有机器学习、BigData、Platform 外，外层的内容就是监管控，这也就是做 AIOps 的目的。只不过是在做监管控时，要使用一些新的方式，以减轻运维的工作量。 QQ20180703-164936 机器学习的运用算法分类 这个可以用来做系统设计的参考 QQ20180703-163744 典型应用场景 主要包含：时序预测、异常检测、模式聚类。 时序预测 （eg：Holt-Winters），开源选择： QQ20180703-164402 异常检测 分析思路：Basic 采用的是四分位方法，Agile 用的是 SARIMA 算法，Robust 用的是趋势分解，Adaptive 在我看起来，采用的是 sigma 标准差。如下图： QQ20180703-171236 对于异常检测的开源库选择，有些是原子的，有些是组合的。Etsy 的 skyline 是比较高级的场景，里面带有数据存储、异常检测分析、告警等；Twitter、Netflix、Numenta 是纯粹的机器学习算法库，没有任何附加内容；Yahoo 的 egads 库可以算是异常检测的原子场景，比 Twitter 和 Netflix 层级稍高。开源选择： QQ20180703-172107 另外这部分，还提到了很多场景案例的分析和对比。 监控的目标需要直达业务结果，业务量下跌即为出现故障，虽然故障可能不是由于系统本身引起，但仍需要发现并定位该故障。如此将一个对业务的监控通过以下流程进行转换，首先对故障进行等级定义，对时间序列的业务指标监控，定位异常点，做出故障通告。 模式聚类 第一个，像 Num、Date、IP、ID 等都是运维 IT 日志里一定会出现的，但在关注模式时不会关注这些。因此，可以在开始就把这些信息替换，节省工作量。 第二个是对齐，对齐也是耗资源的，如何减少对齐的时候强行匹配资源呢？可以开始先走一个距离极其小的聚类，这样每一类中的原始文本差异非常小。此时意味着第二步得到的最底层聚类去做对齐时，在这个类里的对齐耗损就会非常小，可以直接做模式发现。 到第四步的时候，虽然还是聚类，但是消耗的资源已经非常少，因为给出的数据量已经很小，可以快速完成整个速度的迭代。 QQ20180703-173508 ##《企业级 AIOps 实施建议》白皮书 ★★★★★ AIOPs的白皮书，关键内容： 能力框架 学件抽象 学件，亦称 AI 运维组件，类似程序中的 API 或公共库，但 API 及公共库不含具体业务数据，只是某种算法，而 AI 运维组件（或称学件），则是在类似 API 的基础上，兼具对某个运维场景智能化解决的“记忆”能力，将处理这个场景的智能规则保存在了这个组件中。学件（Learnware）= 模型（ model ） + 规 约 （ specification ） ， 具 有 可 重 用 、 可 演 进 、 可 了 解 的 特 性 。 学件类似于百度AIOPs的机器人，但是，角度不同，含义也不同。 能力分级 QQ20180703-174341 整体架构 QQ20180703-174440 “可重用”的特性使得能够获取大量不同的样本； “可演进”的特性使得可以适应环境的变化； “可了解”的特性使得能有效地了解模型的能力。 关键运维场景和经历阶段 见百度部分的场景图 QQ20180705-142858 平台架构 整体功能架构(模块) QQ20180703-174801 AI部分架构 QQ20180703-174949 如上面的两张架构图，具体的工具或者产品应具备以下功能或模块： 交互式建模功能：该功能支持用户在平台上交互式的进行模型的开发调试，通过简单的方法配置完成模型的构建。 算法库：用户可以在算法库中找到常见常用的算法直接使用，算法按照用途分类， 以供用户方便的使用。 样本库：样本库用于管理用户的样本数据，供用户建模时使用，支持样本的增删改查等基本操作。 数据准备：该功能支持用户对数据进行相关的预处理操作，包括关联、合并、分支路由、过滤等。 灵活的计算逻辑表达：在基本常用的节点功能之外，用户还需要自由的表达一些计算逻辑，该需求主要是通过让用户写代码或表达式来支持。 可扩展的底层框架支持：平台本身要能够灵活的支持和兼容多种算法框架引擎，如Spark、TensorFlow 等，以满足不同的场景以及用户的需求。 数据分析探索：该功能是让用户能够方便快捷地了解认识自己的数据，用户只有基于对数据充分的认识与理解，才能很好的完成模型的构建。 模型评估：对模型的效果进行评估的功能，用户需要依据评估的结论对模型进行调整。 参数以及算法搜索：该功能能够自动快速的帮助用户搜索算法的参数，对比不同的算法，帮助用户选择合适的算法以及参数，辅助用户建模。 场景模型：平台针对特定场景沉淀的解决方案，这些场景都是通用常见的，用户可以借鉴参考相关的解决方案以快速的解决实际问题 实验报告：模型除了部署运行，相关挖掘出来的结论也要能够形成报告，以供用户导出或动态发布使用。 模型的版本管理：模型可能有对个不同的版本，线上运行的模型实例可能分属各个不同的版本，版本管理支持模型不同版本构建发布以及模型实例版本切换升级等。 模型部署应用：模型构建完成后需要发布应用，模型部署应用功能支持模型的实例化，以及相关计算任务的运行调度管理。 数据质量保障：全链路的数据监控，能够完整的掌控数据的整个生命周期，具备对丢失的数据执行回传补录的能力，保障数据的可用性。 AIOPs的团队角色 QQ20180703-175610 常见场景和分类 效率提升、质量保障、成本管理。我们的重点在于质量管理，还有一部分的效率提升。 QQ20180703-175704 3个场景的五个阶段 详见附件中的表格。 实施和关键技术 数据平台、需要用到的关键算法。—重点在于怎么对算法进行抽象，得到算法的公共层，从而各个垂直场景中都可以使用。 QQ20180703-195515 应用案例： 时间序列异常检测（腾讯）：有监督算法+无监督算法。 根源告警（京东） 单机房故障自愈（百度）：详见调度部分 面向 AIOps 的算法技术 运维场景通常无法直接基于通用的机器学习算法以黑盒的方式解决，因此需要一些面向AIOps 的算法技术，作为解决具体运维场景的基础。有时一个算法技术还可用于支撑另外一个算法技术。 常见的面向 AIOps 的算法技术包括： 指标趋势预测：通过分析指标历史数据，判断未来一段时间指标趋势及预测值，常见有 Holt-Winters、时序数据分解、ARIMA 等算法。该算法技术可用于异常检测、容量预测、容量规划等场景。 指标聚类: 根据曲线的相似度把多个 KPI 聚成多个类别。该算法技术可以应用于大规模的指标异常检测：在同一指标类别里采用同样的异常检测算法及参数，大幅降低训练和检测开销。常见的算法有 DBSCAN, K-medoids, CLARANS 等，应用的挑战是数据量大，曲线模式复杂。 多指标联动关联挖掘: 多指标联动分析判断多个指标是否经常一起波动或增长。该算法技术可用于构建故障传播关系，从而应用于故障诊断。常见的算法有 Pearson correlation, Spearman correlation, Kendall correlation 等，应用的挑战为 KPI 种类繁多，关联关系复杂。 指标与事件关联挖掘: 自动挖掘文本数据中的事件与指标之间的关联关系（ 比如在程序 A 每次启动的时候 CPU 利用率就上一个台阶）。该算法技术可用于构建故障传播关系，从而应用于故障诊断。常见的算法有 Pearson correlation, J-measure, Two-sample test 等，应用的挑战为事件和 KPI 种类繁多，KPI 测量时间粒度过粗会导致判断相关、先后、单调关系困难。 事件与事件关联挖掘: 分析异常事件之间的关联关系，把历史上经常一起发生的事件关联在一起。该算法技术可用于构建故障传播关系，从而应用于故障诊断。常见的算法有 FP-Growth, Apriori，随机森林等，但前提是异常检测需要准确可靠。 故障传播关系挖掘：融合文本数据与指标数据，基于上述多指标联动关联挖掘、指标与事件关联挖掘、事件与事件关联挖掘等技术、由 tracing 推导出的模块调用关系图、辅以服务器与网络拓扑，构建组件之间的故障传播关系。该算法技术可以应用于故障诊断，其有效性主要取决于其基于的其它技术。 百度智能运维 ★★★★★ 参考： 百度智能运维AIOPs技术沙龙关键PPT.pdf 百度智能运维的技术演进之路 百度智能运维 | 框架在手，AI我有 智能运维分级标准(阶段) QQ20180703-203026 智能运维架构——场景象限划分 高频 X 复杂 image-20180702142018403 智能运维架构——工程思想 image-20180702143030520 运维工程架构——整体框架 image-20180702141741650 智能运维架构——技术栈(技术架构) image-20180702143355014 4.1 运维知识库（运维数据） 所有要处理的数据都来自知识库，以及所有处理后的数据也都会再进入到知识库中。知识库由三部分组成，分别是分为元数据(MetaDB)、状态数据(TSDB)和事件数据(EventDB)。持续的数据建设，是智能运维建设的关键。 QQ20180703-202209 4.2 运维工程研发框架 每个运维智能操作都可以分解成感知、决策、执行这样一个标准流程，这样一个流程叫做智能运维机器人。运维工程研发框架提供感知、决策、执行过程常用的组件，便于用户快速构建智能运维机器人。 image-20180702144434325 1、感知方面，智能异常检测算法替代过去大量误报漏报的阈值检测方法； 2、决策方面，具备全局信息、自动决策的算法组件替代了过去“老中医会诊”的人工决策模式； 3、执行方面，状态机等执行长流程组件的加入，让执行过程可定位、可复用。 image-20180627160312805 感知器是运维机器人的眼睛和耳朵。就像人有两个眼睛和两个耳朵一样。运维机器人也可以挂载多个感知器来获取不同事件源的消息，比如监控的指标数据或者是报警事件，变更事件这些，甚至可以是一个定时器。这些消息可以以推拉两种方式被感知器获取到。这些消息也可以做一定的聚合，达到阈值再触发后续处理。 决策器是运维机器人的大脑，所以为了保证决策的唯一，机器人有且只能有一个决策器。决策器也是使用者主要要扩展实现的部分。除了常见的逻辑判断规则之外，未来我们还会加入决策树等模型，让运维机器人自主控制决策路径。 执行器是运维机器人的手脚，所以同样的，执行器可以并行的执行多个不同的任务。执行器将运维长流程抽象成状态机和工作流两种模式。这样框架就可以记住当前的执行状态，如果运维机器人发生了故障迁移，还可以按照已经执行的状态让长流程断点续起。 4.3 运维开发框架 QQ20180703-201909 把线上环境看做一个黑盒服务，那么我们对它的操作无非读写两类: 所谓的写也就是操作控制流，是那种要对线上状态做一些改变的操作，我们常说的部署、执行命令，都属于这一类； 另一类是读，指的是数据流，也就是要从线上获取状态数据，并进行一些聚合统计之类的处理，我们常说的指标汇聚、异常检测、报警都在这个里面。 通过运维知识库，可以在这两种操作的基础上，封装出多种不同的运维机器人，对业务提供高效率、高质量以及高可用方面的能力。 4.3 运维大脑 运维大脑包括异常检测和故障诊断，这两个部分的共同基础是基本的恒定阈值异常检测算法。恒定阈值异常检测算法利用多种概率模型估计数据的概率分布，并由此产生报警阈值。 QQ20180704-100911 智能运维实践——故障管理解决方案 故障预防 —&gt;故障发现 —&gt;故障自愈 —&gt;... 5.1故障预防 自动拦截异常变更。 —现阶段我们主要是通过人工流程来预防。 5.2 故障发现 算法自动选择： QQ20180704-101456 5.3 故障自愈 image-20180702203156159 百度异常检测 ★★★★★ 参考 百度智能运维实践之异常检测.pdf 异常检测：百度是这样做的 异常检测 流程 image-20180627204634628 系统架构 image-20180702113603340 恒定阈值算法 累计恒定阈值 用来排除单点抖动。 突升突降算法 r空间：引入周期内累计值 同比算法 z空间：引入分布和数据标准化。 参考：三种常用数据标准化方法 时序数据存储及计算职责 使用专门的时序型数据库。 image-20180702110645458 聚合计算实现 支持场景聚合函数sum/avg/... image-20180702112558001 二次计算实现 image-20180702112434310 百度运维大数据存储平台设计与实践.pdf ★★★★ 重点分析了物理层的设计，包括： 大规模时序数据的存储：层次存储结构：Hadoop--冷数据；Hbase—暖数据；Redis--热数据。 海量运维事件数据存储；EventDB 知识库（元数据管理）：系统架构如下图： image-20180702111933197 智能运维 | 单机房故障自愈，收藏这一篇就够了 ★★★★★ 单机房故障止损的能力标准 QQ20180703-203917 故障自愈的技术架构 QQ20180703-203941 故障自愈的算法 基于容量水位的动态均衡（这个可能是BFE模型中的一部分）。智能IDC、专线路由都可以参考这个方案。 可以参考《企业级 AIOps 实施建议》白皮书中的 案例3 基于容量水位的动态均衡 QQ20180703-204031 基于快速熔断的过载保护 在流量调度时，建立快速的熔断机制作为防止服务过载的最后屏障。一旦出现过载风险，则快速停止流量调度，降低次生故障发生的概率。 基于降级功能的过载保护 在流量调度前，如果已经出现对应机房的容量过载情况，则动态联动对应机房的降级功能，实现故障的恢复。 流量调度 ★★★★★ 参考：百度智能运维AIOPs技术沙龙关键PPT 流量调度部分：重点在于模型的定义。 image-20180702204024368 外卖订单量预测异常报警模型实践 ★★★★★ 这篇介绍了Holt-Winters算法在订单异常检测中的应用。文章关键点在于： 1. 之前问过百度的智能运维，说曾经用过该算法，效果不是很好。但在该文章中效果看似还不错。其原因可能在，该文章对Holt-Winters算法进行了2种方式的精简和改进；这点值得借鉴。 2. 给出了异常检测模型，及模型内的各部件关系。 3. 给出了报警模型结构图。从该图可以看出，要实现整体过程，需要离线计算模块，这一块依赖于Hadoop，是我们还不具备的能力。 异常检测模型 基于预测的异常检测模型如下图所示，xt是真实数据，通过预测器得到预测数据，然后xt和pt分别作为比较器的输入，最终得到输出yt。yt是一个二元值，可以用+1（+1表示输入数据正常），-1（-1表示输入数据异常）表示。 QQ20180703-204703 预测器 QQ20180703-204909 比较器模型 QQ20180703-205017 离散度Filter：根据预测误差曲线离散程度过滤出可能的异常点。一个序列的方差表示该序列离散的程度，方差越大，表明该序列波动越大。如果一个预测误差序列方差比较大，那么我们认为预测误差的报警阈值相对大一些才比较合理。离散度Filter利用了这一特性，取连续15分钟的预测误差序列，分为首尾两个序列（e1,e2），如果两个序列的均值差大于e1序列方差的某个倍数，我们就认为该点可能是异常点。 阈值Filter：根据误差绝对值是否超过某个阈值过滤出可能的异常点。利用离散度Filter进行过滤时，报警阈值随着误差序列波动程度变大而变大，但是在输入数据比较小时，误差序列方差比较小，报警阈值也很小，容易出现误报。所以设计了根据误差绝对值进行过滤的阈值Filter。阈值Filter设计了一个分段阈值函数y=f(x)，对于实际值x和预测值p，只有当|x-p|&gt;f(x)时报警。实际使用中，可以寻找一个对数函数替换分段阈值函数，更易于参数调优。 异常预警模型整体结构图 最终的外卖订单异常报警模型结构图如图所示，每天会有定时Job从ETL中统计出最近10天的历史订单量，经过预处理模块，去除异常数据，经过周期数据计算模块得到周期性数据。对当前时刻预测时，取60分钟的真实数据和周期性数据，经过实时预测模块，预测出当前订单量。将连续15分钟的预测值和真实值通过比较器，判断当前时刻是否异常。 QQ20180703-205152 阿里智能运维 ★★★★ 运维体系职责 QQ20180704-154755 自动化是智能化的前提: 我认为智能化最重要的前提是自动化。如果你的系统还没有完成自动化的过程，我认为就不要去做智能化，你还在前面的阶段。智能化非常多的要求都是自动化，如果不够自动化，意味着后边看起来做了一个很好的智能化的算法等等，告诉别人我能给你很大的帮助，结果发现前面自动化过程还没有做完全。 阿里巴巴智能化运维五步走 在运维这五个领域，我们看到的一些可能性，包括我们正在做的事情。 QQ20180704-155008 资源 DC大脑，让控制更加智能化：谷歌的一篇文章，里面有一个消息透露，他们通过更好的智能化，去控制整个机房的智能等等。比如说控制空调的出口，就是风向往哪边吹，控制这个，然后为谷歌节省了非常多的钱，非常可观。 资源画像让资源更好搭配 监控AI化 智能报警：最火的领域。阿里在尝试的一个方向是让你不要去配，阿里根据分析来决定什么情况下需要报警。 异常检测直接影响到效率：很多公司都在做。 用智能化做好故障定位 智能运维整体架构——统一的大数据运营平台 QQ20180705-151848 智能运维——异常检测算法架构 整体算法框架 整体算法框架如下图所示。 首先对数据进行预处理，包括差值补缺和平滑去噪。 然后基于优化后的时间序列分解Seanonal Trend LOESS方法进行基线拟合，滑动平均使曲线平滑。 然后结合时间序列分析、机器学习以及特征工程中的各种方法，判断一个时间片段是否需要报警。 开始设计时并未确定该算法应采取哪些方法，而是被阿里巴巴各行业的业务、形态各异的数据以及判断标准训练出来的。它的优势在于对各行业的数据有较高的适配性，对非技术性的曲线波动有较强的抗干扰能力。 此外，该算法会输出拟合的基线，并且内部系统中可以通过该基线提前100分钟预测趋势，当然距离越近的预测越准确，预测时会将历史波动和局部变化趋势都考虑在内，每个瞬间都会判断这个时刻是否需要报警。 出现报警后，可以回溯到该报警的开始时间和结束时间。由此达到整体的报警功能。 QQ20180704-174605 集群智能分析架构-异常分析 QQ20180705-152108 智能根因推荐 分析流程 QQ20180704-175204 人工智能驱动大数据 技术主要包含4部分：智能解决方案算法平台、 数据资产 管理平台、数据计算开发平台、数据采集管理平台。 算法平台架构 人工智能解决方案算法平台架构： QQ20180704-162223 数据清洗层架构 整体结构 下图中，模型层对各算法进行了抽象，分为：规则、异常检测算法、监督&amp;无监督算法、图算法等。 QQ20180704-160916 自动化标签工厂架构 流程：基础数据 --&gt; 特征 --&gt; 算法 --&gt; 质量评估 QQ20180704-163658 参考 阿里毕玄：智能时代的新运维 还不知道AIOps嘛？阿里这么火的智能运维，你不能不知道！ 清华裴丹:AIOps落地路线图 ★★★★ 整体流程 AIOps引擎 中的“异常检测”模块在检测到异常之后可以将报警第一时间报给运维人员，达到“故障发现”的效果； “异常定位”模块达到“故障止损”的效果，它会给出一些止损的建议，运维专家看到这个定位之后也许他不知道根因，但是他知道怎么去根据已有的预案来进行止损，然后再执行自动化的脚本。如果是软件上线导致的问题我们回卷，如果业务不允许回卷就赶紧发布更新版本；如果是容量不够了，那我们动态扩容；如果部分软硬件出问题了，我们切换一下流量等等。 AIOps引擎中的“根因分析”模块会找出故障的根因，从而对其进行修复。如果根因是硬件出了问题，像慢性病一样的问题，那我们可以让我们的运维人员去修复。 同时，AIOps 引擎中的“异常预测模块”能够提前预测性能瓶颈、容量不足、故障等，从而实现“故障规避”。比如，如果我们预测出来了设备故障的话，那么可以更新设备；如果说我们发现性能上的瓶颈是代码导致的，那就交给研发人员去修改。 核心的AIOps的引擎会积累一个知识库，从里边不断的学习。也就是说监控数据会给AIOps提供训练数据的基础，然后专家会反馈一部分专家知识，上图是我展望的AIOps大概的体系结构，这里面关键的一点是，我们还是离不开运维专家的。最终的止损、规避的决策、软件的代码修复以及设备的更换还是要靠人来做的，但是机器把绝大部分工作都做了，包括异常检测、异常定位、根因分析、异常预测。 QQ20180704-191853 AI 擅长解决问题 QQ20180704-192310 架构（技术路线图） QQ20180704-192434 各类算法对比 QQ20180705-113940 算法产品 QQ20180705-112614 异常检测 QQ20180704-192520 故障发现 QQ20180704-192606 故障止损 QQ20180704-192803 故障修复 QQ20180704-192914 参考 清华裴丹分享AIOps落地路线图，看智能运维如何落地生根 6、裴丹-AIOps在传统行业的落地探索.pdf 腾讯AIOPs ★★★★★ 智能运维思路 QQ20180705-144343 整体架构 运维平台整体架构 QQ20180705-151143 技术架构 QQ20180705-145301 服务构建 QQ20180705-142230 智能运营白皮书用于衡量智能化产品建设的水平，白皮书中定义了: 三个阶段:半智能、浅智能、智能 六项能力:感知、分析、决策、执行、呈现、干预 时间序列异常检测 3类算法 QQ20180705-150219 经典算法的使用场景：AR/MA/ARMA/ARIMA QQ20180705-144654 时间序列异常检测的技术框架 QQ20180705-145655 3-Sigma算法和控制图算法的优缺点 QQ20180705-145900 无监督学习算法的优缺点 有监督算法 告警收敛根源分析 QQ20180705-150747 参考 3、涂彦-AIOps仅仅是异常检测么 avila--腾讯运维的AI实践v_0.4.pdf ★★★★★ max--复杂业务的自动化运维精髓V2.pdf 360 AIOps ★★★★ 时序序列算法 EWMA(指数加权移动平均) QQ20180705-143453 另外还有各种同环比等算法介绍 机器学习架构 QQ20180705-143825 机器学习算法选择 QQ20180705-143924 参考 4、谭学士-360 AIOps 亮剑网络运维 华为AIOps实践 ★★★★ AIOps现实中面临的痛点、难点 QQ20180705-154148 Serverless环境中因果序列追踪 需求: 对Serverless API调用依赖关系、异常检测、响应时间等的监控和分析 API的高频调用、临时性(使用时创建、空闲时销毁)等特性，调用链成为实 时监控的重要手段 函数trigger:提供日志事件触发函数调用的能力:1)能看到事件来源，便于跟 踪问题 2)供用户自定义功能扩展 多源数据的RCA分析探索 为什么说异常检测是AIOps的第一需求: 有异常才需要操作(Ops) 理解什么是正常pattern 单一时序变量 窗口法: sliding window, xxx window。手工配置大量参数(尤其是窗口大小)，平衡延迟和误报率，即延迟短误报率会高，延迟长误报率低 。 ARIMA、EWMA)等算法 多个时序变量预测单一事件 Hidden Markov Model ： 根据预设的事件依赖关系进行预测。 参数不多，计算量较小，每个节点 意义明确，但准确性非常依赖于预 设的Markov Chain Recurrent Neural Network, e.g. LSTM ：隐变量意义不明确，参数较多，计 算复杂度较大，比较难收敛 聚类算法实现网络包的Blackbox分析 Blackbox算法 ：计算因果路径，即不同业务的服务间调用路径。 基于Hierarchical Clustering实现因果路径推导 参考 1、华为-华为三位一体探索 AIOps 关键技术的实践.pdf 原理部分 Holt-Winters模型原理 Holt-Winters模型原理分析及代码实现（python) ★★★★ Exponential smoothing ★★★★ 这两篇文章，由浅入深，从原理上指出了，Holt-Winters为什么可以解决残差、趋势、周期的问题。 Holt-Winters seasonal method ★★★ 这篇文章介绍了原理，美团那篇文章中的公司参考的这里。同时，这里给出了仿真方式。 Holt-Winters原理和初始值的确定 ★★★ 该文章介绍了参数的计算过程。同时给出来很多参考文献。 facebook prophet的探索（python语言） 介绍了时序型的分析框架。提供了Python和R两种语言支持。 可以研究下是否可以拿来使用。 吸收性该框架的思想，用到我们的设计中来。]]></content>
      <categories>
        <category>数据驱动</category>
      </categories>
      <tags>
        <tag>数据驱动</tag>
        <tag>AIOPs</tag>
        <tag>智能运维</tag>
        <tag>算法</tag>
        <tag>人工智能</tag>
        <tag>异常检测</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow快速入门示例]]></title>
    <url>%2F2018%2F07%2F08%2FTensorFlow%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[设置程序 安装最新版本的 TensorFlow 1!pip install -q --upgrade tensorflow 配置导入和 Eager Execution 123456789101112from __future__ import absolute_import, division, print_functionimport osimport matplotlib.pyplot as pltimport tensorflow as tfimport tensorflow.contrib.eager as tfetf.enable_eager_execution()print("TensorFlow version: &#123;&#125;".format(tf.VERSION))print("Eager execution: &#123;&#125;".format(tf.executing_eagerly())) TensorFlow version: 1.9.0-rc2 Eager execution: True 导入和解析训练数据集 下载数据集 123456train_dataset_url = "http://download.tensorflow.org/data/iris_training.csv"train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url), origin=train_dataset_url)print("Local copy of the dataset file: &#123;&#125;".format(train_dataset_fp)) Downloading data from http://download.tensorflow.org/data/iris_training.csv 8192/2194 [================================================================================================================] - 0s 0us/step Local copy of the dataset file: /content/.keras/datasets/iris_training.csv 检查数据: 使用 head -n5 命令查看前 5 个条目： 1!head -n5 &#123;train_dataset_fp&#125; 120,4,setosa,versicolor,virginica 6.4,2.8,5.6,2.2,2 5.0,2.3,3.3,1.0,1 4.9,2.5,4.5,1.7,2 4.9,3.1,1.5,0.1,0 解析数据集 12345678def parse_csv(line): example_defaults = [[0.], [0.], [0.], [0.], [0]] # sets field types parsed_line = tf.decode_csv(line, example_defaults) # First 4 fields are features, combine into single tensor features = tf.reshape(parsed_line[:-1], shape=(4,)) # Last field is the label label = tf.reshape(parsed_line[-1], shape=()) return features, label 创建训练 tf.data.Dataset| 12345678910train_dataset = tf.data.TextLineDataset(train_dataset_fp)train_dataset = train_dataset.skip(1) # skip the first header rowtrain_dataset = train_dataset.map(parse_csv) # parse each rowtrain_dataset = train_dataset.shuffle(buffer_size=1000) # randomizetrain_dataset = train_dataset.batch(32)# View a single example entry from a batchfeatures, label = iter(train_dataset).next()print("example features:", features[0])print("example label:", label[0]) example features: tf.Tensor([5.8 2.7 4.1 1. ], shape=(4,), dtype=float32) example label: tf.Tensor(1, shape=(), dtype=int32) 选择模型类型: 使用 Keras 创建模型 123456model = tf.keras.Sequential([ tf.keras.layers.Dense(10, activation="relu", input_shape=(4,)), # input shape required tf.keras.layers.Dense(10, activation="relu"), tf.keras.layers.Dense(3)])print( model) &lt;tensorflow.python.keras.engine.sequential.Sequential object at 0x7fa0842648d0&gt; 训练模型 定义损失和梯度函数 12345678def loss(model, x, y): y_ = model(x) return tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)def grad(model, inputs, targets): with tf.GradientTape() as tape: loss_value = loss(model, inputs, targets) return tape.gradient(loss_value, model.variables) 创建优化器 TensorFlow 拥有许多可用于训练的优化算法。此模型使用的是 tf.train.GradientDescentOptimizer，它可以实现随机梯度下降法 (SGD)。 1optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01) 训练循环 迭代每个周期。通过一次数据集即为一个周期。 在一个周期中，遍历训练 Dataset 中的每个样本，并获取样本的特征 (x) 和标签 (y)。 根据样本的特征进行预测，并比较预测结果和标签。衡量预测结果的不准确性，并使用所得的值计算模型的损失和梯度。 使用 optimizer 更新模型的变量。 跟踪一些统计信息以进行可视化。 对每个周期重复执行以上步骤。 num_epochs 变量是遍历数据集集合的次数。与直觉恰恰相反的是，训练模型的时间越长，并不能保证模型就越好。 num_epochs 是一个可以调整的超参数。选择正确的次数通常需要一定的经验和实验基础。 1234567891011121314151617181920212223242526272829303132## Note: Rerunning this cell uses the same model variables# keep results for plottingtrain_loss_results = []train_accuracy_results = []num_epochs = 201for epoch in range(num_epochs): epoch_loss_avg = tfe.metrics.Mean() epoch_accuracy = tfe.metrics.Accuracy() # Training loop - using batches of 32 for x, y in train_dataset: # Optimize the model grads = grad(model, x, y) optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step()) # Track progress epoch_loss_avg(loss(model, x, y)) # add current batch loss # compare predicted label to actual label epoch_accuracy(tf.argmax(model(x), axis=1, output_type=tf.int32), y) # end epoch train_loss_results.append(epoch_loss_avg.result()) train_accuracy_results.append(epoch_accuracy.result()) if epoch % 50 == 0: print("Epoch &#123;:03d&#125;: Loss: &#123;:.3f&#125;, Accuracy: &#123;:.3%&#125;".format(epoch, epoch_loss_avg.result(), epoch_accuracy.result())) Epoch 000: Loss: 1.433, Accuracy: 34.167% Epoch 050: Loss: 0.636, Accuracy: 70.000% Epoch 100: Loss: 0.417, Accuracy: 71.667% Epoch 150: Loss: 0.331, Accuracy: 87.500% Epoch 200: Loss: 0.239, Accuracy: 96.667% 可视化损失函数随时间推移而变化的情况 1234567891011fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))fig.suptitle('Training Metrics')axes[0].set_ylabel("Loss", fontsize=14)axes[0].plot(train_loss_results)axes[1].set_ylabel("Accuracy", fontsize=14)axes[1].set_xlabel("Epoch", fontsize=14)axes[1].plot(train_accuracy_results)plt.show() 评估模型的效果 设置测试数据集 12345678910test_url = "http://download.tensorflow.org/data/iris_test.csv"test_fp = tf.keras.utils.get_file(fname=os.path.basename(test_url), origin=test_url)test_dataset = tf.data.TextLineDataset(test_fp)test_dataset = test_dataset.skip(1) # skip header rowtest_dataset = test_dataset.map(parse_csv) # parse each row with the funcition created earliertest_dataset = test_dataset.shuffle(1000) # randomizetest_dataset = test_dataset.batch(32) # use the same batch size as the training set Downloading data from http://download.tensorflow.org/data/iris_test.csv 8192/573 [============================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 0us/step 根据测试数据集评估模型 1234567test_accuracy = tfe.metrics.Accuracy()for (x, y) in test_dataset: prediction = tf.argmax(model(x), axis=1, output_type=tf.int32) test_accuracy(prediction, y)print("Test set accuracy: &#123;:.3%&#125;".format(test_accuracy.result())) Test set accuracy: 96.667% 使用经过训练的模型进行预测 1234567891011121314class_ids = ["Iris setosa", "Iris versicolor", "Iris virginica"]predict_dataset = tf.convert_to_tensor([ [5.1, 3.3, 1.7, 0.5,], [5.9, 3.0, 4.2, 1.5,], [6.9, 3.1, 5.4, 2.1]])predictions = model(predict_dataset)for i, logits in enumerate(predictions): class_idx = tf.argmax(logits).numpy() name = class_ids[class_idx] print("Example &#123;&#125; prediction: &#123;&#125;".format(i, name)) Example 0 prediction: Iris setosa Example 1 prediction: Iris versicolor Example 2 prediction: Iris virginica colab地址 https://colab.research.google.com/drive/1iGHulgf_ioKl_GP_7_v8yTwwyfzP6KTR]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[以太坊白皮书笔记]]></title>
    <url>%2F2018%2F04%2F11%2F%E4%BB%A5%E5%A4%AA%E5%9D%8A%E7%99%BD%E7%9A%AE%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[以太坊白皮书笔记 比特币的两个创新： 一种去中心化的点对点的网上货币 基于工作量证明的区块链概念使得人们可以就交易顺序达成共识。 “智能合约”： 根据事先任意制订的规则来自动转移数字资产的系统。 以太坊的目标： 就是提供一个带有内置的成熟的图灵完备语言的区块链，用这种语言可以创建合约来编码任意状态转换功能，用户只要简单地用几行代码来实现逻辑，就能够创建以上提及的所有系统以及许多我们还想象不到的的其它系统。从而，将区块链应用于货币以外的领域。 历史 中本聪的创新是引入这样一个理念：将一个非常简单的基于节点的去中心化共识协议与工作量证明机制结合在一起。节点通过工作量证明机制获得参与到系统的权利，每十分钟将交易打包到“区块”中，从而创建出不断增长的区块链。 作为状态转换系统的比特币 从技术角度讲，比特币账本可以被认为是一个状态转换系统，该系统包括所有现存的比特币所有权状态和“状态转换函数”。状态转换函数以当前状态和交易为输入，输出新的状态。 UTXO:所有已经被挖出的、没有花费的比特币（技术上称为“未花费的交易输出，unspent transaction outputs)。每个UTXO都有一个面值和所有者。 一笔交易：包括一个或多个输入和一个或多个输出。 每个输入包含一个对现有UTXO的引用和由与所有者地址相对应的私钥创建的密码学签名。 每个输出包含一个新的加入到状态中的UTXO。 在比特币系统中，状态转换函数APPLY(S,TX)-&gt;S’大体上可以如下定义： 交易的每个输入： 如果引用的UTXO不存在于现在的状态中（S），返回错误提示 如果签名与UTXO所有者的签名不一致，返回错误提示 如果所有的UTXO输入面值总额小于所有的UTXO输出面值总额，返回错误提示 返回新状态S’,新状态S中移除了所有的输入UTXO，增加了所有的输出UTXO。 挖矿 每个区块包含一个时间戳、一个随机数、一个对上一个区块的引用（即哈希）和上一区块生成以来发生的所有交易列表。 区块验证算法,即“工作量证明”： 防止攻击：利用交易顺序攻击时，在发生区块链分叉时，区块链长的分支被认为是诚实的区块链。 默克尔树 左：仅提供默克尔树（Merkle tree）上的少量节点已经足够给出分支的合法证明。 右：任何对于默克尔树的任何部分进行改变的尝试都会最终导致链上某处的不一致。 默克尔树是一种二叉树，由一组叶节点、一组中间节点和一个根节点构成。最下面的大量的叶节点包含基础数据，每个中间节点是它的两个子节点的哈希，根节点也是由它的两个子节点的哈希，代表了默克尔树的顶部。 默克尔树的目的是允许区块的数据可以零散地传送：节点可以从一个源下载区块头，从另外的源下载与其有关的树的其它部分，而依然能够确认所有的数据都是正确的。 节点： 轻节点”，它下载区块头，使用区块头确认工作量证明，然后只下载与其交易相关的默克尔树“分支”。 全量节点：存储和处理所有区块的全部数据的节点。 其他区块链应用 去中心化共识技术的应用将会服从幂律分布，大多数的应用太小不足以保证自由区块链的安全，我们还注意到大量的去中心化应用，尤其是去中心化自治组织，需要进行应用之间的交互。 脚本 本质上，比特币系统允许不同的密码学货币进行去中心化的兑换。然而，比特币系统的脚本语言存在一些严重的限制： 缺少图灵完备性：不支持循环语句。 价值盲（Value-blindness）。UTXO脚本不能为账户的取款额度提供精细的的控制。 缺少状态 – UTXO只能是已花费或者未花费状态，这就没有给需要任何其它内部状态的多阶段合约或者脚本留出生存空间。 区块链盲（Blockchain-blindness）- UTXO看不到区块链的数据，例如随机数和上一个区块的哈希。这一缺陷剥夺了脚本语言所拥有的基于随机性的潜在价值，严重地限制了博彩等其它领域应用。 以太坊 以太坊的目的是基于脚本、竞争币和链上元协议（on-chain meta-protocol）概念进行整合和提高，使得开发者能够创建任意的基于共识的、可扩展的、标准化的、特性完备的、易于开发的和协同的应用。 以太坊通过建立终极的抽象的基础层-内置有图灵完备编程语言的区块链-使得任何人都能够创建合约和去中心化应用并在其中设立他们自由定义的所有权规则、交易方式和状态转换函数。 以太坊账户 以太坊的账户包含四个部分： 随机数，用于确定每笔交易只能被处理一次的计数器 账户目前的以太币余额 账户的合约代码，如果有的话 账户的存储（默认为空） 以太币（Ether）： 是以太坊内部的主要加密燃料，用于支付交易费用。 两类账户：外部所有的账户（由私钥控制的）和合约账户（由合约代码控制）。 外部所有的账户没有代码，人们可以通过创建和签名一笔交易从一个外部账户发送消息。 每当合约账户收到一条消息，合约内部的代码就会被激活，允许它对内部存储进行读取和写入，和发送其它消息或者创建合约。 消息和交易 以太坊的消息在某种程度上类似于比特币的交易，但是两者之间存在三点重要的不同。 第一，以太坊的消息可以由外部实体或者合约创建，然而比特币的交易只能从外部创建。 第二，以太坊消息可以选择包含数据。 第三，如果以太坊消息的接受者是合约账户，可以选择进行回应，这意味着以太坊消息也包含函数概念。 以太坊中“交易”是指存储从外部账户发出的消息的签名数据包。交易包含消息的接收者、用于确认发送者的签名、以太币账户余额、要发送的数据和两个被称为STARTGAS和GASPRICE的数值。 STARTGAS就是限制，GASPRICE是每一计算步骤需要支付矿工的费用。 创建合约有单独的交易类型和相应的消息类型；合约的地址是基于账号随机数和交易数据的哈希计算出来的。 以太坊状态转换函数 以太坊的状态转换函数：APPLY(S,TX) -&gt; S'，可以定义如下： 检查交易的格式是否正确（即有正确数值）、签名是否有效和随机数是否与发送者账户的随机数匹配。如否，返回错误。 计算交易费用:fee=STARTGAS * GASPRICE，并从签名中确定发送者的地址。从发送者的账户中减去交易费用和增加发送者的随机数。如果账户余额不足，返回错误。 设定初值GAS = STARTGAS，并根据交易中的字节数减去一定量的瓦斯值。 从发送者的账户转移价值到接收者账户。如果接收账户还不存在，创建此账户。如果接收账户是一个合约，运行合约的代码，直到代码运行结束或者瓦斯用完。 如果因为发送者账户没有足够的钱或者代码执行耗尽瓦斯导致价值转移失败，恢复原来的状态，但是还需要支付交易费用，交易费用加至矿工账户。 否则，将所有剩余的瓦斯归还给发送者，消耗掉的瓦斯作为交易费用发送给矿工。 代码执行 EVM代码（以太坊虚拟机代码）：使用低级的基于堆栈的字节码的语言写成。代码由一系列字节构成，每一个字节代表一种操作。操作可以访问三种存储数据的空间： 堆栈，一种后进先出的数据存储，32字节的数值可以入栈，出栈。 内存，可无限扩展的字节队列。 合约的长期存储，一个秘钥/数值的存储，其中秘钥和数值都是32字节大小，与计算结束即重置的堆栈和内存不同，存储内容将长期保持。 区块链和挖矿 以太坊区块不仅包含交易记录和最近的状态，还包含区块序号和难度值。以太坊中的区块确认算法如下： 检查区块引用的上一个区块是否存在和有效。 检查区块的时间戳是否比引用的上一个区块大，而且小于15分钟。 检查区块序号、难度值、 交易根，叔根和瓦斯限额（许多以太坊特有的底层概念）是否有效。 检查区块的工作量证明是否有效。 将S[0]赋值为上一个区块的STATE_ROOT。 将TX赋值为区块的交易列表，一共有n笔交易。对于属于0……n-1的i，进行状态转换S[i+1] = APPLY(S[i],TX[i])。如果任何一个转换发生错误，或者程序执行到此处所花费的瓦斯（gas）超过了GASLIMIT，返回错误。 用S[n]给S_FINAL赋值, 向矿工支付区块奖励。 检查S-FINAL是否与STATE_ROOT相同。如果相同，区块是有效的。否则，区块是无效的。 应用 三种应用: 第一类是金融应用，为用户提供更强大的用他们的钱管理和参与合约的方法。包括子货币，金融衍生品，对冲合约，储蓄钱包，遗嘱，甚至一些种类的全面的雇佣合约。 第二类是半金融应用，这里有钱的存在但也有很重的非金钱的方面，一个完美的例子是为解决计算问题而设的自我强制悬赏。 还有在线投票和去中心化治理这样的完全的非金融应用。 令牌系统 链上令牌系统有很多应用，如美元或黄金等资产的子货币、公司股票、不可伪造的优惠券、积分奖励， 所有的货币或者令牌系统，从根本上来说是一个带有如下操作的数据库：从A中减去X单位并把X单位加到B上，前提条件是(1)A在交易之前有至少X单位以及(2)交易被A批准。实施一个令牌系统就是把这样一个逻辑实施到一个合约中去。 金融衍生品 金融衍生品是“智能合约”的最普遍的应用，也是最易于用代码实现的之一。 实现金融合约的主要挑战是它们中的大部分需要参照一个外部的价格发布器。最简单地方法是通过由某特定机构（例如纳斯达克）维护的“数据提供“合约进行，该合约的设计使得该机构能够根据需要更新合约，并提供一个接口使得其它合约能够通过发送一个消息给该合约以获取包含价格信息的回复。 ## 身份和信誉系统 去中心化文件存储 以太坊合约允许去中心化存储生态的开发，这样用户通过将他们自己的硬盘或未用的网络空间租出去以获得少量收益，从而降低了文件存储的成本。当一个用户想重新下载他的文件，他可以使用微支付通道协议（例如每32k字节支付1萨博）恢复文件。 去中心化自治组织 去中心化自治组织（DAO, decentralized autonomous organization）”的概念指的是一个拥有一定数量成员或股东的虚拟实体，依靠比如67%多数来决定花钱以及修改代码。 中心化组织（DO） 去中心化自治公司（DAC，decentralized autonomous corporation） 进一步的应用 储蓄钱包。类似银行。 作物保险 一个去中心化的数据发布器。 云计算。 点对点赌博。 预测市场。 杂项和关注 改进版幽灵协议的实施 如果矿工A挖出了一个区块然后矿工B碰巧在A的区块扩散至B之前挖出了另外一个区块，矿工B的区块就会作废并且没有对网络安全作出贡献。此外，这里还有中心化问题：如果A是一个拥有全网30%算力的矿池而B拥有10%的算力，A将面临70%的时间都在产生作废区块的风险而B在90%的时间里都在产生作废区块。 费用 比特币使用的默认方法是纯自愿的交易费用，依靠矿工担当守门人并设定动态的最低费用。因为这种方法是“基于市场的”，使得矿工和交易发送者能够按供需来决定价格，所以这种方法在比特币社区被很顺利地接受了。然而，这个逻辑的问题在于，交易处理并非一个市场；虽然根据直觉把交易处理解释成矿工给发送者提供的服务是很有吸引力的，但事实上一个矿工收录的交易是需要网络中每个节点处理的，所以交易处理中最大部分的成本是由第三方而不是决定是否收录交易的矿工承担的。于是，非常有可能发生公地悲剧。 计算和图灵完备 以太坊虚拟机是图灵完备的。防止恶意循环：每一个交易设定运行执行的最大计算步数来解决问题，如果超过则计算被恢复原状但依然要支付费用。 货币和发行 发行模式如下： 通过发售活动，以太币将以每BTC 1337-2000以太的价格发售，一个旨在为以太坊组织筹资并且为开发者支付报酬的机制已经在其它一些密码学货币平台上成功使用。早期购买者会享受较大的折扣，发售所得的BTC将完全用来支付开发者和研究者的工资和悬赏，以及投入密码学货币生态系统的项目。 0.099x （x为发售总量）将被分配给BTC融资或其它的确定性融资成功之前参与开发的早期贡献者，另外一个0.099x将分配给长期研究项目。 自上线时起每年都将有0.26x（x为发售总量）被矿工挖出。 除了线性的发行方式外，和比特币一样以太币的的供应量增长率长期来看也趋于零。 挖矿的中心化 以太坊使用一个基于为每1000个随机数随机产生唯一哈希的函数的挖矿算法，用足够宽的计算域，去除专用硬件的优势。 扩展性 如果比特币网络处理Visa级的2000tps的交易，它将以每三秒1MB的速度增长（1GB每小时，8TB每年）。太坊全节点只需存储状态而不是完整的区块链，从而得到改善。 因为基于区块链的挖矿算法，至少每个矿工会被迫成为一个全节点，这保证了一定数量的全节点。 综述：去中心化应用 从“传统”网页的角度看来，这些网页是完全静态的内容，因为区块链和其它去中心化协议将完全代替服务器来处理用户发起的请求。 结论 以太坊协议最初是作为一个通过高度通用的语言，提供链上契约、提现限制、金融合约、赌博市场等高级功能，作为升级版密码学货币。 作为图灵完备编程语言，理论上任意的合约创建出来。 以太坊协议比单纯的货币走得更远，可以创建金融和非金融应用。 提供了一个具有独特潜力的平台 通俗理解 是区块链的结构？区块链就是很多块数据的存储，形成链式结构。你可以这样理解，一个区块就是一个Word文件，每分钟产生一个Word文档；如果创世区块链式001.doc，第二分钟就是002.doc,后面依次产生下去003.doc 004.doc ... xxN.doc， 这就像链条一样，所以才叫区块链。总之一句话，每分钟产生的Word文档用链条连接起来，就是区块链。 第二点就是，所有的文档存储在哪里？先说下，传统的金融行业，农业银行的系统，那就农行自己的机器上面。如果农行想修改的数据，可以随便修改。 这种情况，农行就是中心。区块链的存储是网上所有的挖矿机器上面， 任何人都没有权限修改你的数据，也就是Word文档的内容，任何人都不能修改。这也是为什么，区块链是去中心化的，没有中心，没有人能篡改。 区块里面放了什么？ 每个Word文档里面放的就是这一分钟的交易信息。比如，我给你转账1个比特币，就放在当前分钟的Word文档里面。 谁来把这些信息放到Word文档里面 矿工。 每个矿工（也就是计算机）都在抢打包交易信息去权限，抢到了就是挖到了矿，就会得到比特币的奖励。 参考文档 以太坊白皮书]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>以太坊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式]]></title>
    <url>%2F2018%2F04%2F09%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[代码重构 基础 重构的定义:对软件内部结构的一种调整，目的是在不改变”软件之可察行为“前提下，提高其可理解性，降低其修改成本。 重构的意义 优秀设计的根本是：消除重复部分！（DRY = Don’t repeat yourself） 重构让代码更清晰，更容易理解 清晰的代码可以更方便的找到bug，重构可以写出更强健的代码 良好的设计可以在长远时间上提高开发速度 什么时候重构 随时进行重构（在我看来，重构更是一种开发的习惯） 事不过三，代码重复不要超过三次（否则就要”抽“出来） 添加功能时候并一一重构（个人理解是，添加新功能之前，分析并重构，从而更方便添加新功能） 修补错误时 code review时 重构与性能 重构确实会在短期内降低代码执行效率，但优化阶段是可以调整的，而且调整会更容易。 提前优化是万恶之源 从测试开始 无测试，无重构，只依赖手工测试，重构时候人会崩溃的。 重构的保证就是自动化测试 避免事项 重复的代码（这才是真正万恶之源，鄙视一切Ctrl+C/P） 过长函数，会导致责任不明确/难以切割/难以理解等一系列问题 过大类，职责不明确，垃圾滋生地 过长参数列（面向对象不是说说而已） 发散式变化，一个类会响应多种需求而被修改 散弹式修改（其实就是没有封装变化处，由于一个需求，多处需要被修改） 依赖情节（一个类对其他类过多的依赖） 数据泥团（如果数据有意义，就将结构数据变成对象） type code，使用Class替代 switch，少用，考虑多态 过多平行的类，使用类继承并联起来 冗余类，去除它 夸夸其谈的未来性（Matin的文字，侯俊杰的翻译真是…出彩…） 临时值域，封装它 过度耦合的消息链，使用真正需要的函数和对象，而不要依赖于消息链 过度的deleate 过度使用其他类private值域 重复作用的类 不完美的类库，（类库老了，使用者也没办法阿） 纯数据类（类需要行为） 不纯粹的继承（拒绝父类的接口的类） 过多注释，注释多了，就说明代码不清楚了 重构原则 函数重构规则 Extract Method(提取函数)-------将大函数按模块拆分成几个小的函数 Inline Method ---- 内联函数：将微不足道的小函数进行整合 Introduce Explaining Variable---引入解释性变量：将复杂的表达式拆分成多个变量 Remove Assignments to Parameters----移除对参数的赋值 Replace Method with Method Object----以函数对象取代函数：一个函数如果参数过多，可抽为类+函数+参数。 类重构规则 Move Method----方法迁移：当类中的方法不适合放在当前类中时，就需要迁移。 Move Field----搬移字段：当在一个类中的某一个字段，被另一个类的对象频繁使用时，我们就应该考虑将这个字段的位置进行更改了。 Extract Class----提炼类：一个类如果过于复杂，做了好多的事情，违背了“单一职责”的原则，所以需要将其可以独立的模块进行拆分，当然有可能由一个类拆分出多个类。 Introduce Foreign Method----引入外加函数：在不想或者不能修改原类的情况下，为该类添加新的方法。使用包装设计模式。 数据重构规则 对数据重构是很有必要的，因为我们的程序主要是对数据进行处理。如果你的业务逻辑非常复杂，那么对数据进行合理的处理是很有必要的。对数据的组织形式以及操作进行重构，提高了代码的可维护性以及可扩展性。 Self Encapsulate Field (自封装字段)：虽然字段对外是也隐藏的，但是还是有必要为其添加getter方法，在类的内部使用getter方法来代替self.field，该方式称为自封装字段，自己封装的字段，自己使用。 Replace data Value with Object(以对象取代数据值) Change Value to Reference (将值对象改变成引用对象) Replace Array or Dictionary with Object(以对象取代数组或字典)：数据组合起来代表一定的意义，这是最好将其定义成一个实体类。 Duplicate Observed Data(复制“被监测数据”)：即不要将业务代码和非业务代码糅合在一起。比如，不要再Integration层写业务代码。 Change Unidirectional Association to Bidirectional(将单向关联改为双向关联)：Customer与Order的关系是单向关联的，也就是说Order引用了Customer, 而Customer没有引用Order。 Encapsulate Field（封装字段)：public--&gt;private,再通过getter/setter操作。 Encapsulate Collection（封装集合)：当你的类中有集合时，为了对该集合进行封装，你需要为集合创建相应的操作方法，例如增删改查等等。 Replace Subclass with Fields（以字段取代子类)：当你的各个子类中唯一的差别只在“返回常量数据”的函数上。当遇到这种情况时，你就可以将这个返回的数据放到父类中，并在父类中创建相应的工厂方法，然后将子类删除即可。 条件表达式重构规则(if-else) 有时候在实现比较复杂的业务逻辑时，各种条件各种嵌套。如果处理不好的话，代码看上去会非常的糟糕，而且业务逻辑看上去会非常混乱。通过一些重构规则来对条件表达式进行重构，可以让业务逻辑更为清晰，代码更以维护和扩展。 Decompose Conditional(分解条件表达式)：经if后的复杂条件表达式进行提取，将其封装成函数。 Consolidate Conditional Expression(合并条件表达式)：一些条件表达式后的语句体执行的代码块相同。 Consolidate Duplicate Conditional Fragments(合并重复的条件片段)：if与else中的相同语句。 Remove Control Flag(移除控制标记)：标记变量不易维护，不易理解。标记变量一般是可以使用其他语句进行替换的，可以使用break、return、continue等等，这个要根据具体情况而定。复杂的可以用状态机进行设计。 Replace Nested Condition with Guard Clauses(以卫语句取代嵌套的条件)：尽量不要将if-else进行嵌套，因为嵌套的if-else确实不好理解。要去除上面的嵌套模式，我们可以将if后的条件进行翻转，根据具体需求再引入return、break、continue等卫语句。 Replace Condition with Polymorphism(以多态取代条件表达式)：多态就是类的不同类型的对象有着不同的行为状态。当然，对多态再进一步包装，就是工厂设计模式。 继承关系重构规则 Pull Up Field (字段上移) &amp; Pull Down Field (字段下移) 字段上移:两个子类中的相同字段，向上移动。 Extract Subclass (提炼子类)：每个类的职责更为单一，即“单一职责”。 Collapse Hierarchy (折叠继承关系)：与“提炼子类”规则相对应。就是当你的父类与子类差别不大时，我们就可以将子类与父类进行合并。 Form Template Method (构造模板函数)：“模板”其实就是框架，没有具体的实现细节，只有固定不变的步骤，可以说模板不关心具体的细节。 以委托取代继承（Replace Inheritance with Delegation）：子类只使用了父类的部分方法，而且没有继承或者部分继承了父类的数据。在这种情况下我们就可以将这种继承关系修改成委托的关系。具体做法就是修改这种继承关系，在原有子类中添加父类的对象字段，在子类中创建相应的方法，在方法中使用委托对象来调用原始父类中相应的方法。 参考 代码重构 《重构：改善既有代码的设计》]]></content>
      <categories>
        <category>研发技能</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis-generator使用方式及实例]]></title>
    <url>%2F2018%2F02%2F11%2Fmybatis-generator%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F%E5%8F%8A%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[mybatis-generator使用方式及实例 MyBatis Generator (MBG) 是一个Mybatis的代码生成器，它可以帮助我们根据数据库中表的设计生成对应的实体类，xml Mapper文件，接口以及帮助类，也就是我们可以借助该类来进行简单的CRUD操作。这样就避免了我们每使用到一张表的数据就需要手动去创建对应的类和xml文件，这就帮我们节约了大量的时间去开发和业务逻辑有关的功能。下面我主要介绍基于Maven和普通的Java工程两种方式来生成相应的文件。 注：如果对联合查询和存储过程您仍然需要手写SQL和对象。 MySQL环境准备 安装：brew install mysql 启动：mysql.server start 运行：mysql_secure_installation 新建库manager--表tasks 12345678CREATE TABLE IF NOT EXISTS tasks ( task_id INT(11) NOT NULL AUTO_INCREMENT, subject VARCHAR(45) DEFAULT NULL, start_date DATE DEFAULT NULL, end_date DATE DEFAULT NULL, description VARCHAR(200) DEFAULT NULL, PRIMARY KEY (task_id)) ENGINE=InnoDB; 配置 mybatis-generator有三种用法：命令行、eclipse插件、maven插件。本文使用maven插件方式，原因：该方式可以直接集成到项目中，从而方便后续其他人迭代新增表后，自动生成。 参考文档中有详细说明，如果不想细看，可以直接clone代码 https://github.com/DanielJyc/mybatis-generator 。然后： 更改generatorConfig.xml中，jar路径：/Users/daniel/gitworkspace/JavaEETest/Test27-mybatis2/lib/mysql-connector-java-5.1.40.jar为自己的真实路径。 更改表配置信息： 12&lt;table tableName="tasks" enableCountByExample="false" enableUpdateByExample="false" enableDeleteByExample="false" enableSelectByExample="false" selectByExampleQueryId="false"&gt; 更改数据库连接信息 1234db.driver=com.mysql.jdbc.Driverdb.url= jdbc:mysql://localhost:3306/managerdb.username=rootdb.password=**** 根目录generator下，直接执行mvn mybatis-generator:generate即可。 集成到项目中 可以集成到项目中，两种方式： 作为单独的mvn模块。 直接集成到dal层。 新建子mvn模块后，把代码拷贝过去即可。 参考文档 sql教程 Mac下记录使用Homebrew安装Mysql全过程 Mybatis Generator最完整配置详解 利用mybatis-generator自动生成代码 idea + mybatis generator + maven 插件使用]]></content>
      <categories>
        <category>研发技能</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell脚本使用和总结]]></title>
    <url>%2F2018%2F02%2F04%2Fshell%E8%84%9A%E6%9C%AC%E4%BD%BF%E7%94%A8%E5%92%8C%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[shell脚本使用和总结 hello world 新建test1.sh 内容echo Hello world! 如果需要的话，赋予可执行权限chmod +x test1.sh 执行指令 可以在脚本中写入更多的命令，比如：启动app，编译、运行java文件，链接服务器等等；只要你能想到的，基本都可以。在此，我仅以一个简单的例子来说明：通过脚本编译运行一个java文件。 123cd /Users/daniel/Documents/tmpjavac Test2.javajava Test2 匹配规则${}，##, %% , ：- ，：+， ？ 的使用（路径处理） 假设我们定义了一个变量为：file=/dir1/dir2/dir3/my.file.txt 1.可以用${ }分别替换得到不同的值： 12345678$&#123;file#*/&#125;：删掉第一个/ 及其左边的字符串：dir1/dir2/dir3/my.file.txt$&#123;file##*/&#125;：删掉最后一个/ 及其左边的字符串：my.file.txt$&#123;file#*.&#125;：删掉第一个. 及其左边的字符串：file.txt$&#123;file##*.&#125;：删掉最后一个. 及其左边的字符串：txt$&#123;file%/*&#125;：删掉最后一个 / 及其右边的字符串：/dir1/dir2/dir3$&#123;file%%/*&#125;：删掉第一个/ 及其右边的字符串：(空值)$&#123;file%.*&#125;：删掉最后一个 . 及其右边的字符串：/dir1/dir2/dir3/my.file$&#123;file%%.*&#125;：删掉第一个 . 及其右边的字符串：/dir1/dir2/dir3/my 2.记忆的方法为： 12345678# 是 去掉左边（键盘上#在 $ 的左边）%是去掉右边（键盘上% 在$ 的右边）单一符号是最小匹配；两个符号是最大匹配$&#123;file:0:5&#125;：提取最左边的5 个字节：/dir1$&#123;file:5:5&#125;：提取第5 个字节右边的连续5个字节：/dir2也可以对变量值里的字符串作替换：$&#123;file/dir/path&#125;：将第一个dir 替换为path：/path1/dir2/dir3/my.file.txt$&#123;file//dir/path&#125;：将全部dir 替换为path：/path1/path2/path3/my.file.txt 3.利用${ } 还可针对不同的变数状态赋值(沒设定、空值、非空值)： 12345678910111213$&#123;file-my.file.txt&#125; ：假如$file 沒有设定，則使用my.file.txt 作传回值。(空值及非空值時不作处理) $&#123;file:-my.file.txt&#125; ：假如$file 沒有設定或為空值，則使用my.file.txt 作傳回值。(非空值時不作处理)$&#123;file+my.file.txt&#125; ：假如$file 設為空值或非空值，均使用my.file.txt 作傳回值。(沒設定時不作处理)$&#123;file:+my.file.txt&#125; ：若$file 為非空值，則使用my.file.txt 作傳回值。(沒設定及空值時不作处理)$&#123;file=my.file.txt&#125; ：若$file 沒設定，則使用my.file.txt 作傳回值，同時將$file 賦值為my.file.txt 。(空值及非空值時不作处理)$&#123;file:=my.file.txt&#125; ：若$file 沒設定或為空值，則使用my.file.txt 作傳回值，同時將$file 賦值為my.file.txt 。(非空值時不作处理)$&#123;file?my.file.txt&#125; ：若$file 沒設定，則將my.file.txt 輸出至STDERR。(空值及非空值時不作处理)$&#123;file:?my.file.txt&#125; ：若$file 没设定或为空值，则将my.file.txt 输出至STDERR。(非空值時不作处理)$&#123;#var&#125; 可计算出变量值的长度：$&#123;#file&#125; 可得到27 ，因为/dir1/dir2/dir3/my.file.txt 是27个字节 分割字符串 比如，要分割test=&quot;aaa,bbb,cc cc,dd dd&quot;，可以这样: 1234567891011# 1.分割OLD_IFS="$IFS" IFS="-" arr=($filename) IFS="$OLD_IFS" # 2.获取for x in $arr; do echo $xdoneecho $&#123;arr[0]&#125; 实例 取出照片，按照日期创建文件夹，并将文件放到指定文件夹中。 123456789101112131415for filePath in /Users/***/DCIM/*.jpg; do # 1.获取文件名，eg: P70827-165946.jpg filename=$&#123;filePath##*/&#125; echo $&#123;filename&#125; # 2.获取日期标识部分P70827：从头开始取6位 dirName="/Users/***/DCIM/"$&#123;filename:0:6&#125; echo $dirName"\n" # 3.根据日期创建文件夹 mkdir -p $&#123;dirName&#125; # 4.将文件移入该文件夹 mv $&#123;filePath&#125; $&#123;dirName&#125;done 注意事项 等号=两边不能有空格:空格对于linux的shell是一种很典型的分隔符，所以给变量赋值的时候中间不能够有空格。 参考文档 求助linux下批量建立文件夹和移动文件 Shell 教程 shell编程--遍历目录下的文件 shell中的${}，##和%%的使用 linux shell 字符串操作详解 （长度，读取，替换，截取，连接，对比，删除，位置 ）]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[表达式引擎性能比较]]></title>
    <url>%2F2018%2F02%2F03%2F%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%BC%95%E6%93%8E%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[表达式引擎性能比较 表达式引擎选型 选择3种主要的表达式引擎进行性能对比，从而选择最优表达式引擎和最优方案。常用的3中规则引擎：QLExpress、MEVL、JUEL。 性能测试 性能测试维度： 表达式引擎维度：主要采用了3种引擎的5种实现方式，即QLExpress 使用缓存; mvel 不编译; mvel 先编译; mvel 先编译，且指定输入值类型; juel 指定输入值类型。注：QLExpress 不使用缓存的话，性能非常低，就不再进行对比测试。 表达式维度：主要采用了3种类型的表达式，后续的大多数需求，都属于这3种方式。即： 最常见的场景（多个条件进行and操作）： a&lt;100 &amp;&amp; b&gt;=100 &amp;&amp; c&lt;=123, 包含特殊的操作（contains）：a&lt;100 &amp;&amp; b&gt;=100 &amp;&amp; c&lt;=123 &amp;&amp; stringList.contains(str) 一个稍微复杂一点的语法树：a&gt;1 &amp;&amp; ((b&gt;1 || c&lt;1) || (a&gt;1 &amp;&amp; b&lt;1 &amp;&amp; c&gt;1)) 测试方式 10次循环，每次循环执行这5种方式10万次，即每种方式执行100万次。为了保证一定的差异性，变量赋值的时候，采用变化的值。代码详见附录。 执行结果： 对结果进行可视化，很显然Mvel不编译方式耗时最高。 为了方便查看，将Mvel不编译方式过滤掉，得到下图。得到以下结论： 性能高低顺序为：mvel 先编译，且指定输入值类型 &gt; mvel 先编译 &gt; juel 指定输入值类型 &gt; QLExpress 使用缓存 &gt; mvel 不编译。 juel 指定输入值类型性能也可以接受，但是在使用了contains操作后，性能明显降低很多。 结论 mvel 先编译，且指定输入值类型方式性能为最优，采用该方式作为表达式引擎的实现方式。但是，采用自己实现一层缓存：用来缓存预编译的结果，同时，采用2小时无访问，则更新缓存的策略。缓存使用Guava Cache实现。 注： 1. QLExpress自身实现了缓存机制，如果性能要求没那么高的话，也可以采用该方式。 2. 如果规则非常多，比如到了亿级别的规模，并且基本不复用规则的话，就没有必要先编译并缓存了。或者调整缓存策略。 附录 机器配置 1234MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports)处理器 2.3 GHz Intel Core i5内存 8 GB 2133 MHz LPDDR3图形卡 Intel Iris Plus Graphics 640 1536 MB 表达式引擎版本： 123456789101112131415161718192021222324252627&lt;!-- https://mvnrepository.com/artifact/org.mvel/mvel2 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mvel&lt;/groupId&gt; &lt;artifactId&gt;mvel2&lt;/artifactId&gt; &lt;version&gt;2.4.0.Final&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/ognl/ognl --&gt; &lt;dependency&gt; &lt;groupId&gt;ognl&lt;/groupId&gt; &lt;artifactId&gt;ognl&lt;/artifactId&gt; &lt;version&gt;3.2.4&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/de.odysseus.juel/juel-api --&gt; &lt;dependency&gt; &lt;groupId&gt;de.odysseus.juel&lt;/groupId&gt; &lt;artifactId&gt;juel-api&lt;/artifactId&gt; &lt;version&gt;2.2.7&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/de.odysseus.juel/juel-impl --&gt; &lt;dependency&gt; &lt;groupId&gt;de.odysseus.juel&lt;/groupId&gt; &lt;artifactId&gt;juel-impl&lt;/artifactId&gt; &lt;version&gt;2.2.7&lt;/version&gt; &lt;/dependency&gt; Java代码 详见： https://github.com/DanielJyc/expression-language-compare/blob/master/java/src/Test.java 参考文档 mvel github QLExpress MVEL 2.0 官方文档 Transwiki:MVEL Language Guide Mvel2.0使用指南一 基础 mvel2.0语法指南 Ognl/MVEL/Aviator/JSEL 四种表达式引擎执行效率对比 在Excel图表中制作三维立体图表的方法 juel Quickstart THE ART OF BEING EXPRESSIVE]]></content>
      <categories>
        <category>研发技能</category>
      </categories>
      <tags>
        <tag>表达式引擎</tag>
        <tag>QLExpress</tag>
        <tag>MEVL</tag>
        <tag>JUEL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Storm+kafka]]></title>
    <url>%2F2018%2F01%2F19%2FStorm%2Bkafka%2F</url>
    <content type="text"><![CDATA[Storm+kafka Storm+kafka flume实时收集日志，kafka消息队列源源不断生产数据，然后由storm进行实时消费。 参考 Storm+kafka的HelloWorld初体验]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Superset]]></title>
    <url>%2F2018%2F01%2F17%2FApache%20Superset%2F</url>
    <content type="text"><![CDATA[Apache Superset Superset其实是一个自助式数据分析工具，它的主要目标是简化我们的数据探索分析操作。 相似产品对比 name 描述 实现 grafana grafana的druid插件，比较简陋，github一年不更新了 js Metabase 支持数据库种类多，启动方便，支持json查询。图形化查询，只能有一个聚合字段，两个维度 Clojure imply-pivot 基于Plywood，部署方便，能构造复杂的查询。目前已经闭源了，没法二次开发 Js airbnb/superset 权限管理完善，图形可定制性也比较高，github持续更新,集合了metabase的Dashboard和pivot的查询可定制性优点，部署相对麻烦 python+js 数据库支持 Superset 是基于 Druid.io 设计的，但是又支持横向到像 SQLAlchemy 这样的常见Python ORM框架上面。 Druid 是一个基于分布式的快速列式存储，也是一个为BI设计的开源数据存储查询工具。Druid提供了一种实时数据低延迟的插入、灵活的数据探索和快速数据聚合。现有的Druid已经可以支持扩展到TB级别的事件和PB级的数据了，Druid是BI应用的最佳搭档。 跟类似产品Hive相比，速度快了很多。 架构 整个项目的后端是基于Python的，用到了Flask、Pandas、SqlAlchemy。 后端： Flask AppBuilder(鉴权、CRUD、规则） Pandas（分析） SqlAlchemy（数据库ORM） 前端： 用到了npm、react、webpack,这意味着你可以在手机也可以流畅使用。 d3 (数据可视化) nvd3.org(可重用图表) 局限性 Superset的可视化，目前只支持每次可视化一张表，对于多表join的情况还无能为力 依赖于数据库的快速响应，如果数据库本身太慢Superset也没什么办法 语义层的封装还需要完善，因为druid原生只支持部分sql。 参考 解密Airbnb 自助BI神器：Superset 颠覆 Tableau superset官网 druid.io可视化调研 Grafana vs Superset]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Ambari]]></title>
    <url>%2F2018%2F01%2F17%2FApache%20Ambari%2F</url>
    <content type="text"><![CDATA[Apache Ambari 用来创建、管理、监视 Hadoop 的集群，但是这里的 Hadoop 是广义，指的是 Hadoop 整个生态圈（例如 Hive，Hbase，Sqoop，Zookeeper 等），而并不仅是特指 Hadoop。用一句话来说，Ambari 就是为了让 Hadoop 以及相关的大数据软件更容易使用的一个工具。 Ambari 自身也是一个分布式架构的软件，主要由两部分组成：Ambari Server 和 Ambari Agent。简单来说，用户通过 Ambari Server 通知 Ambari Agent 安装对应的软件；Agent 会定时地发送各个机器每个软件模块的状态给 Ambari Server，最终这些状态信息会呈现在 Ambari 的 GUI，方便用户了解到集群的各种状态，并进行相应的维护。 参考 Ambari——大数据平台的搭建利器]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据 - Ambari</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache flume]]></title>
    <url>%2F2018%2F01%2F17%2FApache%20flume%2F</url>
    <content type="text"><![CDATA[Apache flume flume是分布式的日志收集系统，它将各个服务器中的数据收集起来并送到指定的地方去，比如说送到图中的HDFS，简单来说flume就是收集日志的。 ## 同类产品对比 Flume使用基于事务的数据传递方式来保证事件传递的可靠性。而logstash内部是没有persist queue，所以在异常情况下，是可能出现数据丢失的问题的。 具体可参考下面的文档。 ## event event将传输的数据进行封装，是flume传输数据的基本单位，如果是文本文件，通常是一行记录，event也是事务的基本单位。flume的核心是把数据从数据源(source)收集过来，在将收集到的数据送到指定的目的地(sink)。为了保证输送的过程一定成功，在送到目的地(sink)之前，会先缓存数据(channel),待数据真正到达目的地(sink)后，flume在删除自己缓存的数据。 flume架构介绍 agent：本身是一个java进程，运行在日志收集节点—所谓日志收集节点就是服务器节点。 agent里面包含3个核心的组件：source—-&gt;channel—–&gt;sink,类似生产者、仓库、消费者的架构。 source：source组件是专门用来收集数据的，可以处理各种类型、各种格式的日志数据。 channel：source组件把数据收集来以后，临时存放在channel中。用来存放临时数据的——对采集到的数据进行简单的缓存，可以存放在memory、jdbc、file等等。 sink：sink组件是用于把数据发送到目的地的组件，目的地包括hdfs、logger、avro、thrift、ipc、file、null、hbase、solr、自定义。 于flume可以支持多级flume的agent，即flume可以前后相继，例如sink可以将数据写到下一个agent的source中，这样的话就可以连成串了，可以整体处理了。flume还支持扇入(fan-in)、扇出(fan-out)。所谓扇入就是source可以接受多个输入，所谓扇出就是sink可以将数据输出多个目的地destination中。 参考 Flume架构以及应用介绍 Flume日志采集系统——初体验（Logstash对比版） Logstash，flume，sqoop比较]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Siddhi]]></title>
    <url>%2F2018%2F01%2F17%2FSiddhi%2F</url>
    <content type="text"><![CDATA[Siddhi Siddhi是一个复杂事件流程引擎CEP(Complex Event Processing)。使用类SQL的语言描述事件流任务，可以很好的支撑开发一个可扩展的，可配置的流式任务执行引擎。性能管理系统之中，告警模块采用storm作为告警生成组件。传统设计之中，为了支持不同的告警规则类型，我们需要编写不同的业务逻辑代码，但是使用了Siddhi之后，我们只需要配置不同的流任务Siddhiql，即可以支持不同的告警业务。 Siddhi 能做什么？ 简单 ETL:使用类SQL 基于 window 聚合：基于时间窗口 多个流 Join Pattern Query：Pattern allows event streams to be correlated over time and detect event patterns based on the order of event arrival.比如：在一天内，出现一次取现金额 &lt; 100之后，同一张卡，再次出现取现金额 &gt; 10000，则认为是诈骗。 Sequence Query:和 pattern 的区别是，pattern 的多个 event 之间可以是不连续的，但 sequence 的 events 之间必须是连续的。我们可以看个例子，用sequence 来发现股票价格的 peak： 12345from every e1=FilteredStockStream[price&gt;20], e2=FilteredStockStream[((e2[last].price is null) and price&gt;=e1.price) or ((not (e2[last].price is null)) and price&gt;=e2[last].price)], e3=FilteredStockStream[price&lt;e2[last].price] select e1.price as priceInitial, e2[last].price as pricePeak, e3.price as priceAfterPeak insert into PeakStream ; 上面的查询的意思， e1，收到一条 event.price&gt;20。 e2，后续收到的所有 events 的 price，都大于前一条 event。 e3，最终收到一条 event 的 price，小于前一条 event。 ok，我们发现了一个peak。 集成到 JStorm 我将 Siddhi core 封装成一个 Siddhi Bolt，这样可以在 JStorm 的 topology 中很灵活的，选择是否什么方案，可以部分统计用 brain，部分用 Siddhi，非常简单。 参考： Siddhi初探(内含一个实例) CEP简介 让Storm插上CEP的翅膀 - Siddhi调研和集成]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka]]></title>
    <url>%2F2018%2F01%2F17%2Fkafka%2F</url>
    <content type="text"><![CDATA[kafka 是一个分布式消息系统，主要用作数据管道和消息系统。 kafka的架构 如下图，由生产者向kafka集群生产消息，消费者从kafka集群订阅消息。 其中，kafka集群中的消息是按照主题（或者说Topic）来进行组成的。 主题（Topic）：一个主题类似新闻中的体育、娱乐、教育等分类概念，在实际工程中通常一个业务一个主题。 分区（Partition）：一个Topic中的消息数据按照多个分区组织，分区是kafka消息队列组织的最小单位，一个分区可以看做是一个FIFO（先进先出）队列；kafka分区是提高kafka性能的关键手段。 这张图在整体上对kafka集群进行了概要，途中kafka集群是由三台机器（Broker）组成，当然，实际情况可能更多。相应的有3个分区，Partition-0~Partition-2，图中能看到每个分区的数据备份了2份。kafka集群从前端应用程序（producer）生产消息，后端通过各种异构的消费者来订阅消息。kafka集群和各种异构的生产者、消费者都使用zookeeper集群来进行分布式协调管理和分布式状态管理、分布式锁服务的。 参考 Kafka入门学习（一） Kafka topic常见命令解析]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Eagle]]></title>
    <url>%2F2018%2F01%2F17%2FApache-Eagle%2F</url>
    <content type="text"><![CDATA[Apache Eagle Apache Eagle是一个识别大数据平台上的安全和性能问题的开源解决方案。它主要用来即时监测敏感数据访问和恶意活动，并及时采取行动。除了数据活动管理，Eagle也可以用于节点异常检测,集群和作业性能分析。 功能介绍 主要的应用场景包括：监控Hadoop中的数据访问流量;检测非法入侵和违反安全规则的行为;检测并防止敏感数据丢失和访问;实现基于策略的实时检测和预警;实现基于用户行为模式的异常数据行为检测。 检测和报警：Apache Eagle依赖于Apache Storm来进行数据活动和操作日志的流处理，并且可以执行基于策略的检测和报警。它提供多个API：作为基于Storm API上的一层抽象的流式处理API和policy engine provider API的抽象，它将WSO2的开源Siddhi CEP engine作为第一类对象。Siddhi CEP engine支持报警规则的热部署，并且警报可以使用属性过滤和基于窗口的规则（例如，在10分钟内三次以上的访问）来定义。 集群和作业性能分析：通过处理YARN应用日志和对YARN中所有运行的作业进行快照分析来完成的。Eagle可以检测单个作业趋势、数据偏斜问题、故障原因和考虑所有运行的作业情况下评估集群的整体性能。 基于机器学习的policy provider：Apache Eagle中还包括一个基于机器学习的policy provider。它从过去的用户行为中学习，来将数据访问分类为异常或者正常。这个机器学习policy provider评估在Apache Spark框架中离线训练的模型。 Eagle具有如下特点： 高实时： 尽可能地确保能在亚秒级别时间内产生告警，一旦综合多种因素确订为危险操作，立即采取措施阻止非法行为。 可伸缩 简单易用 用户Profile：Eagle 内置提供基于机器学习算法对Hadoop中用户行为习惯建立用户Profile的功能。我们提供多种默认的机器学习算法供你选择用于针对不同HDFS特征集进行建模，通过历史行为模型，Eagle可以实时地检测异常用户行为并产生预警。 基本概念 Site：A site can be considered as a physical data center. Big data platform e.g. Hadoop may be deployed to multiple data centers in an enterprise. Application：An &quot;Application&quot; or &quot;App&quot; is composed of data integration, policies and insights for one data source. Policy：A &quot;Policy&quot; defines the rule to alert. Policy can be simply a filter expression or a complex window based aggregation rules etc. Alerts：An &quot;Alert&quot; is an real-time event detected with certain alert policy or correlation logic, with different severity levels like INFO/WARNING/DANGER. Data Source：A &quot;Data Source&quot; is a monitoring target data. Eagle supports many data sources HDFS audit logs, Hive2 query, MapReduce job etc. Stream：A &quot;Stream&quot; is the streaming data from a data source. Each data source has its own stream. 架构 从这个架构图，可以看到eagle会提供一些内置的应用，通过配置不同的policy，它们能够做很多eagle里面非常核心的事情。 比如一个大的集群里，我们可能希望能够探测一些恶意的操作以及误操作，类似于Hive、MR、Spark的Job，如果一个不相关的人，操作了其他应用独享的数据，这种情况应该及时的通知应用的负责人。security问题在多租户下面其实非常重要，像我们现在一个hadoop集群，不仅仅会提供一些离线的任务，其实这部分任务反而占比很少，大部分都是在线的Yarn任务以及HDFS服务，不同的应用会使用hadoop进行任务分发、状态存储、checkpoint等，但这些任务在文件系统上的隔离是不保证的。在生产环境里，如果某个在线任务的状态文件被其他应用误修改了，整个snapshot都会不可用，一般这样的情况会使当前批次整个回滚，甚至在极端情况下出现错误数据，影响应用的执行语义。 在alertEngine中，你可以添加一个policy，检查JobHistory或者logStash，如果一个Job触发了policy就会通知相关的责任人采取措施。对大集群的监控和报警实际是非常麻烦和耗时的，eagle很好的解决了这个问题。 policy的设置非常灵活，甚至可以用机器学习的方式，离线去train一个模型，然后再把这个模型实时的更新上去。具体的做法是在storm这一层去做模型匹配，比如一个用户通常的几个特征量，read、delete、changename等，会去在线查看实际值与正常值的偏差，超过一定程度就会被eagle认为是异常的。或者一个未授权的用户，做了一件从来没有做过的事情，也可以认为是异常的。 通过内置的应用我们还可以进行作业的异常分析：一个Job有多个stage，上一级stage处理完之后，通过group的方式向下一级节点发送数据，假设hash的对象是userid，那么很可能出现的情况是，下一级一共有十个节点，但是partition之后的数据95%都被分配到了同一个节点上。这个问题通常是很难解决的，但有了eagle，我们就可以及时的发现这种情况，并优化提高Job的性能。 异常分析的另一个好处就是节点分析。稍微大一点规模的Job可能就需要几十上百个节点来执行，通常过程中如果挂掉一个节点，触发FailOver机制重新调度是可以解决的。但如果某个节点上的job经常失败，那么这个节点就应该被识别出来进行处理，避免大量job的重新调度开销。 内置应用 上面这些功能需要底层架构给予支持。eagle底层的流计算引擎是基于storm的，我们知道在传统领域，比如OLAP、数据库这些，通常都会有一门DSL来方便大家使用。近来在流计算领域，一个比较火爆的概念就是我们能不能也创造一门DSL，类似于SQL，但数据却是流式的。比如Spark的StructureStreaming，Flink的TableApi都是因为这个而产生。在eagle上为了支持更方便的执行policy和动态更新，也是需要一个DSL。那么它的做法比较讨巧，相当于利用了Siddhi这个现成的CEP，集成到storm的框架里，利用Siddhi本身的SQL支持来实现storm的DSL。那么用户无论是自定义policy还是自己编写应用，都可以像下面这样写： 定义流定义和查询，并将结果输出到另外一个流里面。 1234define stream TempStream (deviceID long, roomNo int, temp double);from TempStream select roomNo, temp * 9/5 + 32 as temp, F as scale, roomNo &gt;= 100 and roomNo &lt; 110 as isServerRoominsert into RoomTempStream; 多流Join和TimeWindow。 12345from TempStream[temp &gt; 30.0]#window.time(1 min) as T join RegulatorStream[isOn == false]#window.length(1) as Ron T.roomNo == R.roomNoselect T.roomNo, T.temp, R.deviceID, start as actioninsert into RegulatorActionStream ; Pattern Query：这个比较能体现CEP的优势，在下面的查询中，-&gt;标示的是事件顺序，也就是说，这个语义实际上表达了同一张卡在一天内，出现一次取现金额 &lt; 100后再次出现取现金额 &gt; 10000的情况，并将其判断为fraud。这是传统SQL所不具备的，也可以说是专为流式计算而设计。 12345from every a1 = atmStatsStream[amountWithdrawed &lt; 100]-&gt; b1 = atmStatsStream[amountWithdrawed &gt; 10000 and a1.cardNo == b1.cardNo]within 1 dayselect a1.cardNo as cardNo, a1.cardHolderName as cardHolderName, b1.amountWithdrawed as amountWithdrawed, b1.location as location, b1.cardHolderMobile as cardHolderMobileinsert into possibleFraudStream; 元数据设计 在集群中可能有成百上千个节点，每个节点上GB甚至上TB的的日志文件，如果出现一个异常的访问点，我们希望能在毫秒级别上对其进行预警或者是拦截。然后我们知道storm有一个很大的缺陷是它本身逻辑定义完就固定了。按照以往，这种分布式stream逻辑定义完，想再修改系统，必须要把topology重启，生产环境下肯定不希望这样，牺牲实时性的代价太大了，所以eagle整个的结构是元数据驱动（Metadata Driven）。 -w679 从上图我们可以看出eagle的输入和输出其实是非常明确的，那么在元数据的定义上，因为下面的存储是基于Hbase，所以eagle做的非常的灵活。一般对于同一个类型的采集日志(例如某个metric)，在RowKey上会采用一个固定的前缀，后面加上时间序列，这样在设计上就保证了分布性和同一个metric在数据上的连续性。 在生产场景下，可能一开始训练的Policy模型只有几个G的样本数据，但这个数据的增长是非常快的。那么我们不可能在一个月之后，还不去更换它。在eagle中这样的更新是很简单的。由于eagle的元数据驱动特性，engine会去监听元数据的变化。一旦metadata触发了alertEngine注册的listener，内部是可以通过ClassLoader动态部署的，比如动态的去更新storm里面的spout和bolt，这样整个更新过程可以在毫秒的级别就做完，相对来说，提高了几个数量级，并且这个过程是不会丢失数据的。 编写自定义应用 下面简单介绍一下如何编写一个自己的扩展应用： 首先需要提供应用的Provider 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class ExampleApplicationProvider extends AbstractApplicationProvider &#123;private static final Logger LOG = LoggerFactory.getLogger(ExampleApplicationProvider.class);@Overridepublic ExampleStormApplication getApplication() &#123; return new ExampleStormApplication();&#125;@Overridepublic Optional getApplicationListener() &#123; return Optional.of(new ApplicationListener() &#123; @Inject ExampleEntityService entityService; private ApplicationEntity application; @Override public void init(ApplicationEntity applicationEntity) &#123; this.application = applicationEntity; entityService.getEntities(); &#125; @Override public void afterInstall() &#123; LOG.info("afterInstall &#123;&#125;", this.application); &#125; @Override public void afterUninstall() &#123; LOG.info("afterUninstall &#123;&#125;", this.application); &#125; @Override public void beforeStart() &#123; LOG.info("beforeStart &#123;&#125;", this.application); &#125; @Override public void afterStop() &#123; LOG.info("afterStop &#123;&#125;", this.application); &#125; &#125;);&#125;@Overrideprotected void onRegister() &#123; bindToMemoryMetaStore(ExampleEntityService.class,ExampleEntityServiceMemoryImpl.class); bind(ExampleCommonService.class,ExampleCommonServiceImpl.class);&#125; 这里需要注意的是，应用本身的Meta是需要指定存储方式的，这个例子里面我们简单指定为Memory的方式。当然，在生产环境一般可以换成Hbase。 然后提供应用本身的逻辑 123456789101112public class ExampleStormApplication extends StormApplication &#123;@Overridepublic StormTopology execute(Config config, StormEnvironment environment) &#123; TopologyBuilder builder = new TopologyBuilder(); builder.setSpout("metric_spout", environment.getStreamSource("SAMPLE_STREAM", config) , config.getInt("spoutNum")); builder.setBolt("sink_1", environment.getStreamSink("SAMPLE_STREAM_1", config)).fieldsGrouping("metric_spout", new Fields("metric")); builder.setBolt("sink_2", environment.getStreamSink("SAMPLE_STREAM_2", config)).fieldsGrouping("metric_spout", new Fields("metric")); return builder.createTopology();&#125; 最后通过配置指定执行环境等参数 123456789101112131415161718192021222324"application": &#123; "sink": &#123; "type": "org.apache.eagle.app.messaging.KafkaStreamSink", "config": &#123; "kafkaBrokerHost": "", "kafkaZkConnection": "" &#125; &#125;, "storm": &#123; "nimbusHost": "localhost" "nimbusThriftPort": 6627 &#125;&#125;,"appId": "unit_test_example_app""spoutNum": 3"loaded": true"mode": "LOCAL""dataSinkConfig": &#123;"topic": "test_topic","brokerList": "sandbox.hortonworks.com:6667","serializerClass": "kafka.serializer.StringEncoder","keySerializerClass": "kafka.serializer.StringEncoder"&#125; 这里要配置好source和sink，比如kafka的topic、broker。限于篇幅，这里略去了coordinator自身的配置。完成上面的代码和配置，也就完成了一个自定义的应用编写。 数据集成 数据集成使用Apache Kafka通过logstash forwarder 代理或通过log4j kafka appender来实现的。来自多个Hadoop守护进程（例如，namenode，datanode等）的日志条目被反馈到Kafka并由Storm处理。Eagle支持将数据资产分类为多个灵敏度类型。 数据持久化 Eagle支持使用Apache HBase和关系数据库持久化警报。警报可通过电子邮件、Kafka或存储在Eagle支持的存储中进行通知。你也可以开发自己的警报通知插件。 结语 从前面的介绍我们可以看出，整个eagle其实是一套整体的解决方案，这个方案更多的是在应用的层面上进行了许多创新性的使用和整合。但eagle的实时性、可扩展性不仅仅值得在hadoop集群中使用，里面的很多思想其实也是值得给想要搭建流式计算平台的同学进行参考和学习的。而对于底层框架的开发同学，其实eagle在算子层、API层、状态存储层做的许多事情正是很多应用开发者需要自己去做的事情，能不能给开发应用更多的支持，让开发更顺畅更快速，也是值得去思考一下的。 参考 Apache Eagle毕业成为顶级项目 Apache Eagle 陈浩——Apache+Eagle：架构演化和新特性 Apache Eagle 简介--分布式实时 Hadoop 数据安全方案 Apache Eagle——eBay开源分布式实时Hadoop数据安全方案 Getting Started Eagle kafka eagle安装与使用 Kafka Eagle Reference Manual]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[storm学习]]></title>
    <url>%2F2018%2F01%2F17%2Fstorm%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[storm学习 原理和实例部分 流式框架对比 Hadoop只能处理适合进行批量计算的需求；Storm用来解决分布式流式计算系统。除此之外，流计算还有spark streaming和flink。对比： -w389 如果你想要的是一个允许增量计算的高速事件处理系统，Storm会是最佳选择。 如果你必须有状态的计算，恰好一次的递送，并且不介意高延迟的话，那么可以考虑Spark Streaming，特别如果你还计划图形操作、机器学习或者访问SQL的话，Apache Spark的stack允许你将一些library与数据流相结合(Spark SQL，Mllib，GraphX)，它们会提供便捷的一体化编程模型。尤其是数据流算法(例如：K均值流媒体)允许Spark实时决策的促进。 Flink支持增量迭代，具有对迭代自动优化的功能，在迭代式数据处理上，比Spark更突出，Flink基于每个事件一行一行地流式处理，真正的流式计算，流式计算跟Storm性能差不多，支持毫秒级计算，而Spark则只能支持秒级计算。 storm 主要概念 Storm采用的是Master-Slave结构，就是使用一个节点来管理整个集群的运行状态。Master节点被称为：Nimbus，Slave节点用来维护每台机器的状态，被称为Supervisor。 Nimbus的角色是只负责一些管理性的工作，它并不关心Worker之间的数据是如何传输的。 Supervisor的角色是听Nimbus的话，来启动并监控真正进行计算的Worker的进程。 Worker：运行在工作节点上面，被Supervisor守护进程创建的用来干活的JVM进程。一个Worker里面不会运行属于不同的topology的执行任务。 拓扑（topology）：在Storm中，先要设计一个用于实时计算的图状结构，我们称之为拓扑（topology）。这个拓扑将会被提交给集群，由集群中的主控节点（master node）分发代码，将任务分配给工作节点（worker node）执行。一个拓扑中包括spout和bolt两种角色。运行Topology：把代码以及所依赖的jar打进一个jar包，运行strom jar all-your-code.jar backtype.storm.MyTopology arg1 arg2。这个命令会运行主类:backtype.strom.MyTopology，参数是arg1, arg2。这个类的main函数定义这个topology并且把它提交给Nimbus。storm jar负责连接到nimbus并且上传jar文件。 数据源节点Spout：发送消息，负责将数据流以tuple元组的形式发送出去。 普通计算节点Bolt：负责转换这些数据流，在bolt中可以完成计算、过滤等操作，bolt自身也可以随机将数据发送给其他bolt。 记录Tuples：由spout发射出的tuple是不可变数组，对应着固定的键值对。 tuple：storm使用tuple来作为它的数据模型。每个tuple是一堆值，每个值有一个名字，并且每个值可以是任何类型。Tuple本来应该是一个Key-Value的Map，由于各个组件间传递的tuple的字段名称已经事先定义好了，所以Tuple只需要按序填入各个Value，所以就是一个Value List。一个Tuple代表数据流中的一个基本的处理单元，例如一条cookie日志，它可以包含多个Field，每个Field表示一个属性。 Stream：一个没有边界的、源源不断的、连续的Tuple序列就组成了Stream。 一个简单的Topology 看一下storm-starter里面的ExclamationTopology: 1234TopologyBuilder builder =new TopologyBuilder();builder.setSpout(1, new TestWordSpout(),10);builder.setBolt(2, new ExclamationBolt(),3).shuffleGrouping(1);builder.setBolt(3, new ExclamationBolt(),2).shuffleGrouping(2); setSpout和setBolt的三个参数：指定的id、包含处理逻辑的对象(spout或者bolt)、并行度（可选）。 spout要实现IRichSpout的接口；bolt要实现IRichBolt接口。 并行度表示集群里面需要多少个thread来一起执行这个节点；如果你忽略它，那么storm会分配一个线程来执行这个节点。 setBolt方法返回一个InputDeclarer对象，这个对象是用来定义Bolt的输入。 shuffleGrouping表示所有的tuple会被随机的分发给bolt的所有task。给task分发tuple的策略有很多种，后面会介绍： 这个Topology包含一个Spout和两个Bolt。这里第一个Bolt声明它要读取spout所发射的所有的tuple — 使用shuffle grouping。而第二个bolt声明它读取第一个bolt所发射的tuple。Spout发射单词，每个bolt在每个单词后面加个”!!!”。这三个节点被排成一条线: spout发射单词给第一个bolt， 第一个bolt然后把处理好的单词发射给第二个bolt。如果spout发射的单词是[&quot;bob&quot;]和[&quot;john&quot;], 那么第二个bolt会发射[&quot;bolt!!!!!!&quot;]和[&quot;john!!!!!!&quot;]出来。 如果想第二个bolt读取spout和第一个bolt所发射的所有的tuple， 那么应该这样定义第二个bolt:builder.setBolt(3,new ExclamationBolt(),5).shuffleGrouping(1).shuffleGrouping(2); TestWordSpout从[&quot;nathan&quot;, &quot;mike&quot;, &quot;jackson&quot;, &quot;golda&quot;, &quot;bertels&quot;]里面随机选择一个单词发射出来。TestWordSpout里面的nextTuple()方法是这样定义的： 1234567public void nextTuple() &#123; Utils.sleep(100); final String[] words=new String[]&#123;"nathan","mike","jackson","golda","bertels"&#125;; final Random rand =new Random(); final String word = words[rand.nextInt(words.length)]; _collector.emit(newValues(word));&#125; ExclamationBolt把”!!!”拼接到输入tuple后面。实现： 123456789101112131415161718public static class ExclamationBolt implements IRichBolt &#123; OutputCollector _collector; public void prepare(Map conf, TopologyContext context, OutputCollector collector)&#123; _collector = collector; &#125; public void execute(Tuple tuple) &#123; _collector.emit(tuple,new Values(tuple.getString(0) +"!!!")); _collector.ack(tuple); &#125; public void cleanup() &#123;&#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(newFields("word")); &#125;&#125; prepare方法提供给bolt一个Outputcollector用来发射tuple。Bolt可以在任何时候发射tuple: 在prepare, execute或者cleanup方法里面, 或者甚至在另一个线程里面异步发射。这里prepare方法只是简单地把OutputCollector作为一个类字段保存下来给后面execute方法使用。 execute方法从bolt的一个输入接收tuple(一个bolt可能有多个输入源). ExclamationBolt获取tuple的第一个字段，加上”!!!”之后再发射出去。如果一个bolt有多个输入源，可以通过调用Tuple#getSourceComponent方法来知道它是来自哪个输入源的。execute方法里面还有其它一些事情值得一提：输入tuple被作为emit方法的第一个参数，并且输入tuple在最后一行被ack。这些呢都是Storm可靠性API的一部分，后面会解释。 cleanup方法在bolt被关闭的时候调用， 它应该清理所有被打开的资源。但是集群不保证这个方法一定会被执行。比如执行task的机器down掉了，那么根本就没有办法来调用那个方法。cleanup设计的时候是被用来在local mode的时候才被调用(也就是说在一个进程里面模拟整个storm集群), 并且你想在关闭一些topology的时候避免资源泄漏。 declareOutputFields定义一个叫做”word”的字段的tuple。 storm的运行有两种模式: 本地模式和分布式模式。本地模式主要用于开发测试。 本地模式：运行storm-starter里面的topology的时候，它们就是以本地模式运行的，可以看到topology里面的每一个组件在发射什么消息。 分布式模式：storm由一堆机器组成。当提交topology给master的时候，同时也需要提交topology的代码。master负责分发代码，并且负责给topolgoy分配工作进程。如果一个工作进程挂掉了，master节点会重新分配到其它节点。 下面是以本地模式运行ExclamationTopology的代码: 12345678Config conf =new Config();conf.setDebug(true);conf.setNumWorkers(2);LocalCluster cluster =new LocalCluster();cluster.submitTopology("test", conf, builder.createTopology());Utils.sleep(10000);cluster.killTopology("test");cluster.shutdown(); 首先， 这个代码定义通过定义一个LocalCluster对象来定义一个进程内的集群。提交topology给这个虚拟的集群和提交topology给分布式集群是一样的。通过调用submitTopology方法来提交topology，三个参数：要运行的topology的名字、配置对象、要运行的topology本身。 topology的名字用来唯一区别一个topology，可以用这个名字来kill这个topology。必须显式的杀掉一个topology， 否则它会一直运行。 Conf对象可以配置很多东西， 下面两个是最常见的： TOPOLOGY_WORKERS(setNumWorkers) 定义集群分配多少个工作进程执行这个topology。topology里面的每个组件都需要线程来执行；每个组件通过setBolt和setSpout来指定需要的线程数；这些线程都运行在工作进程里面。每一个工作进程包含一些节点的一些工作线程。比如，如果你指定300个线程，60个进程， 那么每个工作进程里面要执行6个线程，而这6个线程可能属于不同的组件(Spout, Bolt)。你可以通过调整每个组件的并行度以及这些线程所在的进程数量来调整topology的性能。 TOPOLOGY_DEBUG(setDebug), 当它被设置成true的话， storm会记录下每个组件所发射的每条消息。这在本地环境调试topology很有用， 但是在线上这么做的话会影响性能的。 Topology的三个组件 运行中的Topology主要由以下三个组件组成的：Worker processes（进程）、Executors (threads)（线程）、Tasks。 -w400 举例： 12345678Config conf =new Config();conf.setNumWorkers(2);TopologyBuilder builder =new TopologyBuilder();builder.setSpout("blue-spout", new BlueSpout(),2);builder.setBolt("green-bolt", new GreenBolt(),2) .setNumTasks(4) //设置Task数量 .shuffleGrouping("blue-spout");builder.setBolt("yellow-bolt",new ExclamationBolt(),6).shuffleGrouping("green-bolt"); 对应的Worker processes（进程）、Executors (threads)（线程）、Tasks数量。指定了2个Worker。共2+2+6=10个Executor线程，每个Worker5个（图中未画出来）。绿色指定了Task数量为4，蓝色和黄色没有指定。 -w600 流分组策略(Stream grouping) 流分组策略告诉topology如何在两个组件之间发送tuple。spouts和bolts以很多task的形式在topology里面同步执行。如果从task的粒度来看一个运行的topology，它应该是这样的: -w502 当Bolt A的一个task要发送一个tuple给Bolt B， 它应该发送给Bolt B的哪个task呢？下面是一些常用的 “路由选择” 机制： ShuffleGrouping：随机选择一个Task来发送。 FiledGrouping：根据Tuple中Fields来做一致性hash，相同hash值的Tuple被发送到相同的Task。 AllGrouping：广播发送，将每一个Tuple发送到所有的Task。 GlobalGrouping：所有的Tuple会被发送到某个Bolt中的id最小的那个Task。 NoneGrouping：不关心Tuple发送给哪个Task来处理，等价于ShuffleGrouping。 DirectGrouping：直接将Tuple发送到指定的Task来处理。 使用其他的语言来定义Bolt Bolt可以使用任何语言来定义。用其它语言定义的bolt会被当作子进程(subprocess)来执行， storm使用JSON消息通过stdin/stdout来和这些subprocess通信。这个通信协议是一个只有100行的库，storm团队给这些库开发了对应的Ruby, Python和Fancy版本。 可靠的消息处理 Storm允许用户在Spout中发射一个新的源Tuple时为其指定一个MessageId，这个MessageId可以是任意的Object对象。多个源Tuple可以共用同一个MessageId，表示这多个源Tuple对用户来说是同一个消息单元。Storm的可靠性是指Storm会告知用户，每一个消息单元是否在一个指定的时间内被完全处理。完全处理的意思是该MessageId绑定的源Tuple以及由该源Tuple衍生的所有Tuple，都经过了Topology中每一个应该到达的Bolt的处理。 在Spout中由message 1绑定的tuple1和tuple2分别经过bolt1和bolt2的处理，然后生成了两个新的Tuple，并最终流向了bolt3。当bolt3处理完之后，称message 1被完全处理了。 Storm中的每一个Topology中都包含有一个Acker组件。Acker组件的任务就是跟踪从Spout中流出的每一个messageId所绑定的Tuple树中的所有Tuple的处理情况。如果在用户设置的最大超时时间内这些Tuple没有被完全处理，那么Acker会告诉Spout该消息处理失败，相反则会告知Spout该消息处理成功。 Storm接口详解 IComponent接口 Spout和Bolt都是其Component。所以，Storm定义了一个名叫IComponent的总接口。IComponent的继承关系如下图所示： 绿色部分是我们最常用、比较简单的部分。红色部分是与事务相关。BaseComponent是Storm提供的“偷懒”的类。为什么这么说呢，它及其子类，都或多或少实现了其接口定义的部分方法。这样我们在用的时候，可以直接继承该类，而不是自己每次都写所有的方法。但值得一提的是，BaseXXX这种定义的类，它所实现的方法，都是空的，直接返回null。 Spout -w535 各个接口说明： open方法：是初始化动作。允许你在该spout初始化时做一些动作，传入了上下文，方便取上下文的一些数据。 close方法：在该spout关闭前执行，但是并不能得到保证其一定被执行。spout是作为task运行在worker内，在cluster模式下，supervisor会直接kill -9 woker的进程，这样它就无法执行了。而在本地模式下，只要不是kill -9, 如果是发送停止命令，是可以保证close的执行的。 activate和deactivate方法 ：一个spout可以被暂时激活和关闭，这两个方法分别在对应的时刻被调用。 nextTuple方法：负责消息的接入，执行数据发射。是Spout中的最重要方法。 ack(Object)方法：传入的Object其实是一个id，唯一表示一个tuple。该方法是这个id所对应的tuple被成功处理后执行。 fail(Object)方法：同ack，只不过是tuple处理失败时执行。 如果继承了BaseRichSpout，就不用实现close、activate、deactivate、ack、fail和getComponentConfiguration方法，只关心最基本核心的部分。 总结：通常情况下（Shell和事务型的除外），实现一个Spout，可以直接实现接口IRichSpout，如果不想写多余的代码，可以直接继承BaseRichSpout。 Bolt 类图如下图所示： -w577 prepare方法：IBolt继承了java.io.Serializable，我们在nimbus上提交了topology以后，创建出来的bolt会序列化后发送到具体执行的worker上去。worker在执行该Bolt时，会先调用prepare方法传入当前执行的上下文。 execute方法：接受一个tuple进行处理，并用prepare方法传入的OutputCollector的ack方法（表示成功）或fail（表示失败）来反馈处理结果。 cleanup方法：同ISpout的close方法，在关闭前调用。同样不保证其一定执行。 Bolt实现时一定要注意execute方法。为什么IBasicBolt并没有继承IBolt？Storm提供了IBasicBolt接口，其目的就是实现该接口的Bolt不用在代码中提供反馈结果了，Storm内部会自动反馈成功。如果你确实要反馈失败，可以抛出FailedException。 总结：通常情况下，实现一个Bolt，可以实现IRichBolt接口或继承BaseRichBolt，如果不想自己处理结果反馈，可以实现IBasicBolt接口或继承BaseBasicBolt，它实际上相当于自动做掉了prepare方法和collector.emit.ack(inputTuple)； 部署 本地打jar：mvn clean install -DskipTests=true,jar包会打到$HOME/.m2/repository目录 为集群打包（包含其他依赖）：mvn package--&gt;target/storm-starter-{version}.jar local模式执行：storm jar target/storm-starter-*.jar org.apache.storm.starter.ExclamationTopology -local 集群模式执行，名称为production-topology:storm jar target/storm-starter-*.jar org.apache.storm.starter.RollingTopWords production-topology 参考 Storm入门学习随记 流式大数据处理的三种框架：Storm，Spark和Flink storm 入门原理介绍 细细品味Storm_Storm简介及安装V1.1.pdf storm的topology提交执行 Example Storm Topologies]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apache Ambari]]></title>
    <url>%2F2018%2F01%2F17%2FApache-Ambari%2F</url>
    <content type="text"><![CDATA[Apache Ambari 用来创建、管理、监视 Hadoop 的集群，但是这里的 Hadoop 是广义，指的是 Hadoop 整个生态圈（例如 Hive，Hbase，Sqoop，Zookeeper 等），而并不仅是特指 Hadoop。用一句话来说，Ambari 就是为了让 Hadoop 以及相关的大数据软件更容易使用的一个工具。 Ambari 自身也是一个分布式架构的软件，主要由两部分组成：Ambari Server 和 Ambari Agent。简单来说，用户通过 Ambari Server 通知 Ambari Agent 安装对应的软件；Agent 会定时地发送各个机器每个软件模块的状态给 Ambari Server，最终这些状态信息会呈现在 Ambari 的 GUI，方便用户了解到集群的各种状态，并进行相应的维护。 参考 Ambari——大数据平台的搭建利器]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F11%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new "My New Post" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习17--增强学习——拟合的值迭代法（fitted value iterator）]]></title>
    <url>%2F2014%2F06%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A017--%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%8B%9F%E5%90%88%E7%9A%84%E5%80%BC%E8%BF%AD%E4%BB%A3%E6%B3%95%EF%BC%88fitted-value-iterator%EF%BC%89%2F</url>
    <content type="text"><![CDATA[#机器学习17--增强学习——拟合的值迭代法（fitted value iterator） 本章不是很理解，只是把笔记摘录了一些。 首先讲到了离散化。 ## Learn a Model 1. 对于如下序列 我们定义Model或者Simulator为： \[S_{t+1}=AS_t+Ba_t\] 2. 然后，我们通过最小化下面的误差就可以得到参数： 拟合的值迭代法（fitted value iterator） 随机的Simulator 通过随机采样，求取平均值，来模拟当前状态s的值函数；然后，最小化误差函数，来估计参数。 1. 取样 取样{\(s^{1},...,s^{m}\)} 包含于\(S\) randomly 2. 初始化 初始化θ:=0 3. 重复迭代 &gt; 大致思路：公有m个状态。通过随机采样k个状态s，求平均值获得\(q(a)\)；通过不同的行为获得\(y^{(i)}\)，求得值函数的最大值；对每一个状态，通过最小化误差函数求取参数θ。 确定的Simulator 模型确定，即我们知道了\(S_{t+1}=AS_t+Ba_t\)，或者\(S_{t+1}=f(s ~ a)\)；从而可以确定下一个状态。我们只需要对每一个状态，通过最小化误差函数求取参数θ。对于上面的步骤，设\(k=1\)。 不确定的Simulator 采用如下方式： &gt; 其中，${}_{t} $为误差，服从高斯分布。 #NG老师的详细过程]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习16--增强学习——有限状态的马尔科夫决策过程MDP]]></title>
    <url>%2F2014%2F06%2F14%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A016--%E5%A2%9E%E5%BC%BA%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E7%9A%84%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8Bmdp%2F</url>
    <content type="text"><![CDATA[#机器学习16--增强学习——有限状态的马尔科夫决策过程MDP 16-20讲均为增强学习相关知识。暂时，只对16、17进行总结。 增强学习：找到一条回报值最大的路径（每步的回报之和最大），就认为是最佳的路径。eg：四足机器人、象棋 AI 程序、机器人控制，手机网络路由，市场决策，工业控制，高效网页索引等。 马尔科夫决策过程——MDP ###基本概念 1. 一个马尔科夫决策过程由一个五元组构成 2. 概念： - 值函数： 为了区分不同π的好坏，并定义在当前状态下，执行某个策略π后，出现的结果的好坏， 需要定义值函数： - 策略(pllicy)： ###公式推导（最优值函数和最优策略） 3. 对于如下问题，Robot开始位于(3,1)位置。目的是右上角。可能有11个状态。 &gt; - 行走的概率： &gt; - 回报函数 &gt; - 在某一点时的值函数。对于上述问题，有11个方程，11个未知量。 4. 进一步化简，我们得到 Bellman 等式 其中，表示下一个状态。 5. 定义最优值函数：\(V^{\star}\)。从而，找到一个当前状态 s 下，最优的行动策略π。 6. 最终，我们得到想要的最优值函数和最优策略： 7. 这里需要注意的是，如果我们能够求得每个 s 下最优的 a，那么从全局来看，的 映射即可生成，而生成的这个映射是最优映射，称为。针对全局的 s，确定了每一个 s的下一个行动 a，不会因为初始状态 s 选取的不同而不同。 有限状态的MDP具体策略的有效算法——值迭代和策略迭代法 前提：状态有限 ### 值迭代法 1. 过程 其中，迭代公式也可以写作： 2. 内循环的有两种策略： 3. 两种迭代法最终收敛到\(V^{\star}\)。我们再用如下公式，求出最优策略\(\pi^{\star}\) 策略迭代法 &gt; 注：在1-(a)中，我们认为得到的V为最优值函数。然后，在(b)中，进行更新得到最优策略。一直重复，知道得到真正的最优策略\(\pi^{\star}\)。 (a)步中的 V 可以通过之前的 Bellman 等式求得 (b)步实际上就是根据(a)步的结果挑选出当前状态 s 下，最优的 a，然后对π(s)做更新。 &gt; 这里的两个步骤，相当于求解11(状态个数)个线性方程。如果状态非常多，显然计算量相当大。 两种方法的总结 规模比较小的 MDP： 策略一般能够更快地收敛。 规模很大（状态很多）MDP：值迭代比较容易（不用求线性方程组）。 MDP 中的参数估计 实际中： &gt; - 未知量：状态转移概率\(P_{sa}\)𣠠和回报函数 R(s) - 已知量： S、 A 和γ 下面我们对状态转移概率\(P_{sa}\)和回报函数 R(s)进行估计： 1. 假设我们已知很多条状态转移路径如下：（相当于样本） &gt; 其中： 2. 如果我们获得了很多上面类似的转移链（相当于有了样本），那么我们就可以使用最大似然估计来估计状态转移概率。 &gt; 注：分子是从 s 状态执行动作 a 后到达 s’的次数，分母是在状态 s 时，执行 a 的次数。两者相除就是在 s 状态下执行 a 后，会转移到 s’的概率。 3. 同样，如果回报函数未知，那么我们认为 R(s)为在 s 状态下已经观测到的回报均值。 4. 我们将参数估计和值迭代结合起来（在不知道状态转移概率情况下）的流程如下： &gt; 在(b)步中我们要做值更新，也是一个循环迭代的过程，在上节中，我们通过将 V 初始化为 0，然后进行迭代来求解 V。嵌套到上面的过程后，如果每次初始化 V 为 0，然后迭代更新， 就会很慢。一个加快速度的方法是每次将 V 初始化为上一次大循环中得到的 V。 也就是说 V 的初值衔接了上次的结果。 NG老师的黑板图 最后把两张NG老师画的图放过来]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习15-3--独立成分分析ICA（Independent Component Analysis）]]></title>
    <url>%2F2014%2F06%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A015-3--%E7%8B%AC%E7%AB%8B%E6%88%90%E5%88%86%E5%88%86%E6%9E%90ica%EF%BC%88independent-component-analysis%EF%BC%89%2F</url>
    <content type="text"><![CDATA[#机器学习15-3--独立成分分析ICA（Independent Component Analysis） 每个人发出的信号\(s\)独立。且信号不能是高斯分布。 用于信号的分离（鸡尾酒宴会问题）。 ICA问题 鸡尾酒宴会问题 - n个人，n个麦克风。从n个麦克风得到一组数据：。其中：i 表示采样的时间顺序，也就是说共得到了 m 组采样，每一组采样都是 n 维的。 - 我们的目标是单单从这 m 组采样数据中分辨出每个人说话的信号s。有 n 个信号源 ，s相互独立。 - A 是一个未知的混合矩阵（mixing matrix），用来组合叠加信号 s。 1. 我们可以得到： &gt; 其中， x 不是一个向量，是一个矩阵 2. 其中每个列向量 3. A 和 s 都是未知的，x 是已知的，我们要想办法根据 x 来推出 s。这个过程也称作为盲信号分离。 4. 最终得到： &gt; - \(s_{(i)}^{j}\)：表示speaker j 在时刻i发出的信号。 &gt; - 对于此，我们需要知道两个量才能求出另外一个，下面我们进一步分析。 ICA算法的前处理步骤 中心化：也就是求 x 均值，然后让所有 x 减去均值，这一步与 PCA 一致。 漂白：目的是为了让x相互独立。将 x 乘以一个矩阵变成 (其协方差矩阵是\(I\))。 &gt; - 其中， &gt; - 其中使用特征值分解来得到 E（特征向量矩阵）和 D（特征值对角矩阵） ，计算公式为 ICA算法 我们假定每\(s_i\)有概率密度\(p_s\)，那么给定时刻原信号的联合分布就是 &gt; 注：每个人发出的声音信号s各自独立。 然后，我们就可以求得p(x) 现在，我们需要知道p(s)和w，才能求得p(x)。 首先，我们假设s 的累积分布函数符合 sigmoid 函数 这就是 s 的密度函数。这里 s 是实数。 然后，我们就剩下W了。我们用最大似然估计的方法求解。 使用前面得到的 x 的概率密度函数，得 &gt; 最终，我们求得： &gt;&gt; 其中α是梯度上升速率，人为指定。 迭代求出 W 后，我们也可以还原出原始信号： 应用 如果把麦克风x换成采集脑电波的电极，信号源s就代表大脑独立进程：心跳、眨眼等。通过将信号x减去心跳、眨眼等无用信号，我们就可以得到大脑内部信号。 ###小结 - ICA 的盲信号分析领域的一个强有力方法，也是求非高斯分布数据隐含因子的方法。 - ICA和PCA对比： &gt; - ICA: 从之前我们熟悉的样本-特征角度看，我们使用 ICA 的前提条件是，认为样本数据由独立非高斯分布的隐含因子产生，隐含因子个数等于特征数。更适合用来还原信号（因为信号比较有规律，经常不是高斯分布的）。 &gt; - PCA : 认为特征是由 k 个正交的特征（也可看作是隐含因子）生成的。更适合用来降维（用那么多特征干嘛，k 个正交的即可） &gt; - 有时候也需要组合两者一起使用。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习15-2--几种算法的对比总结(FA、PCA、混合高斯算法、k-means)]]></title>
    <url>%2F2014%2F06%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A015-2--%E5%87%A0%E7%A7%8D%E7%AE%97%E6%B3%95%E7%9A%84%E5%AF%B9%E6%AF%94%E6%80%BB%E7%BB%93(fa%E3%80%81pca%E3%80%81%E6%B7%B7%E5%90%88%E9%AB%98%E6%96%AF%E7%AE%97%E6%B3%95%E3%80%81k-means)%2F</url>
    <content type="text"><![CDATA[#机器学习15-2--几种算法的对比总结(FA、PCA、混合高斯算法、k-means) FA：无监督学习，z连续，服从高斯分布，作为隐含变量；运用EM方法，通过最大化似然函数，对进行参数估计。主要目的：降维。 PCA：通过变换矩阵进行降维。 混合高斯模型：无监督学习，z离散，服从多项式分布，作为隐含变量；运用EM方法，通过最大化似然函数，对进行参数估计。主要目的：对每个样本进行标签z的分配。 k-means：无监督学习，通过最小化，使每一类的点到其质心的间隔最小。主要目的：对每个样本进行标签z的分配。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习15-1--SVD]]></title>
    <url>%2F2014%2F06%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A015-1--svd%2F</url>
    <content type="text"><![CDATA[#机器学习15-1--SVD 本章在《机器学习实战》的14章有具体的应用——推荐系统。 通过svd，可以将矩阵A进行降维处理。如下图： 1. 调用函数直接计算三个参数。 2. 选取合适的k值。 示意图： &gt; 那么，我们的k值是如何获得呢？我们的做法是计算能量信息，保存90%以上的能量即可。能量的计算：所有奇异值求平方和，累加到90%未止。 注：老师上课的推导过程：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习14--PCA主成分分析]]></title>
    <url>%2F2014%2F06%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A014--pca%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[#机器学习14--PCA主成分分析 ##概述 - 高斯分布的样本 - 原始变量的线性组合表示新的综合变量，即主成分 - 解决特征过多，样本过少的问题 - 两个特征强相关的问题，或者两个特征有一个多余 - 滤除噪声 - 与《模型选择和规则化》对比 &gt; 《模型选择和规则化》要剔除的特征主要是和类标签无关的特征。比如“学生的名字”就和他的“成绩”无关，使用的是互信息的方法。 而这里的特征很多是和类标签有关的，但里面存在噪声或者冗余。在这种情况下，需要一种特征降维的方法来减少特征数，减少噪音和冗余，减少过度拟合的可能性。 - PCA的思想是将n维特征映射到k维上（k&lt;n），这k维是全新的正交特征。这k维特征称为主元，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n‐k维特征。 PCA过程 数据预处理：减去均值，再归一化(除以标准差)。 &gt; 求特征协方差矩阵，如果数据是3维，那么协方差矩阵是 求协方差的特征值和特征向量 将特征值按照从大到小的顺序排序，选择其中最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。 &gt; 这里，用简单的方式理解一下为什么选取前k个：在矩阵特征值与特征向量中，越大的特征值对应的特征向量，对被变换矩阵影响越大。 将样本点投影到选取的特征向量上。假设样例数为m，特征数为n，减去均值后的样本矩阵为\(DataAdjust(m*n)\)，协方差矩阵是 \(n*n\)，选取的k个特征向量组成的矩阵为\(EigenVectors(n*k)\)。那么投影后的数据FinalData为 PCA 理论基础 为什么协方差矩阵的特征向量就是k维理想特征，有三个理论：分别是最大方差理论、最小错误理论和坐标轴相关度理论。详见课件。 ### 最大方差理论 简单理解一下第一种方法： 在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。如前面的图，样本在横轴上的投影方差较大，在纵轴上的投影方差较小，那么认为纵轴上的投影是由噪声引起的。 因此我们认为，最好的k维特征是将n维样本点转换为k维后，每一维上的样本方差都很大。 1. 方差： 2. 求解最值问题： 3. 最终解： &gt; λ就是Σ的特征值，u是特征向量。最佳的投影直线是特征值λ最大时对应的特征向量，其次是λ第二大对应的特征向量，依次类推。 其中的第j 维就是 上的投影。 通过选取最大的k个u，使得方差较小的特征（如噪声）被丢弃。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习13--EM算法--应用到三个模型： 1,2,3因子分析模型]]></title>
    <url>%2F2014%2F06%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A013--em%E7%AE%97%E6%B3%95--%E5%BA%94%E7%94%A8%E5%88%B0%E4%B8%89%E4%B8%AA%E6%A8%A1%E5%9E%8B%EF%BC%9A-1%2C2%2C3%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[#机器学习13--EM算法--应用到三个模型： 1,2,3因子分析模型 使用场所： &gt; - 标签z是连续的 - 样本个数大于特征数 - 高维--&gt;低维 - 潜在的假想变量和随机影响变量的线性组合表示原始变量 ##边缘和条件概率分布 后面的推导用到了边缘和条件分布： 1. 已知 2. 边缘分布 3. 条件概率分布 ###因子分析模型 z被称为因子。我们要做的就是从高维随机连续变量x，变换到低维随机连续变量z。 1. 首先，我们如下假设： &gt; eg：从1维到2维： 1维： 2维 2. 我们令： 3. 下面经过推导，求得的具体值： 4. 因此，我们得到x的边缘分布： 5. 从而我们对样本 进行最大似然估计： &gt; 但是，当似然函数最大时，我们得不到解析解（比如，一元二次方程的通解形式）。根据之前对参数估计的理解，在有隐含变量 z 时，我们可以考虑使用 EM 来进行估计。 ###因子分析的EM估计 ####总体步骤： &gt; 注：这里的要改写为积分符号\(\int_{z^{(i)}}^{}\)。因为，z是连续随机变量。 ####详细步骤 E步 1. 根据前面的结论有： 2. 根据多元高斯公式得： M步 3. 我们目标是最大化： 4. 通过参数估计，我们最终可以得到： 然后将Φ上的对角线上元素抽取出来放到对应的Ψ中，就得到了Ψ。 ###因子分析总结 1. 根据上面的 EM 的过程，要对样本 X 进行因子分析，只需知道要分解的因子数（z 的维度）即可。通过 EM，我们能够得到转换矩阵Λ和误差协方差Ψ。 2. 因子分析(factor analysis)是一种数据简化的技术。原始的变量是可观测的显在变量，而假想变量是不可观测的潜在变量，称为因子。 3. 因子分析与回归分析不同，因子分析中的因子是一个比较抽象的概念，而回归因子有非常明确的实际意义 4. 主成分分析分析与因子分析也有不同，主成分分析仅仅是变量变换，而因子分析需要构造因子模型。 &gt; 主成分分析:原始变量的线性组合表示新的综合变量，即主成分； 因子分析：潜在的假想变量和随机影响变量的线性组合表示原始变量。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习12-2--EM算法--应用到三个模型： 高斯混合模型 ，混合朴素贝叶斯模型，因子分析模型（下一部分）]]></title>
    <url>%2F2014%2F06%2F09%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012-2--em%E7%AE%97%E6%B3%95--%E5%BA%94%E7%94%A8%E5%88%B0%E4%B8%89%E4%B8%AA%E6%A8%A1%E5%9E%8B%EF%BC%9A-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B-%EF%BC%8C%E6%B7%B7%E5%90%88%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%9B%A0%E5%AD%90%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%EF%BC%88%E4%B8%8B%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%89%2F</url>
    <content type="text"><![CDATA[#机器学习12-2--EM算法--应用到三个模型： 高斯混合模型 ，混合朴素贝叶斯模型，因子分析模型（下一部分） 判别模型求的是条件概率p(y|x)。常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件随机场、神经网络等。 生成模型求的是联合概率p(x,y)，即 = p(x|y) ∗ p(y) 。常见的生成模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、LDA、Restricted Boltzmann Machine等。 所以这里说的高斯混合模型，混合朴素贝叶斯模型都是求p(x,y)联合概率的。(下面推导会见原因) &gt; 总结：凡是生成模型，目的都是求出联合概率表达式，然后对联合概率表达式里的各个参数再进行估计，求出其表达式。 下面的EM算法，GMM等三个模型都是做这同一件事：设法求出联合概率，然后对出现的参数进行估计。 ## EM算法 ### Jensen不等式 Jensen不等式表述如下： 如果 f 是凸函数，X 是随机变量，那么 &gt; - 凸函数： - 如果 f 是严格凸函数，那 当且仅当。即：也就是说 X 是常量（后面会用到这个结论）。𞀊- Jensen 不等式应用于凹函数时，不等号方向反向，也就是。后面的log函数就是凹函数。 回顾：函数的期望的求取 EM算法推导步骤 EM算法的作用：进行参数估计。 应用：（因为是无监督，所以一般应用在聚类上，也用在HMM参数估计上）所以凡是有EM算法的，一定是无监督学习.因为EM是对参数聚集 我们的目的：找到每个样例隐含的类别 z，能使得 p(x,z)最大。（即 如果将样本x(i)看作观察值，隐含类别z看作是隐藏变量， 则x可能是类别z， 那么聚类问题也就是参数估计问题） &gt; p(x,z)最大似然估计是： 所以可见用EM算法的模型（高斯混合模型，朴素贝叶斯模型）都是求p(x,y)联合概率，为生成模型。 对上面公式，直接求θ一般比较困难，因为有隐藏变量z存在，但是一般确定了z后，求解就容易了。 p(x,z)最大似然估计是： EM是一种解决存在隐含变量优化问题的有效方法。既然不能直接最大化ℓ(θ)，我们可建立ℓ的下界（E步），再优化下界（M步），见下图第三步，取的就是下界 &gt; 其中， 如果 z 是连续性的，那么 是概率密度函数，需要将求和符号换做积分符号（因子分析模型是如此），即： 对Q(z)进行推导： &gt; - &gt; - z只受参数θ影响： 一般的EM算法步骤 &gt; - 注：在m步中，最终是对参数θ进行估计，而这一步具体到高斯混合模型，则θ有三个参数：\(\mu,\phi, \sigma\) 代替，即高斯混合模型要推导三个参数，下面会讲。 - 最终，我们得到的是每个样例属于哪个类(k个类)，以及参数θ(k组)。 至此，这就是EM算法所有推导，EM算法推导也只能推导这些步，具体再将这些公式推导下去，就要结合模型了。 EM算法的另外一种解释——坐标上升算法 如果我们定义 从前面的推导中我们知道ℓ(θ) ≥ J(Q, θ)，EM 可以看作是 J 的坐标上升法，E 步固定θ，优化Q；M 步固定Q优化θ。 EM算法的图形解释 &gt; 其中，红色线表示每一步的J(Q, θ)。E步:确定红线部分。M步：确定当前红线部分J(Q, θ)的极值点。最终得到局部最优解。 ##混合高斯模型 将EM算法融到高斯混合模型，将上面EM算法的E步、M步的公式再具体推导下去。 混合高斯模型定义：对于每个样例\(x^{(i)}\) ，我们先从k个类别中按多项式分布抽取一个\(z^{(i)}\)(隐含随机变量) ，然后根据所对应的 k 个多值高斯分布中的一个，生成样例\(x^{(i)}\)，整个过程称作混合高斯模型。 条件和符号说明 训练样本： \(x^{(i)}\) ：满足多值高斯分布，即： 。由此可以得到联合分布：。 隐含类别：有 k 个值{1,…,k} 可以选取。 混合高斯模型步骤 E步 1、由EM公式得 M步 2、我们需要在固定 后最大化最大似然估计，求解 &gt; 3、 求解三个参数 &gt; 1. 2. 在∅和μ确定后，分子上面的一串都是常数了，实际上需要优化的公式是： 显然这是一个，有约束条件的规划问题。我们采用前面的拉格朗日方法： 3. Σ的推导也类似，不过稍微复杂一些，毕竟是矩阵。 4、混合高斯模型与前面的高斯判别模型的对比 - 最大似然估计就近似于高斯判别分析模型 - GDA 中类别 y 是伯努利分布，而这里的 z是多项式分布 - 这里的每个样例都有不同的协方差矩阵，而 GDA 中认为只有一个。 5、总结： 最终，我们得到隐含类别的具体值。还有，参数∅μΣ。 ##混合朴素贝叶斯模型 混合高斯的例子：文本聚类: 要对一系列的文本聚类成若干主题。（用svm写文本分类是最好的）news.google.com就是文本聚类一个应用。垃圾邮件过滤（不知道那一封是垃圾邮件）。 ###模型描述 给定m个样本的训练集合是 ， 每个文本\(x^{(i)}\)属于\((0,1)^n\)。即每个文本是n维 0或1的向量。 故= { wordj 是否出现在文本i 里} 我们要对\(z^{i}\)(值是0或1) 进行建模，\(z^{i}\)是隐含随机变量，这里取两个值：2个聚类。所以对混合贝叶斯模型，假设\(z^{i}\)服从参数∅有伯努利分布。 ### 步骤 1、同高斯混合模型，混合贝叶斯模型的联合概率是： 2、由贝叶斯公式可知： 3、E步： &gt; 注：这里三个参数phi,mu,sigma,改成，，与\(∅_{j|z}\) 4、M步： 得到： &gt; 这里Wi表示文本来自于类1，分子Σ表示：类1且包含词j的文档个数，分布表示类1的文档总数。所以全式表示：类1包含词j的比率。 EM算法不能做出绝对的假设0或者1，所以只能用Wi表示，最终Wi的值会靠近0或1，在数值上与0或1无分别。 &gt; 全式表示：类0包含词j的比率 5、迭代上面12步骤，收敛，得到参数估计。然后，带回联合概率，将联合概率排序，由联合概率最高值 ，可得知哪个文本是输入哪个类了。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习12-1--K-means算法(无监督)]]></title>
    <url>%2F2014%2F06%2F09%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A012-1--k-means%E7%AE%97%E6%B3%95(%E6%97%A0%E7%9B%91%E7%9D%A3)%2F</url>
    <content type="text"><![CDATA[#机器学习12-1--K-means算法（无监督） 本章开始接触第一个无监督学习算法。无监督学习算法可用在新闻局累、图像分割等。 ## 无监督学习概念 - 定义：简单讲，就是没有分类标签y。非监督式学习是一种机器学习的方式，并不需要人力来输入标签。它是监督式学习和强化学习等策略之外的一种选择。 - K-means是最简单的聚类，聚类属于无监督学习。 K-means的步骤 符号： &gt; 类的数目：\(k\) 训练样本： 质心点： 类：\(｛c^{(1)}, c^{(2)}, ……, c^{(k)}｝\) 步骤： K-means 算法是将样本聚类成 k 个簇（cluster），具体算法描述如下： 二维效果图(k=2): Kmeans收敛的原因 &gt; 首先，我们定义畸变函数（distortion function）如下： 注：由于畸变函数 J 是非凸函数，意味着我们不能保证取得的最小值是全局最小值，也就是说 k-means对质心初始位置的选取比较感冒，但一般情况下k-means达到的局部最优已经满足需求。但如果你怕陷入局部最优，那么可以选取不同的初始值跑多遍 k-means，然后取其中最小的 J 对应的μ和 c 输出。 聚类后结果是一个个星团，星团里面的点相互距离比较近，星团间的星星距离就比较远了。 K-means与EM的关系： EM的基本思想： &gt; - E步：固定参数θ，优化隐含类别Q &gt; - M步：固定隐含类别Q，优化参数θ K-means用到的EM的基本思想： &gt; - E步：确定隐含类别变量c &gt; - M步：更新其他参数μ来使J最小化 两种算法都类似于坐标上升法。固定一个，优化另外一个。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习11-2--怎样用机器学习ML解决问题和在线学习]]></title>
    <url>%2F2014%2F06%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A011-2--%E6%80%8E%E6%A0%B7%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0ml%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%E5%92%8C%E5%9C%A8%E7%BA%BF%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[机器学习11-2--怎样用机器学习ML解决问题和在线学习 本章讲了，通过误差、偏差和优化算法来进行算法的调试。并讲了一些调试的方法。然后，进行了误差分析，去掉一些不必要的特征。最后举例说明。 算法的调试（Debugging Learning Algorithms） 存在的问题 1、对于贝叶斯逻辑回归问题： 2、如果，误差太大，我们怎样做？ 3、很显然，这样很耗时。 改进方法1：方差variance和偏差bias分析 改进方法2：优化算法和参数 参数是否恰当 算法是否收敛；是否正确优化了函数。 优化算法：通过对比两种算法 改进方法3：直觉 总结 出现的问题，及其解决方法： 误差分析和销蚀分析（ Error analyses and ablative analysis） 误差分析 通过加入新的成分使得准确率逐步升高： ### 销蚀分析 逐一去除某一成分，分析那一个去除时，准确率下降最快。 ###总结 - Error analysis tries to explain the difference between current performance and perfect performance. - Ablative analysis tries to explain the difference between some baseline (much poorer) performance and current performance. 实例问题分析（Getting started on a learning problem） 要解决一个问题，下面列出了两种方法。我们一般采用方法2（Approach #2）。因为，此方法可以快速实现我们的要求。此方法看似简单，但是NG说，很多人没有按照步骤做，结果导致半年甚至一年时间白白浪费掉。 总结 在线学习(online learning) 这一节相对比较独立，我就放在这里吧。 &gt; 1. 以前我们的学习算法：批学习算法（batch learning）；这里为：在线学习(online learning)。 2. 简而言之，每增加一个样本 都要重新计算一次\(\theta\) 。即： eg：我们举一个简单的例子，在逻辑回归中]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习11-1--贝叶斯统计和规则化]]></title>
    <url>%2F2014%2F06%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A011-1--%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1%E5%92%8C%E8%A7%84%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[机器学习11-1--贝叶斯统计和规则化 贝叶斯统计和规则化（ Bayesian statistics and regularization） 贝叶斯统计和规则化的目的：找更好的估计方法来减少过度拟合情况的发生。 回顾,总结： &gt; 1. 线性回归中使用的估计方法是最小二乘法 logistic 回归是条件概率的最大似然估计 朴素贝叶斯是联合概率的最大似然估计 SVM 是二次规划。 频率学派和贝叶斯学派 看待 \(\theta\) 的角度不同 #### 频率学派（frequentist statistics） 认为θ不是随机变量，只是一个未知的常量，因此我们没有把 #### 贝叶斯学派（Bayesian） 认为θ是随机变量。 步骤： 1、先验概率p(θ)：不同的θ值就有不同的概率。训练集：。则：θ的后验概率： &gt; 分母有误： 2、在θ是随机变量的情况下，如果新来一个样例特征为 x，那么为了预测 y。我们可以使用下面的公式： &gt; 注：在不同的模型下计算方式不同。比如在贝叶斯 logistic 回归中： 其中， 3、进而得到期望： &gt; 4、注： &gt; 1. 这次求解p(y|x, S)与之前的方式不同，以前是先求θ，然后直接预测，这次是对所有可 能的θ作积分。 2. 贝叶斯估计将θ视为随机变量，θ的值满足一定的分布，不是固定值，我们无法通过计算获得其值，只能在预测时计算积分。 3. 由于上述问题的存在；显然，后验概率p(θ|S)很难计算。为了解决这个问题，我们需要改变思路。看p(θ|S)公式中的分母，分母其实就是 P(S)，而我们就是要让P(S)在各种参数的影响下能够最大（这里只有参数θ） 。 因此我们只需求出随机变量θ中最可能的取值，这样求出θ后，可将θ视为固定值，那么预测时就不用积分了，而是直接像最大似然估计中求出θ后一样进行预测，这样就变成了点估计。这种方法称为MAP 最大后验概率估计（Maximum a posteriori）方法。 5、MAP 最大后验概率估计（Maximum a posteriori）方法，Θ估计公式为 &gt; 6、进行预测： &gt; \(h_{\hat{\theta}_{MAP}}(X)=\hat{\theta}_{MAP}^{T}X\) 7、总结与应用： &gt; 1. 与最大似然估计对比发现，MAP 只是将θ移进了条件概率中，并且多了一项p(θ)。 2. 一般情况下我们认为。 3. 贝叶斯最大后验概率估计相对于最大似然估计来说更容易克服过度拟合问题。原因： &gt;&gt; a. 整个公式由两项组成，极大化时，不代表此时p(θ)也能最大化。相反，θ是多值高斯分布，极大化时， p(θ)概率反而可能比较小。要达到最大化需要在两者之间达到平衡，也就靠近了偏差和方差线的交叉点。 b. 这个跟机器翻译里的噪声信道模型比较类似，由两个概率决定比有一个概率决定更靠谱。作者声称利用贝叶斯 logistic 回归（使用θMAP的 logistic 回归）应用于文本分类时，即使特征个数 n 远远大于样例个数 m，也很有效。 L1和L2规则化？？？？？？？ 老师上课没有讲到该部分，看了一下Pro Set3。也不是很理解。先引出正则化参数的损失函数，然后介绍了怎样最小化损失函数（过程不是很理解）。对于L1和L2在ODPS的XLab中有提到。后面，再仔细研究研究。对于L2：将后面一项改为2范数。 ### 贝叶斯xi回归L1]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习10--学习理论的基础知识]]></title>
    <url>%2F2014%2F06%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A010--%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[机器学习10--学习理论的基础知识 本章：模型选择（交叉选择的方法）--&gt;特征选择。 模型选择 设可选的模型集合为： ,那么 SVM、 logistic 回归、神经网络等模型都包含在 M 中。 训练集使用 S 来表示 任务：从 M 中选择最好的模型 方法一：简单交叉验证（数据集大） &gt; - 测试集的比例一般占全部数据的 1/4-1/3。30%是典型值。 &gt; - 由于测试集是和训练集中是两个世界的，因此我们可以认为这里的经验错误接近于泛化错误。 &gt; - 得到的最好模型，在全部数据上重新训练的到所需模型。 方法二：k-折叠交叉验证（数据集小） &gt; - 简而言之，这个方法就是将简单交叉验证的测试集改为 1/k，每个模型训练 k 次，测试 k 次，错误率为 k 次的平均。 - 一般讲k 取值为 10 - 数据集非常小的时候：极端情况下，k 可以取值为 m，意味着每次留一个样例做测试，这个称为 leave-one-out cross validation。 特征选择 其实，很多特征对于结果是无用的，想剔除 n 中的无用特征。n 个特征就有\(2^n\)种去除情况（每个特征去或者保留）。属于NP难问题。 本节主要将该问题简化：NP难\(2^n\)--&gt;\(n^2\)--&gt;\(n\) 第一种：启发式搜索方法：前向搜索和后向搜索\(O(n^2)\) \(时间 复杂度为O(n + (n − 1) +(n − 2) + ⋯ + 1) = O(n^2)\)。 #### 前向搜索 #### 后向搜索 先将 F 设置为{1,2,..,n}，然后每次删除一个特征，并评价，直到达到阈值或者为空，然后选择最佳的 F。 ### 第二种：过滤特征选择（Filter feature selection）\(O(n)\) 过滤特征选择方法的想法是针对每一个特征 ，i 从 1 到 n，计算相对于类别标签y的信息量S(i)，得到 n 个结果，然后将 n 个S(i)按照从大到小排名，输出前 k 个特征。显然，这样复杂度大大降低，为 O(n)。 &gt; 1、 $x_i \(是 0/1 离散值的时候 ：** 互信息（Mutual information）公式：![](/img/1401938493509.png) 2、**\)x_i $是多个离散值 注：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习9--学习理论的基础知识]]></title>
    <url>%2F2014%2F06%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A09--%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[#机器学习9--学习理论的基础知识 ## 基本符号 &gt; - \(\epsilon\) 泛化误差： &gt;&gt; - 从训练样本数据中推导的规则，能够适用于新的样本的能力。 &gt;&gt; - 对服从分布D的样本，分类错误的概率。 - \(\hat{\epsilon}\) 训练误差 &gt;&gt; - 训练误差在训练样本中训练出的规则，能够适用于训练样本的能力。 &gt;&gt; - 对训练样本，分类错误的部分，在总的训练集中所占比例。 - \(h\) - \(\hat{h }\) &gt;&gt; - \(\theta\) 和 \(\hat{\theta }\)关系：ERM风险最小化\(\theta\)的过程： - \(h\)和 \(\hat{h }\)关系：ERM风险最小化\(h\)的过程： - \(\epsilon\)和\(\hat{\epsilon}\)关系：误差随着模型复杂度（VC维）**的增加的变化趋势。模型复杂度过低：欠拟合；过大：过拟合。其中，模型复杂度为假设类\(\left| H\right|\)的大小，比如：某一个值为多项式多次。 &gt;&gt; \(H\)：假设类。对于先行分类器： \(D\)： 某一种分布。 基本公式 1、 2、 3、 ##两个引理 1、联合界引理： 2、Hoeffding不等式 &gt; 利用中心极限定理进行推导。其物理意义如下图所示。其中，表示阴影的概率，即错误的上届概率。当m增大时，钟形图收缩，误差下降。 \(H\)为有限的 即： 其中，H为： 1、 2、 即： &gt; 表示： - \(m\)很大时，右边很小，两个误差很接近。 - 是人为给定的值。 3、对于任意h： 4、对于任意非h： &gt; \(m\)很大时，右边很小，两个误差很接近。称为：一致收敛 5、我们关心的是m（样本大小）， （两个误差的差值）和概率（两个误差接近的概率）三者的值。下面，我们对其进行求解。 6、令，当时，我们得到样本的大小： 7、进一步，我们得到的值： 8、对7进行展开： 9、最终得到我们的定理：得到 \(\gamma\) 的值 &gt; 物理意义：我们可以近似地认为：为假设类H的偏差bias；为假设的方差variance。偏差表示误差的大小，随着模型复杂度增大而减小；方差表示拟合得有多好，随着模型复杂度增大，而先减小后增大。如下图： 10、最终，我们还得到另外一个定理，简而言之，固定，求m： &gt; 两个误差收敛的概率 下面开始为第10j ## \(H\)为无限的--更实用 当H有无限值时，即：**$|H|==k \(**。则上面公式10中![](/img/1401850414368.png)，k将趋于无穷大；则m将无穷大。显然，这样是不行的。为了解决这种问题我们引入VC维。从而得到我们的理论。 ### shatters的定义 ![](/img/1401931065879.png) ### VC维的定义： ![](/img/1401931155527.png) &gt; 结论：**对于n维线性分类器：\)VC(H)=n+1\(** eg. ![](/img/1401931314231.png)时：\)VC(H)=3$ 最终我们得到学习理论中的重要理论： 1、定理：得到 \(\gamma\) 的值 &gt; - m为样本数目； - ，从而我们可以得到\(\gamma\)的值。 2、推论：得到\(m\)的值 总结 通过学习本节我们可以大概知道:SVM和LR都不是直接的ERM算法，都是对其近似。本章推荐的理论给出了这两种算法的直观含义的一种解释。如下图：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习8--支持向量机（下）]]></title>
    <url>%2F2014%2F06%2F05%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A08--%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%B8%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[机器学习8--支持向量机（下） 上节回顾：从逻辑回归引出支持向量机--&gt;定义函数间隔和几何间隔--&gt;最优间隔分类器（通过最大化间隔，得到最优）--&gt;引出拉格朗日对偶（作用：通过对偶将算法变得更高效；处理高维）--&gt;将对偶用到最优间隔分类器 本节内容：核函数（升维）--&gt;判定是否是有效的核函数--&gt;L1 norm 软间隔SVM（针对不可分的情况）--&gt;第三种求解最优化的方法：坐标上升法--&gt;SMO优化算法（最快的二次规划优化算法） 核函数 1、定义 &gt; 2、核函数的作用 &gt; a、低维映射到高维，从而更好的拟合； b、将不可分的情况变为可分 3、举例： &gt; 例一： 说明：时间复杂度由 例二： 高斯核（把原始特征映射到无穷维）： 映射后的优点： 4、映射后怎样进行预测： &gt; 预测函数： 映射后：将 问题：怎样求取和判断核函数？下面将会介绍。 核函数的判定 1、符号说明 &gt; K：核函数矩阵$K={K_{ij}} $ 第i,j个样本 \(K_{ij}\) 或者$K(x{(i)},x{(j)}) $ ：核函数$K_{ij}=K(x{(i)},x{(j)}) $ 2、利用Mercer定理 &gt; 简而言之，K是一个有效的核函数&lt;--&gt;核函数矩阵是对称半正定 L1 norm 软间隔SVM 当不可分时，利用L1 软间隔进行分离 1、 加入软间隔后的模型： &gt; （1）凸规划： 其中，C是离群点的权重（我们预定的，为已知数），越大表明对目标函数影响越大，也就是月不希望看到离群点。引入非负参数（称为松弛变量） ， 就允许某些样本点的函数间隔小于1，即在最大间隔区间里面，或者函数间隔是负数，即样本点在对方的区域中。 （2）拉格朗日算子： 其中， （3）推导结果 跟前面模型类似：先写出拉格朗日公式 （如上） ，然后将其看作是变量 w 和 b的函数，分别对其求偏导，得到 w 和 b 的表达式。然后代入公式中，求带入后公式的极大值。得到： &gt;&gt; KTT条件： 第三种求解最优化的方法：坐标上升法 1、三种求解最优化的方法： &gt; （1）梯度上升法(求解最小值问题时，称作梯度下降法) （2）牛顿法（求解最值） （3）坐标上升法（求解最小值问题时，称作坐标下降法） 2、假设求解下面问题： &gt; 其中， 3、算法过程： &gt; &gt;&gt; SMO优化算法 最快的二次规划优化算法，特别针对线性 SVM 和数据稀疏时性能更优。 1、前面得到的结果 &gt; 首先，先看前面的到的结果，如下图： &gt;&gt; 这个问题中： 按照坐标上升法的思路，只固定一个的话，由于限制条件中存在，将导致不再是变量。 因此，我们一次选取两个参数做优化。 2、SMO的主要步骤 &gt; 注：第一步，利用启发式方法选取。第二步，固定其他参数， 3、 具体步骤 &gt; （1）固定除以外的参数，得： （2）为了方便: &gt;&gt; 如下图： 其中，满足：和 L和H的范围： （3）将带入W中： 展开，得： （4）这就是二次函数的最小值问题，容易求得（在纸上画图很容易看出来）： 其中，为最终结果。为求导得到的结果。 同理，求得的最优解。 总结 1、本章的系统结构： &gt; 至此，我们得到： 预测问题只需要求解，即： &lt;--&gt;上面的过程（SMO）求得为的最优解 &lt;--&gt; &lt;--&gt; &lt;--&gt; &lt;--&gt; &lt;--&gt;（最大化几何间隔问题） 2、关于核问题（升维）的说明 &gt; 核问题用来替换第一步中的部分：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习7--支持向量机（上）]]></title>
    <url>%2F2014%2F05%2F30%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A07-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88%E4%B8%8A%EF%BC%89%2F</url>
    <content type="text"><![CDATA[本节：从逻辑回归引出支持向量机--&gt;定义函数间隔和几何间隔--&gt;最优间隔分类器（通过最大化间隔，得到最优）--&gt;引出拉格朗日对偶（作用：通过对偶将算法变得更高效；处理高维）--&gt;将对偶用到最优间隔分类器 本节都是针对很明显的分类'good'，如下图。 逻辑回归和支持向量机关系 两者关系 两者都可以实现分类。LR考虑全局（几经远离的点，通过调节中间线使其更远）；SVM考虑局部（不关心已经确定远离的点）。 ### 形式化表示LR--&gt;SVM表示 \(\theta\) 变为w和b。0、1变为0、-1。 函数间隔和几何间隔 定义函数间隔--&gt;通过归一化，再定义几何间隔--&gt;两者关系。几何间隔的意义：点到超平面的距离。 - 函数间隔 一个训练样本： 所有训练样本： - 几何间隔 一个训练样本： 所有训练样本： - 两者关系 当时，两者等价。 最优化间隔分类器 几何间隔的优化（非凸规划：集合非凸集）--&gt; 函数间隔的优化（非凸规划：最大化函数非凸函数）--&gt; 改写后的规划（凸规划） - 几何间隔的优化 - 函数间隔的优化 - 改写后的规划：令，这样做的意义：将全局的函数间隔定义为1，即将离超平面最近的点的距离定义为，： 拉格朗日对偶 拉格朗日乘数法 1.最优化问题： 2.拉格朗日公式（拉格朗日算子）： 其中，l为约束的个数（样本数目），为拉格朗日乘数。 3.通过求偏导，解得参数 广义拉格朗日：拉格朗日+不等式 1.原始primal的最优化问题： 2.拉格朗日公式（拉格朗日算子）： 其中，为拉格朗日乘数。 3.进一步得到 即： 4.从而将问题转化（p:primal）： 5.定义对偶问题（D:dual）： 定义：，则： 6.得到不等式关系： 7.上面两者等价的条件，KKT条件： ，即KKT条件： &gt; 注: 最优间隔分类器及拉格朗日对偶 在SVM中的应用 优化问题主要是求解w；b仅仅是一个参数。 ### SVM的优化问题 - 1、优化问题 - 2、拉格朗日算子 - 3、没有等式约束，只有不等式。下图中虚线上的三个点称为支持向量（支持向量机中的支持向量）。三点的函数间隔为1。。 ### 求解最优化问题 - 1、对偶问题 - 2、求解极小化问题：对w求偏导 - 3、求解极小化问题：对w求偏导 - 4、将w，b带入L中 注：该函数是关于的函数。通过极大化，来求解得到该值。然后，再得到w和b。 - 5、求解极大化问题： 凸规划： - 6、求解 &gt; 求解得到； 根据得到w； 根据得到b。 其中，b的意义：离超平面最近的正的函数间隔要等于离超平面最近的负的函数间隔。 - 7、进行预测 如下公式所示:有了\(\alpha_i\)，我们不需要求出w，只需将新来的样本和训练数据中的所有样本做内积和即可。那有人会说，与前面所有的样本都做运算是不是太耗时了？其实不然，我们从KKT条件中得到，只有支持向量的\(\alpha_i\)&gt; 0，其他情况\(\alpha_i\)= 0。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习5_6--生成学习算法(高斯判别分析GDA和朴素贝叶斯NB)]]></title>
    <url>%2F2014%2F05%2F29%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A05_6--%E7%94%9F%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%88%E9%AB%98%E6%96%AF%E3%80%81nb%EF%BC%89%2F</url>
    <content type="text"><![CDATA[#生成学习算法 说明： - m为样本数目，i为第i个样本数目； - joint likelihood ：联合似然估计 ## 判别学习算法和生成学习算法的区别： 算法 Learn 说明 概率类型 判别学习算法 \(P(y/x)\) 特征x下，输出y的概率 先验概率（经验为x） 生成学习算法 \(P(x/y)和P(y)\) 在模型y（为良性肿瘤）下，特征x发生的概率 后验概率（投‘果’探‘因’）——条件概率 第一个生成学习算法：GDA（高斯判别分析） 特点 假设输入x连续，服从多元高斯分布。 本例中y服从伯努利分布 GDA模型 输入x连续，服从多元高斯分布。 y服从伯努利分布 似然函数 最大化似然函数得到所需参数 GDA与LR的区别 第二个生成学习算法：Navie Bayes朴素贝叶斯 特点 输入x离散，服从伯努利分布 （这也是为什么成为“朴素”的原因）。但是，x是多元伯努利事件模型。 本例中y服从伯努利分布 贝叶斯假设 输入变量x相互独立（在给定y的基础上）。 NB模型 模型--&gt;最大化似然函数--&gt;得到参数--&gt;进行预测 模型及最大化 最大化似然函数得到的参数 进行预测 Laplace平滑法 用来处理，防止\(0/0\)的情况。 平滑后的NB参数： 第三个生成学习算法：NB的第一种变形：多项式事件模型（处理文本） 生成模型 似然函数 最大化似然函数得到参数 参数的物理意义 Laplace平滑处理 第四个生成学习算法：NB的第二2种变形：x可以取K个值 疑问 怎么样最大化似然函数，得到参数（似乎不是利用梯度法，或者牛顿法）？这个老师让自己去想。。。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习4--GLM 广义线性模型']]></title>
    <url>%2F2014%2F05%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A04-glm-%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[机器学习2-GLM 广义线性模型 对于高斯分布、伯努利分布、多项式分布，均称之为GLM。 ## 指数族 - 将高斯分布、伯努利分布、多项式分布转换为指数族，可以方便求解：首先，由\(p（y|x;\theta）\) 得到\(T(y),a(y),b(y)\)； 然后，再经 \(g(x)--&gt;h(x)--&gt;L(x)--&gt;l(x)\)(正则响应函数，学习算法的输出函数，似然函数，log似然函数)，得到结果。 - 似然函数：表示某种事件发生的可能性；因此，一般似然函数表示为概率相乘的形式。\(E(T(y)|x;\theta)\) - 指数族的三要素：\(T(y),a(y),b(y)\) 多分类逻辑回归（指数族在多项式分布中的应用）&lt;---&gt;LR的推广 二分类逻辑回归 ### 多分类逻辑回归 ### 总结 GLM，以上两种方法，最终都得到似然函数，然后，利用梯度上升法或者牛顿法，进行求解 。 #### 梯度上升法 。 例如：而分类逻辑回归： 最终得到： #### 牛顿法 速度快，只需迭代十多次，但是，计算量很大。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习1_2_3_4--梯度和牛顿算法]]></title>
    <url>%2F2014%2F05%2F21%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01_2_3_4--%E6%A2%AF%E5%BA%A6%E5%92%8C%E7%89%9B%E9%A1%BF%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[#一、 符号说明 \(m\):样本数目（行数）&lt;--&gt;\(i\)某一行 \(n:\)特征数目（列数）&lt;--&gt;\(j\)某一列 \(x,y\):输入和输出变量 \(i^{th}\):training example&lt;--&gt;\((x^{(i)}, y{(j)})\) \(\theta\)学习参数 \(h_{\theta}(x)\):输出函数 #二、 算法总结 ##1. 最小均方（误差）算法LMS 该算法是为了使损失函数最小。损失函数为： 得到梯度下降的基本算法，下面会对其展开详细说明 ###1.1 批量梯度下降 缺点：不适合数据量很大的情况 ###1.2 随机梯度下降 ###1.3 另外一种最小化\(J_{\theta}\)的算法最小二乘法 \((J=0)\) ##2. 局部加权线性回归 优点：在一定程度上防止了过拟合和欠拟合 参数化的学习算法：有固定参数学习，用来数据拟合 非参数学习算法： 局部加权线性回归 ##3.逻辑回归（第一种：最大化\(L(\theta)\)） 基本公式： \(\theta\)的训练方法：利用梯度上升，来最大化似然函数(Likehood Function): 注：得到的最终结果，与LMS中的循环规则完全一样。但是，不同之处在于：此处\(h_{\theta}(x^{(i)})\)为非线性的。 ##4.牛顿（第二种：最大化\(L(\theta)\)） #三、 从概率角度解释最小化\(J\)的含义 使误差为0的概率最大&lt;--&gt;最大化相似函数\(l(\theta)\)或者\(L(\theta)\)&lt;--&gt;最小化\(J\) ：最大似然估计 Written with StackEdit.]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
</search>
